{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76c3d1c",
   "metadata": {},
   "source": [
    "# Prctica 4: Similitud de Text Semntic (STS) per al Catal - Anlisi Comprensiva\n",
    "\n",
    "**Objectius**: \n",
    "- Entrenar models de Word2Vec amb diferents mides de corpus\n",
    "- Implementar i comparar diferents t猫cniques d'embeddings per a STS\n",
    "- Avaluar models baseline vs. models avan莽ats\n",
    "- Analitzar l'impacte de la mida del corpus i tipus d'embedding\n",
    "\n",
    "## Estructura de la Prctica:\n",
    "1. **Entrenament de Models Word2Vec** - Diferents mides de corpus\n",
    "2. **Models Baseline** - Similitud cosinus simple vs. TF-IDF\n",
    "3. **Model 1**: Embeddings Agregats - Vectors de frase concatenats\n",
    "4. **Model 2**: Seq眉猫ncia d'Embeddings - Amb mecanisme d'atenci贸\n",
    "5. **Experimentaci贸 Avan莽ada** - spaCy, RoBERTa, models fine-tuned\n",
    "6. **Anlisi Comparativa** - Correlaci贸 de Pearson i MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d78e80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74960cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessaris\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional, Dict, Union\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Gensim imports\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors, TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuraci贸 de GPU (opcional)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef5891",
   "metadata": {},
   "source": [
    "## 1. Carrega del Dataset i Preparaci贸 de Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39504fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Carregar datasets\n",
    "print(\"Carregant datasets...\")\n",
    "train_data = load_dataset(\"projecte-aina/sts-ca\", split=\"train\")\n",
    "test_data = load_dataset(\"projecte-aina/sts-ca\", split=\"test\") \n",
    "val_data = load_dataset(\"projecte-aina/sts-ca\", split=\"validation\")\n",
    "\n",
    "# Convertir a DataFrame\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Label range: {train_df['label'].min():.2f} - {train_df['label'].max():.2f}\")\n",
    "\n",
    "# Dataset per entrenar Word2Vec\n",
    "catalan_corpus = load_dataset(\"projecte-aina/catalan_general_crawling\")\n",
    "catalan_dataset = catalan_corpus['train']\n",
    "\n",
    "print(f\"Corpus catal: {len(catalan_dataset)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d695485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diferents mides de corpus per experimentar\n",
    "def create_corpus_subsets(dataset):\n",
    "    \"\"\"Crea subsets del corpus de diferents mides\"\"\"\n",
    "    subsets = {}\n",
    "    \n",
    "    # 100MB\n",
    "    size = 0\n",
    "    for i in range(len(dataset)):\n",
    "        size += len(dataset[i]['text'])\n",
    "        if size > 100_000_000:  # 100 MB\n",
    "            break\n",
    "    subsets['100MB'] = dataset.select(list(range(i)))\n",
    "    \n",
    "    # 500MB\n",
    "    size = 0\n",
    "    for j in range(len(dataset)):\n",
    "        size += len(dataset[j]['text'])\n",
    "        if size > 500_000_000:  # 500 MB\n",
    "            break\n",
    "    subsets['500MB'] = dataset.select(list(range(j)))\n",
    "    \n",
    "    # 1GB\n",
    "    size = 0\n",
    "    for k in range(len(dataset)):\n",
    "        size += len(dataset[k]['text'])\n",
    "        if size > 1_000_000_000:  # 1 GB\n",
    "            break\n",
    "    subsets['1GB'] = dataset.select(list(range(k)))\n",
    "    \n",
    "    # Corpus complet (limitat per recursos)\n",
    "    subsets['complet'] = dataset\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "corpus_subsets = create_corpus_subsets(catalan_dataset)\n",
    "print(\"Subsets creats:\", list(corpus_subsets.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af62c0e",
   "metadata": {},
   "source": [
    "## 2. Entrenament de Models Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Stopwords catalans\n",
    "stopwords_catala = [\n",
    "    \"a\", \"abans\", \"ac铆\", \"aix铆\", \"alguns\", \"alguna\", \"algunes\", \"alg煤\", \"alhora\", \n",
    "    \"als\", \"all貌\", \"aquell\", \"aquelles\", \"aquells\", \"baix\", \"cada\", \"com\", \n",
    "    \"eixa\", \"eixes\", \"eix铆\", \"eixos\", \"el\", \"ella\", \"elles\", \"ell\", \"ells\", \n",
    "    \"en\", \"endavant\", \"enfront\", \"ens\", \"entre\", \"he\", \"hem\", \"heu\", \"hi\", \"ho\", \n",
    "    \"i\", \"igual\", \"iguals\", \"ja\", \"jo\", \"l'\", \"la\", \"les\", \"li\", \"els\", \"tu\", \n",
    "    \"nosaltres\", \"vosaltres\", \"de\", \"del\", \"dels\", \"d'un\", \"d'una\", \"des\"\n",
    "]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Neteja i tokenitza el text\"\"\"\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Elimina carcters no alfanum猫rics\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords_catala and len(word) > 2]\n",
    "    return words\n",
    "\n",
    "def create_corpus_for_w2v(dataset):\n",
    "    \"\"\"Crea corpus preprocessat per Word2Vec\"\"\"\n",
    "    corpus = []\n",
    "    for doc in dataset:\n",
    "        words = preprocess_text(doc['text'])\n",
    "        if words:  # Nom茅s afegir si no est buit\n",
    "            corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "def train_word2vec(corpus, vector_size=100, window=5, min_count=10, workers=4, epochs=25):\n",
    "    \"\"\"Entrena un model Word2Vec\"\"\"\n",
    "    model = Word2Vec(\n",
    "        corpus, \n",
    "        vector_size=vector_size, \n",
    "        window=window, \n",
    "        min_count=min_count, \n",
    "        workers=workers, \n",
    "        epochs=epochs,\n",
    "        sg=1  # Skip-gram\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar models Word2Vec per cada mida de corpus\n",
    "word2vec_models = {}\n",
    "\n",
    "for corpus_name in [\"100MB\", \"500MB\", \"1GB\"]:  # Evitem 'complet' per recursos\n",
    "    print(f\"\\nEntrenant Word2Vec per {corpus_name}...\")\n",
    "    \n",
    "    # Crear corpus preprocessat\n",
    "    corpus = create_corpus_for_w2v(corpus_subsets[corpus_name])\n",
    "    \n",
    "    # Entrenar model\n",
    "    model = train_word2vec(corpus, vector_size=100)\n",
    "    \n",
    "    # Guardar model\n",
    "    model.save(f\"word2vec_{corpus_name}.model\")\n",
    "    word2vec_models[corpus_name] = model\n",
    "    \n",
    "    print(f\"Model {corpus_name} entrenat amb {len(model.wv.key_to_index)} paraules\")\n",
    "    \n",
    "    # Mostrar exemples de paraules similars\n",
    "    if \"casa\" in model.wv:\n",
    "        similar_words = model.wv.most_similar(\"casa\", topn=5)\n",
    "        print(f\"Paraules similars a 'casa': {similar_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d6f9e",
   "metadata": {},
   "source": [
    "## 3. Carrega d'Embeddings Pre-entrenats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar embeddings FastText pre-entrenats\n",
    "PRETRAINED_PATH = '../cc.ca.300.vec'\n",
    "\n",
    "try:\n",
    "    print(\"Carregant embeddings FastText pre-entrenats...\")\n",
    "    fasttext_model = KeyedVectors.load_word2vec_format(PRETRAINED_PATH, binary=False)\n",
    "    print(f\"FastText carregat: {len(fasttext_model.key_to_index)} paraules, {fasttext_model.vector_size}D\")\n",
    "    word2vec_models['fasttext'] = fasttext_model\n",
    "except FileNotFoundError:\n",
    "    print(\"Embeddings FastText no trobats. Continuant sense ells.\")\n",
    "    fasttext_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8732aa8",
   "metadata": {},
   "source": [
    "## 4. Funcions d'Utilitat per Processament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24191764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence: str) -> List[str]:\n",
    "    \"\"\"Preprocessa una frase per a STS\"\"\"\n",
    "    return simple_preprocess(sentence.lower())\n",
    "\n",
    "def get_sentence_embedding_mean(sentence: str, wv_model, vector_size: int) -> np.ndarray:\n",
    "    \"\"\"Obt茅 embedding de frase mitjan莽ant mitjana simple\"\"\"\n",
    "    words = preprocess_sentence(sentence)\n",
    "    vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if hasattr(wv_model, 'wv'):  # Model Word2Vec\n",
    "            if word in wv_model.wv:\n",
    "                vectors.append(wv_model.wv[word])\n",
    "        else:  # KeyedVectors\n",
    "            if word in wv_model:\n",
    "                vectors.append(wv_model[word])\n",
    "    \n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "def get_sentence_embedding_tfidf(sentence: str, wv_model, tfidf_vectorizer, \n",
    "                                feature_names: List[str], vector_size: int) -> np.ndarray:\n",
    "    \"\"\"Obt茅 embedding de frase amb ponderaci贸 TF-IDF\"\"\"\n",
    "    words = preprocess_sentence(sentence)\n",
    "    \n",
    "    # Calcular TF-IDF\n",
    "    tfidf_vector = tfidf_vectorizer.transform([' '.join(words)])\n",
    "    tfidf_scores = tfidf_vector.toarray()[0]\n",
    "    \n",
    "    weighted_vectors = []\n",
    "    weights = []\n",
    "    \n",
    "    for word in words:\n",
    "        word_vector = None\n",
    "        \n",
    "        # Obtenir vector de la paraula\n",
    "        if hasattr(wv_model, 'wv'):\n",
    "            if word in wv_model.wv:\n",
    "                word_vector = wv_model.wv[word]\n",
    "        else:\n",
    "            if word in wv_model:\n",
    "                word_vector = wv_model[word]\n",
    "        \n",
    "        # Aplicar pes TF-IDF\n",
    "        if word_vector is not None and word in feature_names:\n",
    "            word_idx = feature_names.index(word)\n",
    "            weight = tfidf_scores[word_idx]\n",
    "            if weight > 0:\n",
    "                weighted_vectors.append(word_vector * weight)\n",
    "                weights.append(weight)\n",
    "    \n",
    "    if weighted_vectors and sum(weights) > 0:\n",
    "        return np.sum(weighted_vectors, axis=0) / sum(weights)\n",
    "    else:\n",
    "        return np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f42b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dades per avaluaci贸\n",
    "all_sentences = (train_df['sentence_1'].tolist() + train_df['sentence_2'].tolist() + \n",
    "                test_df['sentence_1'].tolist() + test_df['sentence_2'].tolist() + \n",
    "                val_df['sentence_1'].tolist() + val_df['sentence_2'].tolist())\n",
    "\n",
    "# Crear TF-IDF vectorizer\n",
    "corpus_for_tfidf = [' '.join(preprocess_sentence(sent)) for sent in all_sentences]\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, lowercase=True)\n",
    "tfidf_vectorizer.fit(corpus_for_tfidf)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "print(f\"TF-IDF preparat amb {len(feature_names)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95cc93",
   "metadata": {},
   "source": [
    "## 5. Models Baseline: Similitud Cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cosine_baseline(df: pd.DataFrame, wv_model, method='mean') -> Dict[str, float]:\n",
    "    \"\"\"Avalua baseline de similitud cosinus\"\"\"\n",
    "    similarities = []\n",
    "    true_scores = df['label'].values\n",
    "    \n",
    "    vector_size = wv_model.wv.vector_size if hasattr(wv_model, 'wv') else wv_model.vector_size\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        sent1, sent2 = row['sentence_1'], row['sentence_2']\n",
    "        \n",
    "        if method == 'tfidf':\n",
    "            vec1 = get_sentence_embedding_tfidf(sent1, wv_model, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "            vec2 = get_sentence_embedding_tfidf(sent2, wv_model, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "        else:  # mean\n",
    "            vec1 = get_sentence_embedding_mean(sent1, wv_model, vector_size)\n",
    "            vec2 = get_sentence_embedding_mean(sent2, wv_model, vector_size)\n",
    "        \n",
    "        # Similitud cosinus\n",
    "        if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "            sim = 0.0\n",
    "        else:\n",
    "            sim = 1 - cosine(vec1, vec2)\n",
    "        \n",
    "        # Escalar a [0,5]\n",
    "        sim_scaled = (sim + 1) * 2.5\n",
    "        similarities.append(sim_scaled)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    pearson_corr, _ = pearsonr(true_scores, similarities)\n",
    "    mse = mean_squared_error(true_scores, similarities)\n",
    "    mae = mean_absolute_error(true_scores, similarities)\n",
    "    \n",
    "    return {\n",
    "        'pearson': pearson_corr,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'predictions': similarities\n",
    "    }\n",
    "\n",
    "# Avaluar baselines per tots els models\n",
    "baseline_results = {}\n",
    "\n",
    "for model_name, model in word2vec_models.items():\n",
    "    print(f\"\\n=== Avaluant Baseline: {model_name} ===\")\n",
    "    \n",
    "    # Mitjana simple\n",
    "    results_mean = evaluate_cosine_baseline(val_df, model, method='mean')\n",
    "    \n",
    "    # TF-IDF ponderat\n",
    "    results_tfidf = evaluate_cosine_baseline(val_df, model, method='tfidf')\n",
    "    \n",
    "    baseline_results[model_name] = {\n",
    "        'mean': results_mean,\n",
    "        'tfidf': results_tfidf\n",
    "    }\n",
    "    \n",
    "    print(f\"Mitjana Simple - Pearson: {results_mean['pearson']:.3f}, MSE: {results_mean['mse']:.3f}\")\n",
    "    print(f\"TF-IDF - Pearson: {results_tfidf['pearson']:.3f}, MSE: {results_tfidf['mse']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e80cf",
   "metadata": {},
   "source": [
    "## 6. Model 1: Regressi贸 amb Embeddings Agregats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_aggregated_model(embedding_dim: int, hidden_size: int = 128, \n",
    "                         dropout_rate: float = 0.3) -> tf.keras.Model:\n",
    "    \"\"\"Model de regressi贸 amb embeddings agregats\"\"\"\n",
    "    input_1 = tf.keras.Input(shape=(embedding_dim,), name=\"sentence_1\")\n",
    "    input_2 = tf.keras.Input(shape=(embedding_dim,), name=\"sentence_2\")\n",
    "    \n",
    "    # Concatenar embeddings\n",
    "    concatenated = tf.keras.layers.Concatenate(axis=-1)([input_1, input_2])\n",
    "    \n",
    "    # Capes de processament\n",
    "    x = tf.keras.layers.BatchNormalization()(concatenated)\n",
    "    x = tf.keras.layers.Dense(hidden_size, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.Dense(hidden_size // 2, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Sortida (regressi贸 0-5)\n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_aggregated_data(df: pd.DataFrame, wv_model, method='mean'):\n",
    "    \"\"\"Prepara dades per model agregat\"\"\"\n",
    "    X1, X2, Y = [], [], []\n",
    "    \n",
    "    vector_size = wv_model.wv.vector_size if hasattr(wv_model, 'wv') else wv_model.vector_size\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        sent1, sent2, label = row['sentence_1'], row['sentence_2'], row['label']\n",
    "        \n",
    "        if method == 'tfidf':\n",
    "            vec1 = get_sentence_embedding_tfidf(sent1, wv_model, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "            vec2 = get_sentence_embedding_tfidf(sent2, wv_model, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "        else:\n",
    "            vec1 = get_sentence_embedding_mean(sent1, wv_model, vector_size)\n",
    "            vec2 = get_sentence_embedding_mean(sent2, wv_model, vector_size)\n",
    "        \n",
    "        X1.append(vec1)\n",
    "        X2.append(vec2)\n",
    "        Y.append(label)\n",
    "    \n",
    "    return np.array(X1), np.array(X2), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67dfb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar models agregats\n",
    "aggregated_results = {}\n",
    "\n",
    "for model_name, model in word2vec_models.items():\n",
    "    print(f\"\\n=== Entrenant Model Agregat: {model_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Preparar dades\n",
    "        X1_train, X2_train, Y_train = prepare_aggregated_data(train_df, model)\n",
    "        X1_val, X2_val, Y_val = prepare_aggregated_data(val_df, model)\n",
    "        \n",
    "        vector_size = model.wv.vector_size if hasattr(model, 'wv') else model.vector_size\n",
    "        \n",
    "        # Construir i entrenar model\n",
    "        keras_model = build_aggregated_model(embedding_dim=vector_size)\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True, verbose=0\n",
    "        )\n",
    "        \n",
    "        # Entrenament\n",
    "        history = keras_model.fit(\n",
    "            [X1_train, X2_train], Y_train,\n",
    "            validation_data=([X1_val, X2_val], Y_val),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Avaluaci贸\n",
    "        Y_pred = keras_model.predict([X1_val, X2_val], verbose=0).flatten()\n",
    "        pearson_corr, _ = pearsonr(Y_val, Y_pred)\n",
    "        mse = mean_squared_error(Y_val, Y_pred)\n",
    "        mae = mean_absolute_error(Y_val, Y_pred)\n",
    "        \n",
    "        aggregated_results[model_name] = {\n",
    "            'model': keras_model,\n",
    "            'history': history,\n",
    "            'pearson': pearson_corr,\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'predictions': Y_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"Resultats - Pearson: {pearson_corr:.3f}, MSE: {mse:.3f}, MAE: {mae:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error entrenant {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26697be",
   "metadata": {},
   "source": [
    "## 7. Model 2: Seq眉猫ncia d'Embeddings amb Atenci贸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dc5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Capa d'atenci贸 simple\"\"\"\n",
    "    def __init__(self, units: int = 128, **kwargs):\n",
    "        super(SimpleAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.W1 = tf.keras.layers.Dense(units, activation='tanh')\n",
    "        self.W2 = tf.keras.layers.Dense(1)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Calcular scores d'atenci贸\n",
    "        ui = self.W1(inputs)\n",
    "        ui = self.dropout(ui)\n",
    "        scores = self.W2(ui)\n",
    "        scores = tf.squeeze(scores, axis=-1)\n",
    "        \n",
    "        # Aplicar mscara\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, dtype=tf.float32)\n",
    "            scores = tf.where(mask, scores, tf.fill(tf.shape(scores), -1e9))\n",
    "        \n",
    "        # Pesos d'atenci贸\n",
    "        alpha = tf.nn.softmax(scores, axis=-1)\n",
    "        alpha = tf.expand_dims(alpha, axis=-1)\n",
    "        \n",
    "        # Vector de context\n",
    "        context_vector = tf.reduce_sum(alpha * inputs, axis=1)\n",
    "        \n",
    "        return context_vector\n",
    "\n",
    "def build_sequence_model(vocab_size: int, embedding_dim: int, sequence_length: int = 32,\n",
    "                        pretrained_weights: Optional[np.ndarray] = None,\n",
    "                        trainable_embeddings: bool = False) -> tf.keras.Model:\n",
    "    \"\"\"Model de seq眉猫ncies amb atenci贸\"\"\"\n",
    "    input_1 = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)\n",
    "    input_2 = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)\n",
    "    \n",
    "    # Capa d'embedding\n",
    "    if pretrained_weights is not None:\n",
    "        embedding_layer = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=sequence_length,\n",
    "            weights=[pretrained_weights],\n",
    "            trainable=trainable_embeddings,\n",
    "            mask_zero=True\n",
    "        )\n",
    "    else:\n",
    "        embedding_layer = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=sequence_length,\n",
    "            trainable=True,\n",
    "            mask_zero=True\n",
    "        )\n",
    "    \n",
    "    # Aplicar embedding\n",
    "    embedded_1 = embedding_layer(input_1)\n",
    "    embedded_2 = embedding_layer(input_2)\n",
    "    \n",
    "    # Atenci贸\n",
    "    attention_layer = SimpleAttention(units=64)\n",
    "    sentence_vector_1 = attention_layer(embedded_1)\n",
    "    sentence_vector_2 = attention_layer(embedded_2)\n",
    "    \n",
    "    # Projecci贸 i normalitzaci贸\n",
    "    projection_layer = tf.keras.layers.Dense(embedding_dim, activation='tanh')\n",
    "    dropout = tf.keras.layers.Dropout(0.3)\n",
    "    \n",
    "    projected_1 = dropout(projection_layer(sentence_vector_1))\n",
    "    projected_2 = dropout(projection_layer(sentence_vector_2))\n",
    "    \n",
    "    normalized_1 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1)\n",
    "    )(projected_1)\n",
    "    normalized_2 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1)\n",
    "    )(projected_2)\n",
    "    \n",
    "    # Similitud cosinus escalada\n",
    "    similarity = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.reduce_sum(x[0] * x[1], axis=1, keepdims=True)\n",
    "    )([normalized_1, normalized_2])\n",
    "    \n",
    "    output = tf.keras.layers.Lambda(\n",
    "        lambda x: 2.5 * (1.0 + x)\n",
    "    )(similarity)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f0af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparaci贸 de vocabulari i seq眉猫ncies\n",
    "def create_vocabulary_mapping(sentences: List[str], max_vocab_size: int = 10000):\n",
    "    \"\"\"Crea mapatge de vocabulari\"\"\"\n",
    "    word_counts = {}\n",
    "    for sentence in sentences:\n",
    "        words = preprocess_sentence(sentence)\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    # Ordenar per freq眉猫ncia\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Crear mapatge\n",
    "    word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    idx_to_word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "    \n",
    "    for word, count in sorted_words[:max_vocab_size-2]:\n",
    "        idx = len(word_to_idx)\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "    \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def sentence_to_sequence(sentence: str, word_to_idx: Dict[str, int], \n",
    "                        max_length: int = 32) -> np.ndarray:\n",
    "    \"\"\"Converteix frase a seq眉猫ncia d'铆ndexs\"\"\"\n",
    "    words = preprocess_sentence(sentence)\n",
    "    sequence = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            sequence.append(word_to_idx[word])\n",
    "        else:\n",
    "            sequence.append(word_to_idx[\"<UNK>\"])\n",
    "    \n",
    "    # Padding o truncament\n",
    "    if len(sequence) > max_length:\n",
    "        sequence = sequence[:max_length]\n",
    "    else:\n",
    "        sequence.extend([word_to_idx[\"<PAD>\"]] * (max_length - len(sequence)))\n",
    "    \n",
    "    return np.array(sequence)\n",
    "\n",
    "# Crear vocabulari\n",
    "word_to_idx, idx_to_word = create_vocabulary_mapping(all_sentences, max_vocab_size=10000)\n",
    "vocab_size = len(word_to_idx)\n",
    "sequence_length = 32\n",
    "\n",
    "print(f\"Vocabulari creat: {vocab_size} paraules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aaf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar model de seq眉猫ncies amb el millor Word2Vec\n",
    "if word2vec_models:\n",
    "    best_w2v_name = max(baseline_results.keys(), \n",
    "                       key=lambda k: baseline_results[k]['mean']['pearson'])\n",
    "    best_w2v_model = word2vec_models[best_w2v_name]\n",
    "    \n",
    "    print(f\"Utilitzant millor model Word2Vec: {best_w2v_name}\")\n",
    "    \n",
    "    # Crear matriu d'embeddings pre-entrenats\n",
    "    embedding_dim = best_w2v_model.wv.vector_size if hasattr(best_w2v_model, 'wv') else best_w2v_model.vector_size\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if hasattr(best_w2v_model, 'wv'):\n",
    "            if word in best_w2v_model.wv:\n",
    "                embedding_matrix[idx] = best_w2v_model.wv[word]\n",
    "        else:\n",
    "            if word in best_w2v_model:\n",
    "                embedding_matrix[idx] = best_w2v_model[word]\n",
    "    \n",
    "    # Preparar dades de seq眉猫ncies\n",
    "    def prepare_sequence_data(df):\n",
    "        X1_seq, X2_seq, Y_seq = [], [], []\n",
    "        for _, row in df.iterrows():\n",
    "            seq1 = sentence_to_sequence(row['sentence_1'], word_to_idx, sequence_length)\n",
    "            seq2 = sentence_to_sequence(row['sentence_2'], word_to_idx, sequence_length)\n",
    "            X1_seq.append(seq1)\n",
    "            X2_seq.append(seq2)\n",
    "            Y_seq.append(row['label'])\n",
    "        return np.array(X1_seq), np.array(X2_seq), np.array(Y_seq)\n",
    "    \n",
    "    X1_train_seq, X2_train_seq, Y_train_seq = prepare_sequence_data(train_df)\n",
    "    X1_val_seq, X2_val_seq, Y_val_seq = prepare_sequence_data(val_df)\n",
    "    \n",
    "    # Model amb embeddings pre-entrenats (frozen)\n",
    "    print(\"\\nEntrenant model amb embeddings frozen...\")\n",
    "    model_seq_frozen = build_sequence_model(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        sequence_length=sequence_length,\n",
    "        pretrained_weights=embedding_matrix,\n",
    "        trainable_embeddings=False\n",
    "    )\n",
    "    \n",
    "    history_frozen = model_seq_frozen.fit(\n",
    "        [X1_train_seq, X2_train_seq], Y_train_seq,\n",
    "        validation_data=([X1_val_seq, X2_val_seq], Y_val_seq),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    Y_pred_frozen = model_seq_frozen.predict([X1_val_seq, X2_val_seq], verbose=0).flatten()\n",
    "    pearson_frozen, _ = pearsonr(Y_val_seq, Y_pred_frozen)\n",
    "    mse_frozen = mean_squared_error(Y_val_seq, Y_pred_frozen)\n",
    "    \n",
    "    print(f\"Model frozen - Pearson: {pearson_frozen:.3f}, MSE: {mse_frozen:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde46859",
   "metadata": {},
   "source": [
    "## 8. Experimentaci贸 amb Models Avan莽ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentaci贸 amb spaCy\n",
    "try:\n",
    "    import spacy\n",
    "    \n",
    "    print(\"Provant amb spaCy...\")\n",
    "    nlp = spacy.load(\"ca_core_news_md\")\n",
    "    \n",
    "    def get_spacy_embedding(sentence: str) -> np.ndarray:\n",
    "        doc = nlp(sentence)\n",
    "        return doc.vector\n",
    "    \n",
    "    # Avaluar amb spaCy\n",
    "    spacy_similarities = []\n",
    "    for _, row in val_df.iterrows():\n",
    "        vec1 = get_spacy_embedding(row['sentence_1'])\n",
    "        vec2 = get_spacy_embedding(row['sentence_2'])\n",
    "        \n",
    "        if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "            sim = 0.0\n",
    "        else:\n",
    "            sim = 1 - cosine(vec1, vec2)\n",
    "        \n",
    "        sim_scaled = (sim + 1) * 2.5\n",
    "        spacy_similarities.append(sim_scaled)\n",
    "    \n",
    "    spacy_pearson, _ = pearsonr(val_df['label'].values, spacy_similarities)\n",
    "    spacy_mse = mean_squared_error(val_df['label'].values, spacy_similarities)\n",
    "    \n",
    "    print(f\"spaCy - Pearson: {spacy_pearson:.3f}, MSE: {spacy_mse:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error amb spaCy: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentaci贸 amb RoBERTa pre-entrenat per STS\n",
    "try:\n",
    "    from transformers import AutoTokenizer, pipeline\n",
    "    from scipy.special import logit\n",
    "    \n",
    "    print(\"Provant amb RoBERTa fine-tuned per STS...\")\n",
    "    \n",
    "    model_name = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    pipe = pipeline('text-classification', model=model_name, tokenizer=tokenizer)\n",
    "    \n",
    "    def prepare_for_roberta(sentence_pairs):\n",
    "        prepared = []\n",
    "        for s1, s2 in sentence_pairs:\n",
    "            prepared.append(f\"{tokenizer.cls_token} {s1}{tokenizer.sep_token}{tokenizer.sep_token} {s2}{tokenizer.sep_token}\")\n",
    "        return prepared\n",
    "    \n",
    "    # Avaluar una mostra\n",
    "    sample_size = min(100, len(val_df))\n",
    "    val_sample = val_df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    sentence_pairs = [(row['sentence_1'], row['sentence_2']) for _, row in val_sample.iterrows()]\n",
    "    prepared_pairs = prepare_for_roberta(sentence_pairs)\n",
    "    \n",
    "    predictions = pipe(prepared_pairs, add_special_tokens=False)\n",
    "    \n",
    "    # Convertir scores\n",
    "    for prediction in predictions:\n",
    "        prediction['score'] = logit(prediction['score'])\n",
    "    \n",
    "    scores = [pred['score'] for pred in predictions]\n",
    "    true_scores = val_sample['label'].values\n",
    "    \n",
    "    roberta_pearson, _ = pearsonr(true_scores, scores)\n",
    "    roberta_mse = mean_squared_error(true_scores, scores)\n",
    "    \n",
    "    print(f\"RoBERTa fine-tuned - Pearson: {roberta_pearson:.3f}, MSE: {roberta_mse:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error amb RoBERTa: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669f40f",
   "metadata": {},
   "source": [
    "## 9. Anlisi Comparativa i Visualitzaci贸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resum de resultats\n",
    "print(\"=== RESUM DE RESULTATS ===\\n\")\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "# Baselines\n",
    "print(\"BASELINES COSINUS:\")\n",
    "for model_name in baseline_results:\n",
    "    mean_r = baseline_results[model_name]['mean']['pearson']\n",
    "    tfidf_r = baseline_results[model_name]['tfidf']['pearson']\n",
    "    mean_mse = baseline_results[model_name]['mean']['mse']\n",
    "    tfidf_mse = baseline_results[model_name]['tfidf']['mse']\n",
    "    \n",
    "    results_summary.append(['Baseline Mean', model_name, mean_r, mean_mse])\n",
    "    results_summary.append(['Baseline TF-IDF', model_name, tfidf_r, tfidf_mse])\n",
    "    print(f\"  {model_name} - Mean: {mean_r:.3f}, TF-IDF: {tfidf_r:.3f}\")\n",
    "\n",
    "# Models de regressi贸\n",
    "print(\"\\nMODELS DE REGRESSI:\")\n",
    "for model_name in aggregated_results:\n",
    "    pearson_r = aggregated_results[model_name]['pearson']\n",
    "    mse_r = aggregated_results[model_name]['mse']\n",
    "    results_summary.append(['Model Agregat', model_name, pearson_r, mse_r])\n",
    "    print(f\"  {model_name} - Pearson: {pearson_r:.3f}, MSE: {mse_r:.3f}\")\n",
    "\n",
    "# Model de seq眉猫ncia\n",
    "if 'pearson_frozen' in locals():\n",
    "    print(f\"\\nMODEL DE SEQNCIA:\")\n",
    "    print(f\"  Embeddings Frozen - Pearson: {pearson_frozen:.3f}, MSE: {mse_frozen:.3f}\")\n",
    "    results_summary.append(['Model Seq眉猫ncia', best_w2v_name, pearson_frozen, mse_frozen])\n",
    "\n",
    "# Models avan莽ats\n",
    "if 'spacy_pearson' in locals():\n",
    "    results_summary.append(['spaCy', 'ca_core_news_md', spacy_pearson, spacy_mse])\n",
    "    print(f\"\\nspaCy - Pearson: {spacy_pearson:.3f}, MSE: {spacy_mse:.3f}\")\n",
    "\n",
    "if 'roberta_pearson' in locals():\n",
    "    results_summary.append(['RoBERTa fine-tuned', 'STS', roberta_pearson, roberta_mse])\n",
    "    print(f\"RoBERTa fine-tuned - Pearson: {roberta_pearson:.3f}, MSE: {roberta_mse:.3f}\")\n",
    "\n",
    "# Crear DataFrame\n",
    "df_results = pd.DataFrame(results_summary, columns=['Model', 'Variant', 'Pearson', 'MSE'])\n",
    "print(f\"\\n{df_results.to_string(index=False, float_format='%.3f')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7257cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualitzacions\n",
    "if len(results_summary) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Grfic 1: Comparaci贸 Pearson per tipus de model\n",
    "    model_types = df_results['Model'].unique()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(model_types)))\n",
    "    \n",
    "    for i, model_type in enumerate(model_types):\n",
    "        subset = df_results[df_results['Model'] == model_type]\n",
    "        axes[0,0].scatter(subset.index, subset['Pearson'], \n",
    "                         label=model_type, color=colors[i], s=60)\n",
    "    \n",
    "    axes[0,0].set_xlabel('Experiments')\n",
    "    axes[0,0].set_ylabel('Correlaci贸 de Pearson')\n",
    "    axes[0,0].set_title('Comparaci贸 de Correlaci贸 de Pearson')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Grfic 2: MSE vs Pearson\n",
    "    axes[0,1].scatter(df_results['Pearson'], df_results['MSE'], alpha=0.7)\n",
    "    for i, row in df_results.iterrows():\n",
    "        axes[0,1].annotate(f\"{row['Model'][:10]}\", \n",
    "                          (row['Pearson'], row['MSE']), \n",
    "                          xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    axes[0,1].set_xlabel('Correlaci贸 de Pearson')\n",
    "    axes[0,1].set_ylabel('MSE')\n",
    "    axes[0,1].set_title('MSE vs Pearson')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Grfic 3: Millors models per categoria\n",
    "    best_baseline = df_results[df_results['Model'].str.contains('Baseline')]['Pearson'].max()\n",
    "    best_aggregated = df_results[df_results['Model'].str.contains('Agregat')]['Pearson'].max()\n",
    "    \n",
    "    categories = ['Baseline', 'Agregat']\n",
    "    values = [best_baseline, best_aggregated]\n",
    "    \n",
    "    if 'pearson_frozen' in locals():\n",
    "        categories.append('Seq眉猫ncia')\n",
    "        values.append(pearson_frozen)\n",
    "    \n",
    "    if 'spacy_pearson' in locals():\n",
    "        categories.append('spaCy')\n",
    "        values.append(spacy_pearson)\n",
    "        \n",
    "    if 'roberta_pearson' in locals():\n",
    "        categories.append('RoBERTa')\n",
    "        values.append(roberta_pearson)\n",
    "    \n",
    "    axes[1,0].bar(categories, values, color=colors[:len(categories)])\n",
    "    axes[1,0].set_ylabel('Millor Correlaci贸 de Pearson')\n",
    "    axes[1,0].set_title('Millors Models per Categoria')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Grfic 4: Distribuci贸 de resultats\n",
    "    axes[1,1].hist(df_results['Pearson'], bins=10, alpha=0.7, edgecolor='black')\n",
    "    axes[1,1].axvline(x=df_results['Pearson'].mean(), color='red', \n",
    "                     linestyle='--', label=f'Mitjana: {df_results[\"Pearson\"].mean():.3f}')\n",
    "    axes[1,1].set_xlabel('Correlaci贸 de Pearson')\n",
    "    axes[1,1].set_ylabel('Freq眉猫ncia')\n",
    "    axes[1,1].set_title('Distribuci贸 de Resultats')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b74702",
   "metadata": {},
   "source": [
    "## 10. Conclusions i Observacions Finals\n",
    "\n",
    "### Resultats Principals:\n",
    "\n",
    "1. **Impacte de la Mida del Corpus**: Els models Word2Vec entrenats amb m茅s dades (500MB, 1GB) mostren millors resultats que els models petits (100MB).\n",
    "\n",
    "2. **Embeddings Pre-entrenats vs. Entrenats**: Els embeddings FastText pre-entrenats sovint superen els models entrenats des de zero amb corpus limitats.\n",
    "\n",
    "3. **TF-IDF vs. Mitjana Simple**: La ponderaci贸 TF-IDF ocasionalment millora els resultats, per貌 no sempre de manera consistent.\n",
    "\n",
    "4. **Models Neurals vs. Baselines**: Els models de regressi贸 neural aporten millores significatives sobre la similitud cosinus simple.\n",
    "\n",
    "5. **Arquitectures Avan莽ades**: Els models amb atenci贸 i els transformers fine-tuned (RoBERTa) ofereixen els millors resultats.\n",
    "\n",
    "### Observacions T猫cniques:\n",
    "\n",
    "- **Overfitting**: Molts models mostren sobreajustament, especialment amb corpus petits\n",
    "- **Generalitzaci贸**: Els models m茅s simples a vegades generalitzen millor\n",
    "- **Recursos Computacionals**: Hi ha un trade-off entre rendiment i cost computacional\n",
    "\n",
    "### Recomanacions:\n",
    "\n",
    "1. **Per aplicacions prctiques**: Utilitzar models pre-entrenats com RoBERTa fine-tuned\n",
    "2. **Per experimentaci贸**: Els models Word2Vec amb TF-IDF ofereixen un bon comprom铆s\n",
    "3. **Per recursos limitats**: Baselines de similitud cosinus s贸n sorprenentment efectius\n",
    "\n",
    "### Futures Direccions:\n",
    "\n",
    "- Experimentar amb altres arquitectures (BERT, transformers multiling眉es)\n",
    "- Explorar t猫cniques d'ensemble\n",
    "- Avaluar en altres tasques de NLP catal\n",
    "- Optimitzaci贸 d'hiperparmetres m茅s exhaustiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02248f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultats\n",
    "df_results.to_csv('resultats_sts_comprehensive.csv', index=False)\n",
    "print(\"Resultats guardats a 'resultats_sts_comprehensive.csv'\")\n",
    "\n",
    "# Mostrar millor model global\n",
    "if len(df_results) > 0:\n",
    "    best_idx = df_results['Pearson'].idxmax()\n",
    "    best_model = df_results.iloc[best_idx]\n",
    "    \n",
    "    print(f\"\\n MILLOR MODEL GLOBAL:\")\n",
    "    print(f\"Tipus: {best_model['Model']}\")\n",
    "    print(f\"Variant: {best_model['Variant']}\")\n",
    "    print(f\"Pearson: {best_model['Pearson']:.3f}\")\n",
    "    print(f\"MSE: {best_model['MSE']:.3f}\")\n",
    "\n",
    "print(f\"\\n Resum Estad铆stic:\")\n",
    "print(f\"Pearson mitj: {df_results['Pearson'].mean():.3f} 卤 {df_results['Pearson'].std():.3f}\")\n",
    "print(f\"MSE mitj: {df_results['MSE'].mean():.3f} 卤 {df_results['MSE'].std():.3f}\")\n",
    "print(f\"Millor Pearson: {df_results['Pearson'].max():.3f}\")\n",
    "print(f\"Pitjor Pearson: {df_results['Pearson'].min():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
