{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76c3d1c",
   "metadata": {},
   "source": [
    "# Pràctica 4: Similitud de Text Semàntic (STS) per al Català - Anàlisi Comprensiva\n",
    "\n",
    "**Objectius**: \n",
    "- Entrenar models de Word2Vec amb diferents mides de corpus\n",
    "- Implementar i comparar diferents tècniques d'embeddings per a STS\n",
    "- Avaluar models baseline vs. models avançats\n",
    "- Analitzar l'impacte de la mida del corpus i tipus d'embedding\n",
    "\n",
    "## Estructura de la Pràctica:\n",
    "1. **Entrenament de Models Word2Vec** - Diferents mides de corpus\n",
    "2. **Models Baseline** - Similitud cosinus simple vs. TF-IDF\n",
    "3. **Model 1**: Embeddings Agregats - Vectors de frase concatenats\n",
    "4. **Model 2**: Seqüència d'Embeddings - Amb mecanisme d'atenció\n",
    "5. **Experimentació Avançada** - spaCy, RoBERTa, models fine-tuned\n",
    "6. **Anàlisi Comparativa** - Correlació de Pearson i MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d78e80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74960cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessaris\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional, Dict, Union\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Gensim imports\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors, TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuració de GPU (opcional)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef5891",
   "metadata": {},
   "source": [
    "## 1. Carrega del Dataset i Preparació de Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39504fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Carregar datasets\n",
    "print(\"Carregant datasets...\")\n",
    "train_data = load_dataset(\"projecte-aina/sts-ca\", split=\"train\")\n",
    "test_data = load_dataset(\"projecte-aina/sts-ca\", split=\"test\") \n",
    "val_data = load_dataset(\"projecte-aina/sts-ca\", split=\"validation\")\n",
    "\n",
    "# Convertir a DataFrame\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Label range: {train_df['label'].min():.2f} - {train_df['label'].max():.2f}\")\n",
    "\n",
    "# Dataset per entrenar Word2Vec\n",
    "catalan_corpus = load_dataset(\"projecte-aina/catalan_general_crawling\")\n",
    "catalan_dataset = catalan_corpus['train']\n",
    "\n",
    "print(f\"Corpus català: {len(catalan_dataset)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d695485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diferents mides de corpus per experimentar\n",
    "def create_corpus_subsets(dataset):\n",
    "    \"\"\"Crea subsets del corpus de diferents mides\"\"\"\n",
    "    subsets = {}\n",
    "    \n",
    "    # 100MB\n",
    "    size = 0\n",
    "    for i in range(len(dataset)):\n",
    "        size += len(dataset[i]['text'])\n",
    "        if size > 100_000_000:  # 100 MB\n",
    "            break\n",
    "    subsets['100MB'] = dataset.select(list(range(i)))\n",
    "    \n",
    "    # 500MB\n",
    "    size = 0\n",
    "    for j in range(len(dataset)):\n",
    "        size += len(dataset[j]['text'])\n",
    "        if size > 500_000_000:  # 500 MB\n",
    "            break\n",
    "    subsets['500MB'] = dataset.select(list(range(j)))\n",
    "    \n",
    "    # 1GB\n",
    "    size = 0\n",
    "    for k in range(len(dataset)):\n",
    "        size += len(dataset[k]['text'])\n",
    "        if size > 1_000_000_000:  # 1 GB\n",
    "            break\n",
    "    subsets['1GB'] = dataset.select(list(range(k)))\n",
    "    \n",
    "    # Corpus complet (limitat per recursos)\n",
    "    subsets['complet'] = dataset\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "corpus_subsets = create_corpus_subsets(catalan_dataset)\n",
    "print(\"Subsets creats:\", list(corpus_subsets.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af62c0e",
   "metadata": {},
   "source": [
    "## 2. Entrenament de Models Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Stopwords catalans\n",
    "stopwords_catala = [\n",
    "    \"a\", \"abans\", \"ací\", \"així\", \"alguns\", \"alguna\", \"algunes\", \"algú\", \"alhora\", \n",
    "    \"als\", \"allò\", \"aquell\", \"aquelles\", \"aquells\", \"baix\", \"cada\", \"com\", \n",
    "    \"eixa\", \"eixes\", \"eixí\", \"eixos\", \"el\", \"ella\", \"elles\", \"ell\", \"ells\", \n",
    "    \"en\", \"endavant\", \"enfront\", \"ens\", \"entre\", \"he\", \"hem\", \"heu\", \"hi\", \"ho\", \n",
    "    \"i\", \"igual\", \"iguals\", \"ja\", \"jo\", \"l'\", \"la\", \"les\", \"li\", \"els\", \"tu\", \n",
    "    \"nosaltres\", \"vosaltres\", \"de\", \"del\", \"dels\", \"d'un\", \"d'una\", \"des\"\n",
    "]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Neteja i tokenitza el text\"\"\"\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Elimina caràcters no alfanumèrics\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords_catala and len(word) > 2]\n",
    "    return words\n",
    "\n",
    "def create_corpus_for_w2v(dataset):\n",
    "    \"\"\"Crea corpus preprocessat per Word2Vec\"\"\"\n",
    "    corpus = []\n",
    "    for doc in dataset:\n",
    "        words = preprocess_text(doc['text'])\n",
    "        if words:  # Només afegir si no està buit\n",
    "            corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "def train_word2vec(corpus, vector_size=100, window=5, min_count=10, workers=4, epochs=25):\n",
    "    \"\"\"Entrena un model Word2Vec\"\"\"\n",
    "    model = Word2Vec(\n",
    "        corpus, \n",
    "        vector_size=vector_size, \n",
    "        window=window, \n",
    "        min_count=min_count, \n",
    "        workers=workers, \n",
    "        epochs=epochs,\n",
    "        sg=1  # Skip-gram\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar models Word2Vec per cada mida de corpus\n",
    "word2vec_models = {}\n",
    "\n",
    "for corpus_name in [\"100MB\", \"500MB\", \"1GB\"]:  # Evitem 'complet' per recursos\n",
    "    print(f\"\\nEntrenant Word2Vec per {corpus_name}...\")\n",
    "    \n",
    "    # Crear corpus preprocessat\n",
    "    corpus = create_corpus_for_w2v(corpus_subsets[corpus_name])\n",
    "    \n",
    "    # Entrenar model\n",
    "    model = train_word2vec(corpus, vector_size=100)\n",
    "    \n",
    "    # Guardar model\n",
    "    model.save(f\"word2vec_{corpus_name}.model\")\n",
    "    word2vec_models[corpus_name] = model\n",
    "    \n",
    "    print(f\"Model {corpus_name} entrenat amb {len(model.wv.key_to_index)} paraules\")\n",
    "    \n",
    "    # Mostrar exemples de paraules similars\n",
    "    if \"casa\" in model.wv:\n",
    "        similar_words = model.wv.most_similar(\"casa\", topn=5)\n",
    "        print(f\"Paraules similars a 'casa': {similar_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d6f9e",
   "metadata": {},
   "source": [
    "## 3. Carrega d'Embeddings Pre-entrenats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar embeddings FastText pre-entrenats\n",
    "PRETRAINED_PATH = '../cc.ca.300.vec'\n",
    "\n",
    "try:\n",
    "    print(\"Carregant embeddings FastText pre-entrenats...\")\n",
    "    fasttext_model = KeyedVectors.load_word2vec_format(PRETRAINED_PATH, binary=False)\n",
    "    print(f\"FastText carregat: {len(fasttext_model.key_to_index)} paraules, {fasttext_model.vector_size}D\")\n",
    "    word2vec_models['fasttext'] = fasttext_model\n",
    "except FileNotFoundError:\n",
    "    print(\"Embeddings FastText no trobats. Continuant sense ells.\")\n",
    "    fasttext_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8732aa8",
   "metadata": {},
   "source": [
    "## 4. Funcions d'Utilitat per Processament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24191764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence: str) -> List[str]:\n",
    "    \"\"\"Preprocessa una frase per a STS\"\"\"\n",
    "    return simple_preprocess(sentence.lower())\n",
    "\n",
    "def get_sentence_embedding_mean(sentence: str, wv_model, vector_size: int) -> np.ndarray:\n",
    "    \"\"\"Obté embedding de frase mitjançant mitjana simple\"\"\"\n",
    "    words = preprocess_sentence(sentence)\n",
    "    vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if hasattr(wv_model, 'wv'):  # Model Word2Vec\n",
    "            if word in wv_model.wv:\n",
    "                vectors.append(wv_model.wv[word])\n",
    "        else:  # KeyedVectors\n",
    "            if word in wv_model:\n",
    "                vectors.append(wv_model[word])\n",
    "    \n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "def get_sentence_embedding_tfidf(sentence: str, wv_model, tfidf_vectorizer, \n",
    "                                feature_names: List[str], vector_size: int) -> np.ndarray:\n",
    "    \"\"\"Obté embedding de frase amb ponderació TF-IDF\"\"\"\n",
    "    words = preprocess_sentence(sentence)\n",
    "    \n",
    "    # Calcular TF-IDF\n",
    "    tfidf_vector = tfidf_vectorizer.transform([' '.join(words)])\n",
    "    tfidf_scores = tfidf_vector.toarray()[0]\n",
    "    \n",
    "    weighted_vectors = []\n",
    "    weights = []\n",
    "    \n",
    "    for word in words:\n",
    "        word_vector = None\n",
    "        \n",
    "        # Obtenir vector de la paraula\n",
    "        if hasattr(wv_model, 'wv'):\n",
    "            if word in wv_model.wv:\n",
    "                word_vector = wv_model.wv[word]\n",
    "        else:\n",
    "            if word in wv_model:\n",
    "                word_vector = wv_model[word]\n",
    "        \n",
    "        # Aplicar pes TF-IDF\n",
    "        if word_vector is not None and word in feature_names:\n",
    "            word_idx = feature_names.index(word)\n",
    "            weight = tfidf_scores[word_idx]\n",
    "            if weight > 0:\n",
    "                weighted_vectors.append(word_vector * weight)\n",
    "                weights.append(weight)\n",
    "    \n",
    "    if weighted_vectors and sum(weights) > 0:\n",
    "        return np.sum(weighted_vectors, axis=0) / sum(weights)\n",
    "    else:\n",
    "        return np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f42b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dades per avaluació\n",
    "all_sentences = (train_df['sentence_1'].tolist() + train_df['sentence_2'].tolist() + \n",
    "                test_df['sentence_1'].tolist() + test_df['sentence_2'].tolist() + \n",
    "                val_df['sentence_1'].tolist() + val_df['sentence_2'].tolist())\n",
    "\n",
    "# Crear TF-IDF vectorizer\n",
    "corpus_for_tfidf = [' '.join(preprocess_sentence(sent)) for sent in all_sentences]\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, lowercase=True)\n",
    "tfidf_vectorizer.fit(corpus_for_tfidf)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "print(f\"TF-IDF preparat amb {len(feature_names)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95cc93",
   "metadata": {},
   "source": [
    "## 5. Models Baseline: Similitud Cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cosine_baseline(df: pd.DataFrame, wv_model, method='mean') -> Dict[str, float]:\n",
    "    \"\"\"Avalua baseline de similitud cosinus\"\"\"\n",
    "    similarities = []\n",
    "    true_scores = df['label'].values\n",
    "    \n",
    "    vector_size = wv_model.wv.vector_size if hasattr(wv_model, 'wv') else wv_model.vector_size\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        sent1, sent2 = row['sentence_1'], row['sentence_2']\n",
    "        \n",
    "        if method == 'tfidf':\n",
    "            vec1 = get_sentence_embedding_tfidf(sent1, wv_model, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "            vec2 = get_sentence_embedding_tfidf(sent2, wv_model, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "        else:  # mean\n",
    "            vec1 = get_sentence_embedding_mean(sent1, wv_model, vector_size)\n",
    "            vec2 = get_sentence_embedding_mean(sent2, wv_model, vector_size)\n",
    "        \n",
    "        # Similitud cosinus\n",
    "        if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "            sim = 0.0\n",
    "        else:\n",
    "            sim = 1 - cosine(vec1, vec2)\n",
    "        \n",
    "        # Escalar a [0,5]\n",
    "        sim_scaled = (sim + 1) * 2.5\n",
    "        similarities.append(sim_scaled)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    pearson_corr, _ = pearsonr(true_scores, similarities)\n",
    "    mse = mean_squared_error(true_scores, similarities)\n",
    "    mae = mean_absolute_error(true_scores, similarities)\n",
    "    \n",
    "    return {\n",
    "        'pearson': pearson_corr,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'predictions': similarities\n",
    "    }\n",
    "\n",
    "# Avaluar baselines per tots els models\n",
    "baseline_results = {}\n",
    "\n",
    "for model_name, model in word2vec_models.items():\n",
    "    print(f\"\\n=== Avaluant Baseline: {model_name} ===\")\n",
    "    \n",
    "    # Mitjana simple\n",
    "    results_mean = evaluate_cosine_baseline(val_df, model, method='mean')\n",
    "    \n",
    "    # TF-IDF ponderat\n",
    "    results_tfidf = evaluate_cosine_baseline(val_df, model, method='tfidf')\n",
    "    \n",
    "    baseline_results[model_name] = {\n",
    "        'mean': results_mean,\n",
    "        'tfidf': results_tfidf\n",
    "    }\n",
    "    \n",
    "    print(f\"Mitjana Simple - Pearson: {results_mean['pearson']:.3f}, MSE: {results_mean['mse']:.3f}\")\n",
    "    print(f\"TF-IDF - Pearson: {results_tfidf['pearson']:.3f}, MSE: {results_tfidf['mse']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e80cf",
   "metadata": {},
   "source": [
    "## 6. Model 1: Regressió amb Embeddings Agregats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_aggregated_model(embedding_dim: int, hidden_size: int = 128, \n",
    "                         dropout_rate: float = 0.3) -> tf.keras.Model:\n",
    "    \"\"\"Model de regressió amb embeddings agregats\"\"\"\n",
    "    input_1 = tf.keras.Input(shape=(embedding_dim,), name=\"sentence_1\")\n",
    "    input_2 = tf.keras.Input(shape=(embedding_dim,), name=\"sentence_2\")\n",
    "    \n",
    "    # Concatenar embeddings\n",
    "    concatenated = tf.keras.layers.Concatenate(axis=-1)([input_1, input_2])\n",
    "    \n",
    "    # Capes de processament\n",
    "    x = tf.keras.layers.BatchNormalization()(concatenated)\n",
    "    x = tf.keras.layers.Dense(hidden_size, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.Dense(hidden_size // 2, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Sortida (regressió 0-5)\n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_aggregated_data(df: pd.DataFrame, wv_model, method='mean'):\n",
    "    \"\"\"Prepara dades per model agregat\"\"\"\n",
    "    X1, X2, Y = [], [], []\n",
    "    \n",
    "    vector_size = wv_model.wv.vector_size if hasattr(wv_model, 'wv') else wv_model.vector_size\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        sent1, sent2, label = row['sentence_1'], row['sentence_2'], row['label']\n",
    "        \n",
    "        if method == 'tfidf':\n",
    "            vec1 = get_sentence_embedding_tfidf(sent1, wv_model, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "            vec2 = get_sentence_embedding_tfidf(sent2, wv_model, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "        else:\n",
    "            vec1 = get_sentence_embedding_mean(sent1, wv_model, vector_size)\n",
    "            vec2 = get_sentence_embedding_mean(sent2, wv_model, vector_size)\n",
    "        \n",
    "        X1.append(vec1)\n",
    "        X2.append(vec2)\n",
    "        Y.append(label)\n",
    "    \n",
    "    return np.array(X1), np.array(X2), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67dfb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar models agregats\n",
    "aggregated_results = {}\n",
    "\n",
    "for model_name, model in word2vec_models.items():\n",
    "    print(f\"\\n=== Entrenant Model Agregat: {model_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Preparar dades\n",
    "        X1_train, X2_train, Y_train = prepare_aggregated_data(train_df, model)\n",
    "        X1_val, X2_val, Y_val = prepare_aggregated_data(val_df, model)\n",
    "        \n",
    "        vector_size = model.wv.vector_size if hasattr(model, 'wv') else model.vector_size\n",
    "        \n",
    "        # Construir i entrenar model\n",
    "        keras_model = build_aggregated_model(embedding_dim=vector_size)\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True, verbose=0\n",
    "        )\n",
    "        \n",
    "        # Entrenament\n",
    "        history = keras_model.fit(\n",
    "            [X1_train, X2_train], Y_train,\n",
    "            validation_data=([X1_val, X2_val], Y_val),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Avaluació\n",
    "        Y_pred = keras_model.predict([X1_val, X2_val], verbose=0).flatten()\n",
    "        pearson_corr, _ = pearsonr(Y_val, Y_pred)\n",
    "        mse = mean_squared_error(Y_val, Y_pred)\n",
    "        mae = mean_absolute_error(Y_val, Y_pred)\n",
    "        \n",
    "        aggregated_results[model_name] = {\n",
    "            'model': keras_model,\n",
    "            'history': history,\n",
    "            'pearson': pearson_corr,\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'predictions': Y_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"Resultats - Pearson: {pearson_corr:.3f}, MSE: {mse:.3f}, MAE: {mae:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error entrenant {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26697be",
   "metadata": {},
   "source": [
    "## 7. Model 2: Seqüència d'Embeddings amb Atenció"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dc5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Capa d'atenció simple\"\"\"\n",
    "    def __init__(self, units: int = 128, **kwargs):\n",
    "        super(SimpleAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.W1 = tf.keras.layers.Dense(units, activation='tanh')\n",
    "        self.W2 = tf.keras.layers.Dense(1)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Calcular scores d'atenció\n",
    "        ui = self.W1(inputs)\n",
    "        ui = self.dropout(ui)\n",
    "        scores = self.W2(ui)\n",
    "        scores = tf.squeeze(scores, axis=-1)\n",
    "        \n",
    "        # Aplicar màscara\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, dtype=tf.float32)\n",
    "            scores = tf.where(mask, scores, tf.fill(tf.shape(scores), -1e9))\n",
    "        \n",
    "        # Pesos d'atenció\n",
    "        alpha = tf.nn.softmax(scores, axis=-1)\n",
    "        alpha = tf.expand_dims(alpha, axis=-1)\n",
    "        \n",
    "        # Vector de context\n",
    "        context_vector = tf.reduce_sum(alpha * inputs, axis=1)\n",
    "        \n",
    "        return context_vector\n",
    "\n",
    "def build_sequence_model(vocab_size: int, embedding_dim: int, sequence_length: int = 32,\n",
    "                        pretrained_weights: Optional[np.ndarray] = None,\n",
    "                        trainable_embeddings: bool = False) -> tf.keras.Model:\n",
    "    \"\"\"Model de seqüències amb atenció\"\"\"\n",
    "    input_1 = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)\n",
    "    input_2 = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)\n",
    "    \n",
    "    # Capa d'embedding\n",
    "    if pretrained_weights is not None:\n",
    "        embedding_layer = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=sequence_length,\n",
    "            weights=[pretrained_weights],\n",
    "            trainable=trainable_embeddings,\n",
    "            mask_zero=True\n",
    "        )\n",
    "    else:\n",
    "        embedding_layer = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=sequence_length,\n",
    "            trainable=True,\n",
    "            mask_zero=True\n",
    "        )\n",
    "    \n",
    "    # Aplicar embedding\n",
    "    embedded_1 = embedding_layer(input_1)\n",
    "    embedded_2 = embedding_layer(input_2)\n",
    "    \n",
    "    # Atenció\n",
    "    attention_layer = SimpleAttention(units=64)\n",
    "    sentence_vector_1 = attention_layer(embedded_1)\n",
    "    sentence_vector_2 = attention_layer(embedded_2)\n",
    "    \n",
    "    # Projecció i normalització\n",
    "    projection_layer = tf.keras.layers.Dense(embedding_dim, activation='tanh')\n",
    "    dropout = tf.keras.layers.Dropout(0.3)\n",
    "    \n",
    "    projected_1 = dropout(projection_layer(sentence_vector_1))\n",
    "    projected_2 = dropout(projection_layer(sentence_vector_2))\n",
    "    \n",
    "    normalized_1 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1)\n",
    "    )(projected_1)\n",
    "    normalized_2 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1)\n",
    "    )(projected_2)\n",
    "    \n",
    "    # Similitud cosinus escalada\n",
    "    similarity = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.reduce_sum(x[0] * x[1], axis=1, keepdims=True)\n",
    "    )([normalized_1, normalized_2])\n",
    "    \n",
    "    output = tf.keras.layers.Lambda(\n",
    "        lambda x: 2.5 * (1.0 + x)\n",
    "    )(similarity)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f0af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparació de vocabulari i seqüències\n",
    "def create_vocabulary_mapping(sentences: List[str], max_vocab_size: int = 10000):\n",
    "    \"\"\"Crea mapatge de vocabulari\"\"\"\n",
    "    word_counts = {}\n",
    "    for sentence in sentences:\n",
    "        words = preprocess_sentence(sentence)\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    # Ordenar per freqüència\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Crear mapatge\n",
    "    word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    idx_to_word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "    \n",
    "    for word, count in sorted_words[:max_vocab_size-2]:\n",
    "        idx = len(word_to_idx)\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "    \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def sentence_to_sequence(sentence: str, word_to_idx: Dict[str, int], \n",
    "                        max_length: int = 32) -> np.ndarray:\n",
    "    \"\"\"Converteix frase a seqüència d'índexs\"\"\"\n",
    "    words = preprocess_sentence(sentence)\n",
    "    sequence = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            sequence.append(word_to_idx[word])\n",
    "        else:\n",
    "            sequence.append(word_to_idx[\"<UNK>\"])\n",
    "    \n",
    "    # Padding o truncament\n",
    "    if len(sequence) > max_length:\n",
    "        sequence = sequence[:max_length]\n",
    "    else:\n",
    "        sequence.extend([word_to_idx[\"<PAD>\"]] * (max_length - len(sequence)))\n",
    "    \n",
    "    return np.array(sequence)\n",
    "\n",
    "# Crear vocabulari\n",
    "word_to_idx, idx_to_word = create_vocabulary_mapping(all_sentences, max_vocab_size=10000)\n",
    "vocab_size = len(word_to_idx)\n",
    "sequence_length = 32\n",
    "\n",
    "print(f\"Vocabulari creat: {vocab_size} paraules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aaf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar model de seqüències amb el millor Word2Vec\n",
    "if word2vec_models:\n",
    "    best_w2v_name = max(baseline_results.keys(), \n",
    "                       key=lambda k: baseline_results[k]['mean']['pearson'])\n",
    "    best_w2v_model = word2vec_models[best_w2v_name]\n",
    "    \n",
    "    print(f\"Utilitzant millor model Word2Vec: {best_w2v_name}\")\n",
    "    \n",
    "    # Crear matriu d'embeddings pre-entrenats\n",
    "    embedding_dim = best_w2v_model.wv.vector_size if hasattr(best_w2v_model, 'wv') else best_w2v_model.vector_size\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if hasattr(best_w2v_model, 'wv'):\n",
    "            if word in best_w2v_model.wv:\n",
    "                embedding_matrix[idx] = best_w2v_model.wv[word]\n",
    "        else:\n",
    "            if word in best_w2v_model:\n",
    "                embedding_matrix[idx] = best_w2v_model[word]\n",
    "    \n",
    "    # Preparar dades de seqüències\n",
    "    def prepare_sequence_data(df):\n",
    "        X1_seq, X2_seq, Y_seq = [], [], []\n",
    "        for _, row in df.iterrows():\n",
    "            seq1 = sentence_to_sequence(row['sentence_1'], word_to_idx, sequence_length)\n",
    "            seq2 = sentence_to_sequence(row['sentence_2'], word_to_idx, sequence_length)\n",
    "            X1_seq.append(seq1)\n",
    "            X2_seq.append(seq2)\n",
    "            Y_seq.append(row['label'])\n",
    "        return np.array(X1_seq), np.array(X2_seq), np.array(Y_seq)\n",
    "    \n",
    "    X1_train_seq, X2_train_seq, Y_train_seq = prepare_sequence_data(train_df)\n",
    "    X1_val_seq, X2_val_seq, Y_val_seq = prepare_sequence_data(val_df)\n",
    "    \n",
    "    # Model amb embeddings pre-entrenats (frozen)\n",
    "    print(\"\\nEntrenant model amb embeddings frozen...\")\n",
    "    model_seq_frozen = build_sequence_model(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        sequence_length=sequence_length,\n",
    "        pretrained_weights=embedding_matrix,\n",
    "        trainable_embeddings=False\n",
    "    )\n",
    "    \n",
    "    history_frozen = model_seq_frozen.fit(\n",
    "        [X1_train_seq, X2_train_seq], Y_train_seq,\n",
    "        validation_data=([X1_val_seq, X2_val_seq], Y_val_seq),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    Y_pred_frozen = model_seq_frozen.predict([X1_val_seq, X2_val_seq], verbose=0).flatten()\n",
    "    pearson_frozen, _ = pearsonr(Y_val_seq, Y_pred_frozen)\n",
    "    mse_frozen = mean_squared_error(Y_val_seq, Y_pred_frozen)\n",
    "    \n",
    "    print(f\"Model frozen - Pearson: {pearson_frozen:.3f}, MSE: {mse_frozen:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde46859",
   "metadata": {},
   "source": [
    "## 8. Experimentació amb Models Avançats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentació amb spaCy\n",
    "try:\n",
    "    import spacy\n",
    "    \n",
    "    print(\"Provant amb spaCy...\")\n",
    "    nlp = spacy.load(\"ca_core_news_md\")\n",
    "    \n",
    "    def get_spacy_embedding(sentence: str) -> np.ndarray:\n",
    "        doc = nlp(sentence)\n",
    "        return doc.vector\n",
    "    \n",
    "    # Avaluar amb spaCy\n",
    "    spacy_similarities = []\n",
    "    for _, row in val_df.iterrows():\n",
    "        vec1 = get_spacy_embedding(row['sentence_1'])\n",
    "        vec2 = get_spacy_embedding(row['sentence_2'])\n",
    "        \n",
    "        if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "            sim = 0.0\n",
    "        else:\n",
    "            sim = 1 - cosine(vec1, vec2)\n",
    "        \n",
    "        sim_scaled = (sim + 1) * 2.5\n",
    "        spacy_similarities.append(sim_scaled)\n",
    "    \n",
    "    spacy_pearson, _ = pearsonr(val_df['label'].values, spacy_similarities)\n",
    "    spacy_mse = mean_squared_error(val_df['label'].values, spacy_similarities)\n",
    "    \n",
    "    print(f\"spaCy - Pearson: {spacy_pearson:.3f}, MSE: {spacy_mse:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error amb spaCy: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentació amb RoBERTa pre-entrenat per STS\n",
    "try:\n",
    "    from transformers import AutoTokenizer, pipeline\n",
    "    from scipy.special import logit\n",
    "    \n",
    "    print(\"Provant amb RoBERTa fine-tuned per STS...\")\n",
    "    \n",
    "    model_name = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    pipe = pipeline('text-classification', model=model_name, tokenizer=tokenizer)\n",
    "    \n",
    "    def prepare_for_roberta(sentence_pairs):\n",
    "        prepared = []\n",
    "        for s1, s2 in sentence_pairs:\n",
    "            prepared.append(f\"{tokenizer.cls_token} {s1}{tokenizer.sep_token}{tokenizer.sep_token} {s2}{tokenizer.sep_token}\")\n",
    "        return prepared\n",
    "    \n",
    "    # Avaluar una mostra\n",
    "    sample_size = min(100, len(val_df))\n",
    "    val_sample = val_df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    sentence_pairs = [(row['sentence_1'], row['sentence_2']) for _, row in val_sample.iterrows()]\n",
    "    prepared_pairs = prepare_for_roberta(sentence_pairs)\n",
    "    \n",
    "    predictions = pipe(prepared_pairs, add_special_tokens=False)\n",
    "    \n",
    "    # Convertir scores\n",
    "    for prediction in predictions:\n",
    "        prediction['score'] = logit(prediction['score'])\n",
    "    \n",
    "    scores = [pred['score'] for pred in predictions]\n",
    "    true_scores = val_sample['label'].values\n",
    "    \n",
    "    roberta_pearson, _ = pearsonr(true_scores, scores)\n",
    "    roberta_mse = mean_squared_error(true_scores, scores)\n",
    "    \n",
    "    print(f\"RoBERTa fine-tuned - Pearson: {roberta_pearson:.3f}, MSE: {roberta_mse:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error amb RoBERTa: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669f40f",
   "metadata": {},
   "source": [
    "## 9. Anàlisi Comparativa i Visualització"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resum de resultats\n",
    "print(\"=== RESUM DE RESULTATS ===\\n\")\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "# Baselines\n",
    "print(\"BASELINES COSINUS:\")\n",
    "for model_name in baseline_results:\n",
    "    mean_r = baseline_results[model_name]['mean']['pearson']\n",
    "    tfidf_r = baseline_results[model_name]['tfidf']['pearson']\n",
    "    mean_mse = baseline_results[model_name]['mean']['mse']\n",
    "    tfidf_mse = baseline_results[model_name]['tfidf']['mse']\n",
    "    \n",
    "    results_summary.append(['Baseline Mean', model_name, mean_r, mean_mse])\n",
    "    results_summary.append(['Baseline TF-IDF', model_name, tfidf_r, tfidf_mse])\n",
    "    print(f\"  {model_name} - Mean: {mean_r:.3f}, TF-IDF: {tfidf_r:.3f}\")\n",
    "\n",
    "# Models de regressió\n",
    "print(\"\\nMODELS DE REGRESSIÓ:\")\n",
    "for model_name in aggregated_results:\n",
    "    pearson_r = aggregated_results[model_name]['pearson']\n",
    "    mse_r = aggregated_results[model_name]['mse']\n",
    "    results_summary.append(['Model Agregat', model_name, pearson_r, mse_r])\n",
    "    print(f\"  {model_name} - Pearson: {pearson_r:.3f}, MSE: {mse_r:.3f}\")\n",
    "\n",
    "# Model de seqüència\n",
    "if 'pearson_frozen' in locals():\n",
    "    print(f\"\\nMODEL DE SEQÜÈNCIA:\")\n",
    "    print(f\"  Embeddings Frozen - Pearson: {pearson_frozen:.3f}, MSE: {mse_frozen:.3f}\")\n",
    "    results_summary.append(['Model Seqüència', best_w2v_name, pearson_frozen, mse_frozen])\n",
    "\n",
    "# Models avançats\n",
    "if 'spacy_pearson' in locals():\n",
    "    results_summary.append(['spaCy', 'ca_core_news_md', spacy_pearson, spacy_mse])\n",
    "    print(f\"\\nspaCy - Pearson: {spacy_pearson:.3f}, MSE: {spacy_mse:.3f}\")\n",
    "\n",
    "if 'roberta_pearson' in locals():\n",
    "    results_summary.append(['RoBERTa fine-tuned', 'STS', roberta_pearson, roberta_mse])\n",
    "    print(f\"RoBERTa fine-tuned - Pearson: {roberta_pearson:.3f}, MSE: {roberta_mse:.3f}\")\n",
    "\n",
    "# Crear DataFrame\n",
    "df_results = pd.DataFrame(results_summary, columns=['Model', 'Variant', 'Pearson', 'MSE'])\n",
    "print(f\"\\n{df_results.to_string(index=False, float_format='%.3f')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7257cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualitzacions\n",
    "if len(results_summary) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Gràfic 1: Comparació Pearson per tipus de model\n",
    "    model_types = df_results['Model'].unique()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(model_types)))\n",
    "    \n",
    "    for i, model_type in enumerate(model_types):\n",
    "        subset = df_results[df_results['Model'] == model_type]\n",
    "        axes[0,0].scatter(subset.index, subset['Pearson'], \n",
    "                         label=model_type, color=colors[i], s=60)\n",
    "    \n",
    "    axes[0,0].set_xlabel('Experiments')\n",
    "    axes[0,0].set_ylabel('Correlació de Pearson')\n",
    "    axes[0,0].set_title('Comparació de Correlació de Pearson')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gràfic 2: MSE vs Pearson\n",
    "    axes[0,1].scatter(df_results['Pearson'], df_results['MSE'], alpha=0.7)\n",
    "    for i, row in df_results.iterrows():\n",
    "        axes[0,1].annotate(f\"{row['Model'][:10]}\", \n",
    "                          (row['Pearson'], row['MSE']), \n",
    "                          xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    axes[0,1].set_xlabel('Correlació de Pearson')\n",
    "    axes[0,1].set_ylabel('MSE')\n",
    "    axes[0,1].set_title('MSE vs Pearson')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gràfic 3: Millors models per categoria\n",
    "    best_baseline = df_results[df_results['Model'].str.contains('Baseline')]['Pearson'].max()\n",
    "    best_aggregated = df_results[df_results['Model'].str.contains('Agregat')]['Pearson'].max()\n",
    "    \n",
    "    categories = ['Baseline', 'Agregat']\n",
    "    values = [best_baseline, best_aggregated]\n",
    "    \n",
    "    if 'pearson_frozen' in locals():\n",
    "        categories.append('Seqüència')\n",
    "        values.append(pearson_frozen)\n",
    "    \n",
    "    if 'spacy_pearson' in locals():\n",
    "        categories.append('spaCy')\n",
    "        values.append(spacy_pearson)\n",
    "        \n",
    "    if 'roberta_pearson' in locals():\n",
    "        categories.append('RoBERTa')\n",
    "        values.append(roberta_pearson)\n",
    "    \n",
    "    axes[1,0].bar(categories, values, color=colors[:len(categories)])\n",
    "    axes[1,0].set_ylabel('Millor Correlació de Pearson')\n",
    "    axes[1,0].set_title('Millors Models per Categoria')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gràfic 4: Distribució de resultats\n",
    "    axes[1,1].hist(df_results['Pearson'], bins=10, alpha=0.7, edgecolor='black')\n",
    "    axes[1,1].axvline(x=df_results['Pearson'].mean(), color='red', \n",
    "                     linestyle='--', label=f'Mitjana: {df_results[\"Pearson\"].mean():.3f}')\n",
    "    axes[1,1].set_xlabel('Correlació de Pearson')\n",
    "    axes[1,1].set_ylabel('Freqüència')\n",
    "    axes[1,1].set_title('Distribució de Resultats')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b74702",
   "metadata": {},
   "source": [
    "## 10. Conclusions i Observacions Finals\n",
    "\n",
    "### Resultats Principals:\n",
    "\n",
    "1. **Impacte de la Mida del Corpus**: Els models Word2Vec entrenats amb més dades (500MB, 1GB) mostren millors resultats que els models petits (100MB).\n",
    "\n",
    "2. **Embeddings Pre-entrenats vs. Entrenats**: Els embeddings FastText pre-entrenats sovint superen els models entrenats des de zero amb corpus limitats.\n",
    "\n",
    "3. **TF-IDF vs. Mitjana Simple**: La ponderació TF-IDF ocasionalment millora els resultats, però no sempre de manera consistent.\n",
    "\n",
    "4. **Models Neurals vs. Baselines**: Els models de regressió neural aporten millores significatives sobre la similitud cosinus simple.\n",
    "\n",
    "5. **Arquitectures Avançades**: Els models amb atenció i els transformers fine-tuned (RoBERTa) ofereixen els millors resultats.\n",
    "\n",
    "### Observacions Tècniques:\n",
    "\n",
    "- **Overfitting**: Molts models mostren sobreajustament, especialment amb corpus petits\n",
    "- **Generalització**: Els models més simples a vegades generalitzen millor\n",
    "- **Recursos Computacionals**: Hi ha un trade-off entre rendiment i cost computacional\n",
    "\n",
    "### Recomanacions:\n",
    "\n",
    "1. **Per aplicacions pràctiques**: Utilitzar models pre-entrenats com RoBERTa fine-tuned\n",
    "2. **Per experimentació**: Els models Word2Vec amb TF-IDF ofereixen un bon compromís\n",
    "3. **Per recursos limitats**: Baselines de similitud cosinus són sorprenentment efectius\n",
    "\n",
    "### Futures Direccions:\n",
    "\n",
    "- Experimentar amb altres arquitectures (BERT, transformers multilingües)\n",
    "- Explorar tècniques d'ensemble\n",
    "- Avaluar en altres tasques de NLP català\n",
    "- Optimització d'hiperparàmetres més exhaustiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02248f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultats\n",
    "df_results.to_csv('resultats_sts_comprehensive.csv', index=False)\n",
    "print(\"Resultats guardats a 'resultats_sts_comprehensive.csv'\")\n",
    "\n",
    "# Mostrar millor model global\n",
    "if len(df_results) > 0:\n",
    "    best_idx = df_results['Pearson'].idxmax()\n",
    "    best_model = df_results.iloc[best_idx]\n",
    "    \n",
    "    print(f\"\\n🏆 MILLOR MODEL GLOBAL:\")\n",
    "    print(f\"Tipus: {best_model['Model']}\")\n",
    "    print(f\"Variant: {best_model['Variant']}\")\n",
    "    print(f\"Pearson: {best_model['Pearson']:.3f}\")\n",
    "    print(f\"MSE: {best_model['MSE']:.3f}\")\n",
    "\n",
    "print(f\"\\n📊 Resum Estadístic:\")\n",
    "print(f\"Pearson mitjà: {df_results['Pearson'].mean():.3f} ± {df_results['Pearson'].std():.3f}\")\n",
    "print(f\"MSE mitjà: {df_results['MSE'].mean():.3f} ± {df_results['MSE'].std():.3f}\")\n",
    "print(f\"Millor Pearson: {df_results['Pearson'].max():.3f}\")\n",
    "print(f\"Pitjor Pearson: {df_results['Pearson'].min():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
