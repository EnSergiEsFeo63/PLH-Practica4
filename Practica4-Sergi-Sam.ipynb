{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369c3062",
   "metadata": {},
   "source": [
    "Sergi Flores i Sam Brumwell\n",
    "\n",
    "# Pràctica 4: Similitud de Text Semàntic (STS) per al Català\n",
    "\n",
    "**Objectiu**: Aquest treball té com a objectiu principal desenvolupar, implementar i avaluar diferents arquitectures de models per a la tasca de similitud semàntica en català utilitzant el dataset STS-ca del projecte AINA, comparant varies tècniques i arquitectures per tal de veure les seves fortaleses i debilitats i seleccionar la més addient per aquesta tasca.\n",
    "\n",
    "\n",
    "**Mètriques**: L'avaluació dels models es realitza mitjançant la **correlació de Pearson** com a mètrica principal, complementada amb l'error quadràtic mitjà (MSE) i l'error absolut mitjà (MAE). Aquesta aproximació permet mesurar tant la qualitat de l'ordenació de similituds com la precisió numèrica de les prediccions.\n",
    "\n",
    "## Estructura de la Pràctica:\n",
    "1. **Preparació d'Embeddings** - Carregar i truncar embeddings Word2Vec\n",
    "2. **Models Baseline** - Similitud cosinus simple\n",
    "3. **Model 1**: Embeddings Agregats - Vectors de frase concatenats\n",
    "4. **Model 2**: Seqüència d'Embeddings - Amb mecanisme d'atenció\n",
    "5. **Experimentació Avançada** - Comparació amb altres embeddings\n",
    "6. **Conclusions i Observacions** - Comparativa global\n",
    "7. **Entrenament amb dades TECLA** - Entrenar un model de classificació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfce3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "# Imports necessaris\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional, Dict, Union\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Configuració de GPU (opcional)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d1369",
   "metadata": {},
   "source": [
    "## 1. Carrega del Dataset STS-ca\n",
    "\n",
    "Aquí carreguem el dataset \"STS-ca\" (Semantic Textual Similarity for Catalan) del projecte AINA, utilitzant la llibreria `datasets` de Hugging Face. Aquest dataset conté parells de frases en català i una etiqueta numèrica que indica el seu grau de similitud semàntica (normalment en una escala de 0 a 5).\n",
    "\n",
    "El dataset es divideix en tres parts:\n",
    "- `train`: conjunt d'entrenament, utilitzat per ajustar els paràmetres dels models.\n",
    "- `test`: conjunt de prova, utilitzat per a l'avaluació final del model seleccionat.\n",
    "- `validation`: conjunt de validació, utilitzat per monitorar el rendiment durant l'entrenament i per a l'ajust d'hiperparàmetres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91201ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregant dataset STS-ca...\n",
      "Train samples: 2073\n",
      "Test samples: 500\n",
      "Validation samples: 500\n",
      "Label range: 0.00 - 5.00\n",
      "\n",
      "Exemples del dataset:\n",
      "Frase 1: Atorga per primer cop les mencions Encarna Sanahuja a la inclusió de la perspectiva de gènere en docència Universitària\n",
      "Frase 2: Creen la menció M. Encarna Sanahuja a la inclusió de la perspectiva de gènere en docència universitària\n",
      "Similitud: 3.5\n",
      "--------------------------------------------------\n",
      "Frase 1: Finalment, afegiu-hi els bolets que haureu saltat en una paella amb oli i deixeu-ho coure tot junt durant 5 minuts.\n",
      "Frase 2: Finalment, poseu-hi les minipastanagues tallades a dauets, els pèsols, rectifiqueu-ho de sal i deixeu-ho coure tot junt durant un parell de minuts més.\n",
      "Similitud: 1.25\n",
      "--------------------------------------------------\n",
      "Frase 1: El TC suspèn el pla d'acció exterior i de relacions amb la UE de la Generalitat\n",
      "Frase 2: El Constitucional manté la suspensió del pla estratègic d'acció exterior i relacions amb la UE\n",
      "Similitud: 3.67\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Carregar el dataset STS-ca\n",
    "print(\"Carregant dataset STS-ca...\")\n",
    "train_data = load_dataset(\"projecte-aina/sts-ca\", split=\"train\")\n",
    "test_data = load_dataset(\"projecte-aina/sts-ca\", split=\"test\") \n",
    "val_data = load_dataset(\"projecte-aina/sts-ca\", split=\"validation\")\n",
    "\n",
    "# Convertir a DataFrame per facilitar la manipulació\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Label range: {train_df['label'].min():.2f} - {train_df['label'].max():.2f}\")\n",
    "\n",
    "# Mostrar alguns exemples\n",
    "print(\"\\nExemples del dataset:\")\n",
    "for i in range(3):\n",
    "    print(f\"Frase 1: {train_df.iloc[i]['sentence_1']}\")\n",
    "    print(f\"Frase 2: {train_df.iloc[i]['sentence_2']}\")\n",
    "    print(f\"Similitud: {train_df.iloc[i]['label']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721b324",
   "metadata": {},
   "source": [
    "## 2. Preparació d'Embeddings Word2Vec\n",
    "\n",
    "En aquesta secció, carreguem embeddings de paraules pre-entrenats. Utilitzarem el model Word2Vec `cc.ca.300.vec`, que conté vectors de 300 dimensions per a paraules en català."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "643b615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregant model Word2Vec...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m WV_MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcc.ca.300.vec\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCarregant model Word2Vec...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m kv_model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWV_MODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel carregat amb èxit. Vocabulari: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(kv_model\u001b[38;5;241m.\u001b[39mkey_to_index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m paraules\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensió dels vectors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkv_model\u001b[38;5;241m.\u001b[39mvector_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\models\\keyedvectors.py:2069\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2065\u001b[0m         _word2vec_read_binary(\n\u001b[0;32m   2066\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[0;32m   2067\u001b[0m         )\n\u001b[0;32m   2068\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2069\u001b[0m         \u001b[43m_word2vec_read_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(kv):\n\u001b[0;32m   2071\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2073\u001b[0m         kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(kv),\n\u001b[0;32m   2074\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\models\\keyedvectors.py:1975\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1973\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1974\u001b[0m word, weights \u001b[38;5;241m=\u001b[39m _word2vec_line_to_vector(line, datatype, unicode_errors, encoding)\n\u001b[1;32m-> 1975\u001b[0m \u001b[43m_add_word_to_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\models\\keyedvectors.py:1923\u001b[0m, in \u001b[0;36m_add_word_to_kv\u001b[1;34m(kv, counts, word, weights, vocab_size)\u001b[0m\n\u001b[0;32m   1921\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocabulary file is incomplete: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is missing\u001b[39m\u001b[38;5;124m\"\u001b[39m, word)\n\u001b[0;32m   1922\u001b[0m     word_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m \u001b[43mkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_vecattr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\models\\keyedvectors.py:353\u001b[0m, in \u001b[0;36mKeyedVectors.set_vecattr\u001b[1;34m(self, key, attr, val)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_vecattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, attr, val):\n\u001b[0;32m    335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set attribute associated with the given key to value.\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallocate_vecattrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key)\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos[attr][index] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\models\\keyedvectors.py:299\u001b[0m, in \u001b[0;36mKeyedVectors.allocate_vecattrs\u001b[1;34m(self, attrs, types)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_int\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos:\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_int\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_int\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint32)\n\u001b[1;32m--> 299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallocate_vecattrs\u001b[39m(\u001b[38;5;28mself\u001b[39m, attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m    The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;66;03m# with no arguments, adjust lengths of existing vecattr arrays to match length of index_to_key\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Carregar el model Word2Vec pre-entrenat\n",
    "WV_MODEL_PATH = 'cc.ca.300.vec'\n",
    "\n",
    "print(\"Carregant model Word2Vec...\")\n",
    "\n",
    "kv_model = KeyedVectors.load_word2vec_format(WV_MODEL_PATH, binary=False)\n",
    "print(f\"Model carregat amb èxit. Vocabulari: {len(kv_model.key_to_index)} paraules\")\n",
    "print(f\"Dimensió dels vectors: {kv_model.vector_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785bf1d",
   "metadata": {},
   "source": [
    "A continuació mostrem alguns exemples dels embeddings generats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc2b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector per 'casa': [-0.0359 -0.0161 -0.0268  0.0022 -0.0873]... (dim=300)\n",
      "Vector per 'gat': [ 0.0061  0.0675 -0.0248 -0.0541 -0.0722]... (dim=300)\n",
      "Vector per 'aigua': [-0.0031  0.0427 -0.0397  0.0366 -0.0551]... (dim=300)\n",
      "Vector per 'carbassot': [-0.0029 -0.0462 -0.0255 -0.0149 -0.0358]... (dim=300)\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'ús\n",
    "test_words = [\"casa\", \"gat\", \"aigua\", \"carbassot\"]\n",
    "for word in test_words:\n",
    "    if word in kv_model:\n",
    "        print(f\"Vector per '{word}': {kv_model[word][:5]}... (dim={kv_model.vector_size})\")\n",
    "    else:\n",
    "        print(f\"Paraula '{word}' no trobada al vocabulari\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966bb51",
   "metadata": {},
   "source": [
    "\n",
    "#### Truncament d'Embeddings\n",
    "\n",
    "Es defineix la funció `create_truncated_embeddings` per generar versions dels embeddings amb dimensions més petites (50, 100, 150). Això permetrà experimentar com la dimensionalitat dels embeddings afecta el rendiment dels models. El truncament es fa simplement seleccionant les primeres `N` dimensions del vector original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creant embeddings de 50 dimensions...\n",
      "  Completat: 2000000 paraules truncades a 50D\n",
      "Creant embeddings de 100 dimensions...\n",
      "  Completat: 2000000 paraules truncades a 100D\n",
      "Creant embeddings de 150 dimensions...\n",
      "  Completat: 2000000 paraules truncades a 150D\n",
      "Creant embeddings de 300 dimensions...\n",
      "  Completat: 2000000 paraules truncades a 300D\n",
      "\n",
      "Versions d'embeddings disponibles: [50, 100, 150, 300]\n"
     ]
    }
   ],
   "source": [
    "# Funció per truncar embeddings a dimensions més petites\n",
    "def create_truncated_embeddings(kv_model, dimensions: List[int]) -> Dict[int, Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Crea versions truncades dels embeddings amb diferents dimensions\n",
    "    \"\"\"\n",
    "    if kv_model is None:\n",
    "        return {}\n",
    "    \n",
    "    truncated_models = {}\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        print(f\"Creant embeddings de {dim} dimensions...\")\n",
    "        truncated_dict = {}\n",
    "        \n",
    "        for word in kv_model.key_to_index:\n",
    "            original_vector = kv_model[word]\n",
    "            truncated_vector = original_vector[:dim]\n",
    "            truncated_dict[word] = truncated_vector\n",
    "            \n",
    "        truncated_models[dim] = truncated_dict\n",
    "        print(f\"  Completat: {len(truncated_dict)} paraules truncades a {dim}D\")\n",
    "    \n",
    "    return truncated_models\n",
    "\n",
    "# Crear versions truncades\n",
    "dimensions = [50, 100, 150, 300]  # Incloem 300 per consistència\n",
    "if kv_model is not None:\n",
    "    truncated_embeddings = create_truncated_embeddings(kv_model, dimensions)\n",
    "    print(f\"\\nVersions d'embeddings disponibles: {list(truncated_embeddings.keys())}\")\n",
    "else:\n",
    "    truncated_embeddings = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6bd55",
   "metadata": {},
   "source": [
    "Visualitzem algun exemple dels embeddings truncats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833692a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model truncat a 50 dimensions:\n",
      "[-0.0359 -0.0161 -0.0268  0.0022 -0.0873  0.003   0.0992 -0.0075  0.068\n",
      " -0.029   0.0186  0.1191  0.0155  0.0375  0.0158  0.0449 -0.1111  0.0606\n",
      "  0.022   0.0341  0.0304 -0.0182 -0.024   0.1791 -0.0036  0.0754 -0.1102\n",
      "  0.0247  0.0228  0.028   0.0685 -0.0146 -0.087  -0.0444  0.0057  0.0172\n",
      "  0.0022  0.1482  0.029  -0.0377  0.0114 -0.044  -0.0019 -0.0501  0.002\n",
      " -0.1389 -0.0044  0.0512 -0.0065  0.0007]\n",
      "Model truncat a 100 dimensions:\n",
      "[-0.0359 -0.0161 -0.0268  0.0022 -0.0873  0.003   0.0992 -0.0075  0.068\n",
      " -0.029   0.0186  0.1191  0.0155  0.0375  0.0158  0.0449 -0.1111  0.0606\n",
      "  0.022   0.0341  0.0304 -0.0182 -0.024   0.1791 -0.0036  0.0754 -0.1102\n",
      "  0.0247  0.0228  0.028   0.0685 -0.0146 -0.087  -0.0444  0.0057  0.0172\n",
      "  0.0022  0.1482  0.029  -0.0377  0.0114 -0.044  -0.0019 -0.0501  0.002\n",
      " -0.1389 -0.0044  0.0512 -0.0065  0.0007  0.0245  0.0052  0.0003  0.0288\n",
      "  0.0476  0.0015  0.1529 -0.0132  0.025  -0.0274  0.0314  0.0709 -0.0816\n",
      " -0.0661  0.0462  0.0075 -0.0541 -0.0488 -0.0282  0.0264  0.0198  0.0271\n",
      " -0.0073  0.0243  0.0406 -0.0808 -0.0458  0.0353 -0.0387  0.0227 -0.044\n",
      " -0.067   0.0412 -0.0122  0.026  -0.0659 -0.0286 -0.0276  0.0097  0.0809\n",
      " -0.0535  0.158  -0.0912  0.043  -0.0738  0.0319  0.0324 -0.0082  0.085\n",
      " -0.0845]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model truncat a 50 dimensions:\")\n",
    "print(truncated_embeddings[50]['casa'])\n",
    "print(\"Model truncat a 100 dimensions:\")\n",
    "print(truncated_embeddings[100]['casa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44726e0e",
   "metadata": {},
   "source": [
    "## 3. Funcions d'Utilitat per Processament de Text\n",
    "\n",
    "Aquesta secció defineix funcions auxiliars clau per al processament de text i la generació d'embeddings de frases, que seran utilitzades pels diferents models.\n",
    "\n",
    "**Funcions Definides:**\n",
    "\n",
    "1.  `preprocess_sentence`:\n",
    "    *   **Objectiu**: Tokenitza la frase.\n",
    "    *   **Funcionament**: Converteix a minúscules i divideix en paraules amb `simple_preprocess`.\n",
    "\n",
    "2.  `get_sentence_embedding_simple`:\n",
    "    *   **Objectiu**: Crea un embedding de frase mitjançant la mitjana dels embeddings de paraules.\n",
    "    *   **Funcionament**: Preprocessa la frase, obté els vectors de paraules existents del diccionari d'embeddings i en calcula la mitjana. Retorna zeros si no hi ha embeddings.\n",
    "\n",
    "3.  `get_sentence_embedding_tfidf`:\n",
    "    *   **Objectiu**: Crea un embedding de frase amb una mitjana ponderada per TF-IDF dels embeddings de paraules.\n",
    "    *   **Funcionament**: Preprocessa la frase, calcula pesos TF-IDF, i fa una mitjana ponderada dels embeddings de paraules. Retorna zeros si no es poden obtenir vectors ponderats.\n",
    "\n",
    "**Preparació del Vocabulari per TF-IDF:**\n",
    "*   Es recullen totes les frases dels conjunts d'entrenament, validació i prova.\n",
    "*   Es preprocessen per extreure totes les paraules i construir un vocabulari global. Aquest vocabulari serà utilitzat per entrenar el `TfidfVectorizer` més endavant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessant frases del dataset...\n",
      "Total de frases processades: 6146\n",
      "Vocabulari únic: 13125 paraules\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocessa una frase: tokenització simple\n",
    "    \"\"\"\n",
    "    return simple_preprocess(sentence.lower())\n",
    "\n",
    "def get_sentence_embedding_simple(sentence: str, embeddings_dict: Dict[str, np.ndarray], \n",
    "                                vector_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obté l'embedding d'una frase fent la mitjana dels embeddings de les paraules\n",
    "    \"\"\"\n",
    "    words = preprocess_sentence(sentence)\n",
    "    vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in embeddings_dict:\n",
    "            vectors.append(embeddings_dict[word])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "def get_sentence_embedding_tfidf(sentence: str, embeddings_dict: Dict[str, np.ndarray], \n",
    "                               tfidf_vectorizer: TfidfVectorizer, \n",
    "                               feature_names: List[str], vector_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obté l'embedding d'una frase fent la mitjana ponderada amb TF-IDF\n",
    "    \"\"\"\n",
    "    words = preprocess_sentence(sentence)\n",
    "    \n",
    "    # Calcular TF-IDF per a la frase\n",
    "    tfidf_vector = tfidf_vectorizer.transform([' '.join(words)])\n",
    "    tfidf_scores = tfidf_vector.toarray()[0]\n",
    "    \n",
    "    weighted_vectors = []\n",
    "    weights = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in embeddings_dict and word in feature_names:\n",
    "            word_idx = feature_names.index(word)\n",
    "            weight = tfidf_scores[word_idx]\n",
    "            if weight > 0:\n",
    "                weighted_vectors.append(embeddings_dict[word] * weight)\n",
    "                weights.append(weight)\n",
    "    \n",
    "    if weighted_vectors and sum(weights) > 0:\n",
    "        return np.sum(weighted_vectors, axis=0) / sum(weights)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Preprocessar totes les frases del dataset\n",
    "print(\"Preprocessant frases del dataset...\")\n",
    "all_sentences = (train_df['sentence_1'].tolist() + train_df['sentence_2'].tolist() + \n",
    "                test_df['sentence_1'].tolist() + test_df['sentence_2'].tolist() + \n",
    "                val_df['sentence_1'].tolist() + val_df['sentence_2'].tolist())\n",
    "\n",
    "# Crear vocabulari per TF-IDF\n",
    "all_words = []\n",
    "for sentence in all_sentences:\n",
    "    all_words.extend(preprocess_sentence(sentence))\n",
    "\n",
    "print(f\"Total de frases processades: {len(all_sentences)}\")\n",
    "print(f\"Vocabulari únic: {len(set(all_words))} paraules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c61dd3",
   "metadata": {},
   "source": [
    "## 4. Baseline: Similitud Cosinus\n",
    "\n",
    "Aquesta secció implementa un model baseline simple per establir un punt de referència per a la tasca de STS. L'enfocament consisteix a calcular la similitud cosinus directa entre els embeddings de frases, obtinguts mitjançant la mitjana dels embeddings de paraules Word2Vec.\n",
    "\n",
    "**Objectiu i Metodologia:**\n",
    "\n",
    "El baseline serveix per establir una línia base de rendiment que els models més complexos hauran de superar. S'utilitza la similitud cosinus perquè és una mètrica estàndard per mesurar la similitud semàntica entre vectors d'embedding. Es proven dues variants: mitjana simple dels embeddings de paraules i mitjana ponderada per TF-IDF, aquesta última per donar més pes a paraules més discriminatives.\n",
    "\n",
    "Les similituds cosinus (rang `[-1,1]`) s'escalen a `[0,5]` per coincidir amb les etiquetes del dataset STS-ca. S'avaluen múltiples dimensions d'embedding (50D, 100D, 150D, 300D) per analitzar l'impacte de la dimensionalitat.\n",
    "\n",
    "La mètrica principal d'avaluació és la correlació de Pearson, que mesura com de bé les prediccions del model es correlacionen amb les puntuacions de similitud humanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparant TF-IDF vectorizer...\n",
      "Avaluant baselines de similitud cosinus...\n"
     ]
    }
   ],
   "source": [
    "def evaluate_cosine_baseline(df: pd.DataFrame, embeddings_dict: Dict[str, np.ndarray], \n",
    "                           vector_size: int, use_tfidf: bool = False, \n",
    "                           tfidf_vectorizer=None, feature_names=None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Avalua el baseline de similitud cosinus\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    true_scores = df['label'].values\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        sent1, sent2 = row['sentence_1'], row['sentence_2']\n",
    "        \n",
    "        if use_tfidf and tfidf_vectorizer is not None:\n",
    "            vec1 = get_sentence_embedding_tfidf(sent1, embeddings_dict, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "            vec2 = get_sentence_embedding_tfidf(sent2, embeddings_dict, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "        else:\n",
    "            vec1 = get_sentence_embedding_simple(sent1, embeddings_dict, vector_size)\n",
    "            vec2 = get_sentence_embedding_simple(sent2, embeddings_dict, vector_size)\n",
    "        \n",
    "        # Calcular similitud cosinus\n",
    "        if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "            sim = 0.0\n",
    "        else:\n",
    "            sim = 1 - cosine(vec1, vec2)\n",
    "        \n",
    "        # Escalar de [-1,1] a [0,5] per coincidir amb les etiquetes\n",
    "        sim_scaled = (sim + 1) * 2.5\n",
    "        similarities.append(sim_scaled)\n",
    "    \n",
    "    # Calcular mètriques\n",
    "    similarities = np.array(similarities)\n",
    "    pearson_corr, _ = pearsonr(true_scores, similarities)\n",
    "    mse = mean_squared_error(true_scores, similarities)\n",
    "    mae = mean_absolute_error(true_scores, similarities)\n",
    "    \n",
    "    return {\n",
    "        'pearson': pearson_corr,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'predictions': similarities\n",
    "    }\n",
    "\n",
    "# Preparar TF-IDF\n",
    "if kv_model is not None:\n",
    "    print(\"Preparant TF-IDF vectorizer...\")\n",
    "    corpus_for_tfidf = [' '.join(preprocess_sentence(sent)) for sent in all_sentences]\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=10000, lowercase=True)\n",
    "    tfidf_vectorizer.fit(corpus_for_tfidf)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out().tolist()\n",
    "    \n",
    "    print(\"Avaluant baselines de similitud cosinus...\")\n",
    "    \n",
    "    # Avaluar per diferents dimensions\n",
    "    baseline_results = {}\n",
    "    results_list = []\n",
    "    for dim in [50, 100, 150, 300]:\n",
    "        if dim in truncated_embeddings:\n",
    "            # Mitjana simple\n",
    "            results_simple = evaluate_cosine_baseline(\n",
    "                val_df, truncated_embeddings[dim], dim, use_tfidf=False\n",
    "            )\n",
    "\n",
    "            # Mitjana ponderada TF-IDF\n",
    "            results_tfidf = evaluate_cosine_baseline(\n",
    "                val_df, truncated_embeddings[dim], dim, use_tfidf=True,\n",
    "                tfidf_vectorizer=tfidf_vectorizer, feature_names=feature_names\n",
    "            )\n",
    "\n",
    "            baseline_results[dim] = {\n",
    "                'simple': results_simple,\n",
    "                'tfidf': results_tfidf\n",
    "            }\n",
    "\n",
    "            results_list.append({\n",
    "                'Model': 'Baseline Cosinus Simple',\n",
    "                'Dimensions': f'{dim}D',\n",
    "                'Pearson': results_simple['pearson'],\n",
    "                'MSE': results_simple['mse'],\n",
    "                'MAE': results_simple['mae']\n",
    "            })\n",
    "            results_list.append({\n",
    "                'Model': 'Baseline Cosinus TF-IDF',\n",
    "                'Dimensions': f'{dim}D',\n",
    "                'Pearson': results_tfidf['pearson'],\n",
    "                'MSE': results_tfidf['mse'],\n",
    "                'MAE': results_tfidf['mae']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300eea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARACIÓ MODELS BASELINE COSINUS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6f5e7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_6f5e7_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_6f5e7_level0_col1\" class=\"col_heading level0 col1\" >Dimensions</th>\n",
       "      <th id=\"T_6f5e7_level0_col2\" class=\"col_heading level0 col2\" >Pearson</th>\n",
       "      <th id=\"T_6f5e7_level0_col3\" class=\"col_heading level0 col3\" >MSE</th>\n",
       "      <th id=\"T_6f5e7_level0_col4\" class=\"col_heading level0 col4\" >MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_6f5e7_row0_col0\" class=\"data row0 col0\" >Baseline Cosinus Simple</td>\n",
       "      <td id=\"T_6f5e7_row0_col1\" class=\"data row0 col1\" >50D</td>\n",
       "      <td id=\"T_6f5e7_row0_col2\" class=\"data row0 col2\" >0.175156</td>\n",
       "      <td id=\"T_6f5e7_row0_col3\" class=\"data row0 col3\" >6.156660</td>\n",
       "      <td id=\"T_6f5e7_row0_col4\" class=\"data row0 col4\" >2.335413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6f5e7_row1_col0\" class=\"data row1 col0\" >Baseline Cosinus TF-IDF</td>\n",
       "      <td id=\"T_6f5e7_row1_col1\" class=\"data row1 col1\" >50D</td>\n",
       "      <td id=\"T_6f5e7_row1_col2\" class=\"data row1 col2\" >0.288679</td>\n",
       "      <td id=\"T_6f5e7_row1_col3\" class=\"data row1 col3\" >5.862487</td>\n",
       "      <td id=\"T_6f5e7_row1_col4\" class=\"data row1 col4\" >2.279328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6f5e7_row2_col0\" class=\"data row2 col0\" >Baseline Cosinus Simple</td>\n",
       "      <td id=\"T_6f5e7_row2_col1\" class=\"data row2 col1\" >100D</td>\n",
       "      <td id=\"T_6f5e7_row2_col2\" class=\"data row2 col2\" >0.213043</td>\n",
       "      <td id=\"T_6f5e7_row2_col3\" class=\"data row2 col3\" >5.936303</td>\n",
       "      <td id=\"T_6f5e7_row2_col4\" class=\"data row2 col4\" >2.290625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6f5e7_row3_col0\" class=\"data row3 col0\" >Baseline Cosinus TF-IDF</td>\n",
       "      <td id=\"T_6f5e7_row3_col1\" class=\"data row3 col1\" >100D</td>\n",
       "      <td id=\"T_6f5e7_row3_col2\" class=\"data row3 col2\" >0.322226</td>\n",
       "      <td id=\"T_6f5e7_row3_col3\" class=\"data row3 col3\" >5.572608</td>\n",
       "      <td id=\"T_6f5e7_row3_col4\" class=\"data row3 col4\" >2.219603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6f5e7_row4_col0\" class=\"data row4 col0\" >Baseline Cosinus Simple</td>\n",
       "      <td id=\"T_6f5e7_row4_col1\" class=\"data row4 col1\" >150D</td>\n",
       "      <td id=\"T_6f5e7_row4_col2\" class=\"data row4 col2\" >0.241811</td>\n",
       "      <td id=\"T_6f5e7_row4_col3\" class=\"data row4 col3\" >5.704033</td>\n",
       "      <td id=\"T_6f5e7_row4_col4\" class=\"data row4 col4\" >2.242433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6f5e7_row5_col0\" class=\"data row5 col0\" >Baseline Cosinus TF-IDF</td>\n",
       "      <td id=\"T_6f5e7_row5_col1\" class=\"data row5 col1\" >150D</td>\n",
       "      <td id=\"T_6f5e7_row5_col2\" class=\"data row5 col2\" >0.356393</td>\n",
       "      <td id=\"T_6f5e7_row5_col3\" class=\"data row5 col3\" >5.332225</td>\n",
       "      <td id=\"T_6f5e7_row5_col4\" class=\"data row5 col4\" >2.169574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6f5e7_row6_col0\" class=\"data row6 col0\" >Baseline Cosinus Simple</td>\n",
       "      <td id=\"T_6f5e7_row6_col1\" class=\"data row6 col1\" >300D</td>\n",
       "      <td id=\"T_6f5e7_row6_col2\" class=\"data row6 col2\" >0.243586</td>\n",
       "      <td id=\"T_6f5e7_row6_col3\" class=\"data row6 col3\" >5.424708</td>\n",
       "      <td id=\"T_6f5e7_row6_col4\" class=\"data row6 col4\" >2.181605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6f5e7_row7_col0\" class=\"data row7 col0\" >Baseline Cosinus TF-IDF</td>\n",
       "      <td id=\"T_6f5e7_row7_col1\" class=\"data row7 col1\" >300D</td>\n",
       "      <td id=\"T_6f5e7_row7_col2\" class=\"data row7 col2\" >0.354348</td>\n",
       "      <td id=\"T_6f5e7_row7_col3\" class=\"data row7 col3\" >5.084731</td>\n",
       "      <td id=\"T_6f5e7_row7_col4\" class=\"data row7 col4\" >2.115081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fb0d6c9910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n=== COMPARACIÓ MODELS BASELINE COSINUS ===\")\n",
    "df_baseline_results = pd.DataFrame(results_list)\n",
    "display(df_baseline_results.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dea643",
   "metadata": {},
   "source": [
    "### Anàlisi dels Resultats Baseline\n",
    "\n",
    "L'avaluació dels models baseline mostra patrons clars sobre l'impacte de la dimensionalitat i les tècniques de ponderació en la similitud semàntica. Els resultats revelen una millora consistent amb l'augment de dimensions: el model simple passa de 0.175 a 0.244 Pearson (39% de millora) entre 50D i 300D, amb els guanys més significatius concentrats fins a 150D.\n",
    "\n",
    "La ponderació TF-IDF supera sistemàticament la mitjana simple, proporcionant millores del 65% en dimensions baixes i del 47% aproximadament en dimensions mitjanes. Aquesta superioritat consistent demostra la importància de ponderar adequadament les paraules discriminatives per capturar millor la similitud semàntica.\n",
    "\n",
    "La configuració òptima es troba en TF-IDF amb 150D (Pearson: 0.356), oferint el millor equilibri rendiment-eficiència, ja que l'augment a 300D només aporta millores marginals. Tots els models mostren valors MSE consistents al voltant de 5-6, indicant prediccions dins d'un rang raonable.\n",
    "\n",
    "Aquest baseline estableix un **llindar de referència de 0.356 Pearson** que els models més sofisticats hauran de superar per demostrar millores significatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b53801",
   "metadata": {},
   "source": [
    "## 5. Model 1: Regressió amb Embeddings Agregats\n",
    "\n",
    "Aquest primer model neuronal representa un salt qualitatiu respecte als baselines, implementant una arquitectura de xarxa neuronal densa per aprendre relacions complexes entre parells de frases.\n",
    "\n",
    "### Arquitectura i Metodologia\n",
    "\n",
    "L'aproximació dels embeddings agregats es basa en convertir cada frase a un vector de longitud fixa mitjançant la mitjana dels embeddings de les seves paraules. Aquests vectors s'alimenten a una xarxa neuronal que aprèn a predir la similitud semàntica directament des de la representació vectorial de les frases.\n",
    "\n",
    "L'arquitectura del model segueix un disseny jeràrquic amb múltiples capes denses. Primer, els dos vectors de frase es concatenen per formar un vector d'entrada de dimensió `2 × embedding_dim`. Aquest vector passa per capes de normalització per lotes, seguides de capes denses amb activació ReLU i regularització mitjançant dropout. La sortida final és una predicció numèrica de similitud obtinguda amb una capa densa lineal.\n",
    "\n",
    "### Implementació i Entrenament\n",
    "\n",
    "El model incorpora diverses tècniques de regularització per evitar l'overfitting: batch normalization per estabilitzar l'entrenament, dropout amb taxes variables segons la profunditat de la capa, i early stopping basat en la loss de validació. L'optimitzador Adam amb learning rate adaptatiu permet una convergència eficient.\n",
    "\n",
    "Durant l'entrenament, es proven diferents dimensions d'embedding (50D, 100D, 150D, 300D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entrenant Model Agregat 50D ===\n",
      "Forma de les dades: X1_train=(2073, 50), Y_train=(2073,)\n",
      "Rang Y_train: [0.00, 5.00]\n",
      "Epoch 1/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 1.1194 - mae: 1.1343 - root_mean_squared_error: 1.3985 - val_loss: 0.7032 - val_mae: 0.6608 - val_root_mean_squared_error: 0.8524 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9279 - mae: 0.9318 - root_mean_squared_error: 1.1611 - val_loss: 0.6962 - val_mae: 0.6590 - val_root_mean_squared_error: 0.8507 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8378 - mae: 0.8369 - root_mean_squared_error: 1.0403 - val_loss: 0.6939 - val_mae: 0.6751 - val_root_mean_squared_error: 0.8608 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8146 - mae: 0.8139 - root_mean_squared_error: 1.0249 - val_loss: 0.6959 - val_mae: 0.6888 - val_root_mean_squared_error: 0.8764 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7302 - mae: 0.7168 - root_mean_squared_error: 0.9175 - val_loss: 0.6749 - val_mae: 0.6670 - val_root_mean_squared_error: 0.8545 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6881 - mae: 0.6779 - root_mean_squared_error: 0.8616 - val_loss: 0.6674 - val_mae: 0.6688 - val_root_mean_squared_error: 0.8567 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6488 - mae: 0.6359 - root_mean_squared_error: 0.8167 - val_loss: 0.6515 - val_mae: 0.6572 - val_root_mean_squared_error: 0.8451 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6132 - mae: 0.6032 - root_mean_squared_error: 0.7760 - val_loss: 0.6461 - val_mae: 0.6649 - val_root_mean_squared_error: 0.8519 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6111 - mae: 0.6045 - root_mean_squared_error: 0.7928 - val_loss: 0.6380 - val_mae: 0.6641 - val_root_mean_squared_error: 0.8528 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5878 - mae: 0.5919 - root_mean_squared_error: 0.7589 - val_loss: 0.6128 - val_mae: 0.6362 - val_root_mean_squared_error: 0.8246 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5557 - mae: 0.5503 - root_mean_squared_error: 0.7281 - val_loss: 0.6108 - val_mae: 0.6472 - val_root_mean_squared_error: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5560 - mae: 0.5692 - root_mean_squared_error: 0.7414 - val_loss: 0.6003 - val_mae: 0.6445 - val_root_mean_squared_error: 0.8337 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5228 - mae: 0.5320 - root_mean_squared_error: 0.6972 - val_loss: 0.5911 - val_mae: 0.6445 - val_root_mean_squared_error: 0.8307 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5136 - mae: 0.5311 - root_mean_squared_error: 0.6971 - val_loss: 0.5830 - val_mae: 0.6432 - val_root_mean_squared_error: 0.8314 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4919 - mae: 0.5183 - root_mean_squared_error: 0.6675 - val_loss: 0.5711 - val_mae: 0.6380 - val_root_mean_squared_error: 0.8227 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4734 - mae: 0.5011 - root_mean_squared_error: 0.6470 - val_loss: 0.5597 - val_mae: 0.6277 - val_root_mean_squared_error: 0.8176 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4527 - mae: 0.4795 - root_mean_squared_error: 0.6243 - val_loss: 0.5461 - val_mae: 0.6171 - val_root_mean_squared_error: 0.8082 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4555 - mae: 0.5010 - root_mean_squared_error: 0.6426 - val_loss: 0.5433 - val_mae: 0.6255 - val_root_mean_squared_error: 0.8153 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4454 - mae: 0.4883 - root_mean_squared_error: 0.6405 - val_loss: 0.5292 - val_mae: 0.6135 - val_root_mean_squared_error: 0.8028 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4260 - mae: 0.4724 - root_mean_squared_error: 0.6166 - val_loss: 0.5246 - val_mae: 0.6193 - val_root_mean_squared_error: 0.8062 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4147 - mae: 0.4624 - root_mean_squared_error: 0.6118 - val_loss: 0.5161 - val_mae: 0.6129 - val_root_mean_squared_error: 0.8033 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4060 - mae: 0.4600 - root_mean_squared_error: 0.6016 - val_loss: 0.5118 - val_mae: 0.6180 - val_root_mean_squared_error: 0.8069 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3967 - mae: 0.4595 - root_mean_squared_error: 0.5995 - val_loss: 0.4976 - val_mae: 0.6084 - val_root_mean_squared_error: 0.7928 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3737 - mae: 0.4363 - root_mean_squared_error: 0.5652 - val_loss: 0.4948 - val_mae: 0.6113 - val_root_mean_squared_error: 0.7966 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3600 - mae: 0.4189 - root_mean_squared_error: 0.5532 - val_loss: 0.4851 - val_mae: 0.6036 - val_root_mean_squared_error: 0.7918 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3534 - mae: 0.4261 - root_mean_squared_error: 0.5493 - val_loss: 0.4907 - val_mae: 0.6176 - val_root_mean_squared_error: 0.8089 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3569 - mae: 0.4373 - root_mean_squared_error: 0.5751 - val_loss: 0.4848 - val_mae: 0.6183 - val_root_mean_squared_error: 0.8082 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3393 - mae: 0.4183 - root_mean_squared_error: 0.5456 - val_loss: 0.4789 - val_mae: 0.6181 - val_root_mean_squared_error: 0.8082 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3377 - mae: 0.4236 - root_mean_squared_error: 0.5500 - val_loss: 0.4769 - val_mae: 0.6227 - val_root_mean_squared_error: 0.8133 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3144 - mae: 0.3976 - root_mean_squared_error: 0.5119 - val_loss: 0.4744 - val_mae: 0.6275 - val_root_mean_squared_error: 0.8155 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3084 - mae: 0.3947 - root_mean_squared_error: 0.5134 - val_loss: 0.4705 - val_mae: 0.6282 - val_root_mean_squared_error: 0.8183 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3041 - mae: 0.3937 - root_mean_squared_error: 0.5140 - val_loss: 0.4687 - val_mae: 0.6288 - val_root_mean_squared_error: 0.8266 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2926 - mae: 0.3814 - root_mean_squared_error: 0.5003 - val_loss: 0.4606 - val_mae: 0.6220 - val_root_mean_squared_error: 0.8224 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2929 - mae: 0.3978 - root_mean_squared_error: 0.5091 - val_loss: 0.4535 - val_mae: 0.6200 - val_root_mean_squared_error: 0.8171 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2904 - mae: 0.3937 - root_mean_squared_error: 0.5155 - val_loss: 0.4558 - val_mae: 0.6240 - val_root_mean_squared_error: 0.8283 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2738 - mae: 0.3812 - root_mean_squared_error: 0.4861 - val_loss: 0.4494 - val_mae: 0.6212 - val_root_mean_squared_error: 0.8238 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2699 - mae: 0.3747 - root_mean_squared_error: 0.4901 - val_loss: 0.4454 - val_mae: 0.6246 - val_root_mean_squared_error: 0.8231 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2621 - mae: 0.3659 - root_mean_squared_error: 0.4796 - val_loss: 0.4391 - val_mae: 0.6184 - val_root_mean_squared_error: 0.8179 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2650 - mae: 0.3820 - root_mean_squared_error: 0.4960 - val_loss: 0.4395 - val_mae: 0.6256 - val_root_mean_squared_error: 0.8283 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2618 - mae: 0.3768 - root_mean_squared_error: 0.5001 - val_loss: 0.4244 - val_mae: 0.6086 - val_root_mean_squared_error: 0.8075 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2482 - mae: 0.3659 - root_mean_squared_error: 0.4722 - val_loss: 0.4314 - val_mae: 0.6231 - val_root_mean_squared_error: 0.8206 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2331 - mae: 0.3417 - root_mean_squared_error: 0.4467 - val_loss: 0.4263 - val_mae: 0.6188 - val_root_mean_squared_error: 0.8220 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2386 - mae: 0.3572 - root_mean_squared_error: 0.4684 - val_loss: 0.4248 - val_mae: 0.6248 - val_root_mean_squared_error: 0.8270 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2206 - mae: 0.3367 - root_mean_squared_error: 0.4332 - val_loss: 0.4191 - val_mae: 0.6213 - val_root_mean_squared_error: 0.8202 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2279 - mae: 0.3521 - root_mean_squared_error: 0.4574 - val_loss: 0.4146 - val_mae: 0.6219 - val_root_mean_squared_error: 0.8183 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2203 - mae: 0.3482 - root_mean_squared_error: 0.4465 - val_loss: 0.4125 - val_mae: 0.6236 - val_root_mean_squared_error: 0.8237 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2169 - mae: 0.3415 - root_mean_squared_error: 0.4465 - val_loss: 0.4155 - val_mae: 0.6315 - val_root_mean_squared_error: 0.8294 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2172 - mae: 0.3468 - root_mean_squared_error: 0.4536 - val_loss: 0.4039 - val_mae: 0.6165 - val_root_mean_squared_error: 0.8205 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2156 - mae: 0.3487 - root_mean_squared_error: 0.4546 - val_loss: 0.4089 - val_mae: 0.6292 - val_root_mean_squared_error: 0.8326 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2018 - mae: 0.3286 - root_mean_squared_error: 0.4296 - val_loss: 0.3942 - val_mae: 0.6075 - val_root_mean_squared_error: 0.8114 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1951 - mae: 0.3264 - root_mean_squared_error: 0.4178 - val_loss: 0.3962 - val_mae: 0.6155 - val_root_mean_squared_error: 0.8190 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1928 - mae: 0.3224 - root_mean_squared_error: 0.4185 - val_loss: 0.4007 - val_mae: 0.6260 - val_root_mean_squared_error: 0.8261 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1841 - mae: 0.3144 - root_mean_squared_error: 0.4007 - val_loss: 0.3950 - val_mae: 0.6198 - val_root_mean_squared_error: 0.8223 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1798 - mae: 0.3078 - root_mean_squared_error: 0.3963 - val_loss: 0.4056 - val_mae: 0.6438 - val_root_mean_squared_error: 0.8412 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1785 - mae: 0.3096 - root_mean_squared_error: 0.3990 - val_loss: 0.3926 - val_mae: 0.6235 - val_root_mean_squared_error: 0.8249 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1754 - mae: 0.3095 - root_mean_squared_error: 0.3968 - val_loss: 0.3950 - val_mae: 0.6267 - val_root_mean_squared_error: 0.8311 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1709 - mae: 0.2990 - root_mean_squared_error: 0.3915 - val_loss: 0.3917 - val_mae: 0.6297 - val_root_mean_squared_error: 0.8305 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1722 - mae: 0.3117 - root_mean_squared_error: 0.3990 - val_loss: 0.3888 - val_mae: 0.6281 - val_root_mean_squared_error: 0.8262 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1713 - mae: 0.3115 - root_mean_squared_error: 0.4017 - val_loss: 0.3842 - val_mae: 0.6224 - val_root_mean_squared_error: 0.8216 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1654 - mae: 0.2960 - root_mean_squared_error: 0.3927 - val_loss: 0.3856 - val_mae: 0.6302 - val_root_mean_squared_error: 0.8283 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1667 - mae: 0.3042 - root_mean_squared_error: 0.4001 - val_loss: 0.3859 - val_mae: 0.6248 - val_root_mean_squared_error: 0.8284 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1614 - mae: 0.3019 - root_mean_squared_error: 0.3888 - val_loss: 0.3800 - val_mae: 0.6216 - val_root_mean_squared_error: 0.8244 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1526 - mae: 0.2862 - root_mean_squared_error: 0.3687 - val_loss: 0.3801 - val_mae: 0.6272 - val_root_mean_squared_error: 0.8253 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1636 - mae: 0.3034 - root_mean_squared_error: 0.4042 - val_loss: 0.3823 - val_mae: 0.6325 - val_root_mean_squared_error: 0.8318 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1577 - mae: 0.3008 - root_mean_squared_error: 0.3912 - val_loss: 0.3638 - val_mae: 0.6029 - val_root_mean_squared_error: 0.8059 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1533 - mae: 0.2933 - root_mean_squared_error: 0.3816 - val_loss: 0.3652 - val_mae: 0.6116 - val_root_mean_squared_error: 0.8111 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1490 - mae: 0.2921 - root_mean_squared_error: 0.3743 - val_loss: 0.3704 - val_mae: 0.6159 - val_root_mean_squared_error: 0.8211 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1436 - mae: 0.2799 - root_mean_squared_error: 0.3622 - val_loss: 0.3656 - val_mae: 0.6111 - val_root_mean_squared_error: 0.8147 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1523 - mae: 0.3009 - root_mean_squared_error: 0.3889 - val_loss: 0.3685 - val_mae: 0.6201 - val_root_mean_squared_error: 0.8200 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1447 - mae: 0.2842 - root_mean_squared_error: 0.3709 - val_loss: 0.3654 - val_mae: 0.6110 - val_root_mean_squared_error: 0.8168 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1436 - mae: 0.2852 - root_mean_squared_error: 0.3706 - val_loss: 0.3748 - val_mae: 0.6290 - val_root_mean_squared_error: 0.8326 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "\u001b[1m27/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1447 - mae: 0.2866 - root_mean_squared_error: 0.3766\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1444 - mae: 0.2858 - root_mean_squared_error: 0.3759 - val_loss: 0.3658 - val_mae: 0.6193 - val_root_mean_squared_error: 0.8194 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1487 - mae: 0.2959 - root_mean_squared_error: 0.3893 - val_loss: 0.3615 - val_mae: 0.6155 - val_root_mean_squared_error: 0.8128 - learning_rate: 3.0000e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1330 - mae: 0.2668 - root_mean_squared_error: 0.3470 - val_loss: 0.3665 - val_mae: 0.6245 - val_root_mean_squared_error: 0.8223 - learning_rate: 3.0000e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1247 - mae: 0.2558 - root_mean_squared_error: 0.3249 - val_loss: 0.3646 - val_mae: 0.6206 - val_root_mean_squared_error: 0.8209 - learning_rate: 3.0000e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1238 - mae: 0.2515 - root_mean_squared_error: 0.3247 - val_loss: 0.3620 - val_mae: 0.6171 - val_root_mean_squared_error: 0.8194 - learning_rate: 3.0000e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1166 - mae: 0.2358 - root_mean_squared_error: 0.3033 - val_loss: 0.3646 - val_mae: 0.6208 - val_root_mean_squared_error: 0.8258 - learning_rate: 3.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1186 - mae: 0.2403 - root_mean_squared_error: 0.3133 - val_loss: 0.3613 - val_mae: 0.6175 - val_root_mean_squared_error: 0.8199 - learning_rate: 3.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1180 - mae: 0.2429 - root_mean_squared_error: 0.3132 - val_loss: 0.3600 - val_mae: 0.6167 - val_root_mean_squared_error: 0.8183 - learning_rate: 3.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1157 - mae: 0.2382 - root_mean_squared_error: 0.3089 - val_loss: 0.3572 - val_mae: 0.6151 - val_root_mean_squared_error: 0.8154 - learning_rate: 3.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1123 - mae: 0.2341 - root_mean_squared_error: 0.3002 - val_loss: 0.3557 - val_mae: 0.6137 - val_root_mean_squared_error: 0.8152 - learning_rate: 3.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1165 - mae: 0.2423 - root_mean_squared_error: 0.3180 - val_loss: 0.3524 - val_mae: 0.6093 - val_root_mean_squared_error: 0.8106 - learning_rate: 3.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1147 - mae: 0.2439 - root_mean_squared_error: 0.3127 - val_loss: 0.3533 - val_mae: 0.6115 - val_root_mean_squared_error: 0.8128 - learning_rate: 3.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1111 - mae: 0.2360 - root_mean_squared_error: 0.3034 - val_loss: 0.3534 - val_mae: 0.6121 - val_root_mean_squared_error: 0.8148 - learning_rate: 3.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1083 - mae: 0.2297 - root_mean_squared_error: 0.2967 - val_loss: 0.3541 - val_mae: 0.6147 - val_root_mean_squared_error: 0.8177 - learning_rate: 3.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1047 - mae: 0.2211 - root_mean_squared_error: 0.2866 - val_loss: 0.3537 - val_mae: 0.6123 - val_root_mean_squared_error: 0.8201 - learning_rate: 3.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1067 - mae: 0.2318 - root_mean_squared_error: 0.2960 - val_loss: 0.3480 - val_mae: 0.6067 - val_root_mean_squared_error: 0.8119 - learning_rate: 3.0000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1047 - mae: 0.2290 - root_mean_squared_error: 0.2916 - val_loss: 0.3496 - val_mae: 0.6115 - val_root_mean_squared_error: 0.8154 - learning_rate: 3.0000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1055 - mae: 0.2304 - root_mean_squared_error: 0.2967 - val_loss: 0.3463 - val_mae: 0.6053 - val_root_mean_squared_error: 0.8122 - learning_rate: 3.0000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1021 - mae: 0.2204 - root_mean_squared_error: 0.2878 - val_loss: 0.3485 - val_mae: 0.6091 - val_root_mean_squared_error: 0.8158 - learning_rate: 3.0000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1031 - mae: 0.2224 - root_mean_squared_error: 0.2936 - val_loss: 0.3478 - val_mae: 0.6086 - val_root_mean_squared_error: 0.8150 - learning_rate: 3.0000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0988 - mae: 0.2207 - root_mean_squared_error: 0.2805 - val_loss: 0.3493 - val_mae: 0.6113 - val_root_mean_squared_error: 0.8193 - learning_rate: 3.0000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1010 - mae: 0.2237 - root_mean_squared_error: 0.2908 - val_loss: 0.3443 - val_mae: 0.6043 - val_root_mean_squared_error: 0.8120 - learning_rate: 3.0000e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0988 - mae: 0.2187 - root_mean_squared_error: 0.2853 - val_loss: 0.3477 - val_mae: 0.6115 - val_root_mean_squared_error: 0.8179 - learning_rate: 3.0000e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0997 - mae: 0.2241 - root_mean_squared_error: 0.2906 - val_loss: 0.3450 - val_mae: 0.6083 - val_root_mean_squared_error: 0.8169 - learning_rate: 3.0000e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0997 - mae: 0.2275 - root_mean_squared_error: 0.2927 - val_loss: 0.3451 - val_mae: 0.6067 - val_root_mean_squared_error: 0.8161 - learning_rate: 3.0000e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0972 - mae: 0.2188 - root_mean_squared_error: 0.2868 - val_loss: 0.3444 - val_mae: 0.6083 - val_root_mean_squared_error: 0.8157 - learning_rate: 3.0000e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0928 - mae: 0.2119 - root_mean_squared_error: 0.2729 - val_loss: 0.3396 - val_mae: 0.6026 - val_root_mean_squared_error: 0.8072 - learning_rate: 3.0000e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0956 - mae: 0.2200 - root_mean_squared_error: 0.2852 - val_loss: 0.3451 - val_mae: 0.6118 - val_root_mean_squared_error: 0.8169 - learning_rate: 3.0000e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0947 - mae: 0.2175 - root_mean_squared_error: 0.2849 - val_loss: 0.3439 - val_mae: 0.6091 - val_root_mean_squared_error: 0.8163 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 98.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Resultats 50D - Pearson: 0.389, MSE: 0.652, MAE: 0.603\n",
      "Prediccions - Min: 0.877, Max: 4.623, Mean: 2.492\n",
      "Targets - Min: 0.000, Max: 5.000, Mean: 2.575\n",
      "\n",
      "=== Entrenant Model Agregat 100D ===\n",
      "Forma de les dades: X1_train=(2073, 100), Y_train=(2073,)\n",
      "Rang Y_train: [0.00, 5.00]\n",
      "Epoch 1/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 1.2190 - mae: 1.1580 - root_mean_squared_error: 1.4123 - val_loss: 0.7877 - val_mae: 0.6580 - val_root_mean_squared_error: 0.8516 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9916 - mae: 0.9026 - root_mean_squared_error: 1.1402 - val_loss: 0.7780 - val_mae: 0.6540 - val_root_mean_squared_error: 0.8499 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8937 - mae: 0.7977 - root_mean_squared_error: 1.0219 - val_loss: 0.7667 - val_mae: 0.6501 - val_root_mean_squared_error: 0.8480 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8270 - mae: 0.7362 - root_mean_squared_error: 0.9382 - val_loss: 0.7538 - val_mae: 0.6495 - val_root_mean_squared_error: 0.8464 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8166 - mae: 0.7327 - root_mean_squared_error: 0.9452 - val_loss: 0.7402 - val_mae: 0.6439 - val_root_mean_squared_error: 0.8425 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7839 - mae: 0.7094 - root_mean_squared_error: 0.9164 - val_loss: 0.7243 - val_mae: 0.6442 - val_root_mean_squared_error: 0.8378 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7115 - mae: 0.6287 - root_mean_squared_error: 0.8184 - val_loss: 0.7095 - val_mae: 0.6419 - val_root_mean_squared_error: 0.8344 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6974 - mae: 0.6298 - root_mean_squared_error: 0.8098 - val_loss: 0.6959 - val_mae: 0.6312 - val_root_mean_squared_error: 0.8287 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6520 - mae: 0.5836 - root_mean_squared_error: 0.7603 - val_loss: 0.6749 - val_mae: 0.6291 - val_root_mean_squared_error: 0.8168 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6400 - mae: 0.5811 - root_mean_squared_error: 0.7575 - val_loss: 0.6649 - val_mae: 0.6354 - val_root_mean_squared_error: 0.8214 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6071 - mae: 0.5547 - root_mean_squared_error: 0.7203 - val_loss: 0.6479 - val_mae: 0.6219 - val_root_mean_squared_error: 0.8090 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5732 - mae: 0.5232 - root_mean_squared_error: 0.6782 - val_loss: 0.6339 - val_mae: 0.6150 - val_root_mean_squared_error: 0.8041 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5600 - mae: 0.5242 - root_mean_squared_error: 0.6749 - val_loss: 0.6302 - val_mae: 0.6314 - val_root_mean_squared_error: 0.8183 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5368 - mae: 0.4981 - root_mean_squared_error: 0.6551 - val_loss: 0.6186 - val_mae: 0.6286 - val_root_mean_squared_error: 0.8168 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5106 - mae: 0.4757 - root_mean_squared_error: 0.6238 - val_loss: 0.6063 - val_mae: 0.6267 - val_root_mean_squared_error: 0.8143 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4976 - mae: 0.4739 - root_mean_squared_error: 0.6162 - val_loss: 0.5974 - val_mae: 0.6250 - val_root_mean_squared_error: 0.8135 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4845 - mae: 0.4727 - root_mean_squared_error: 0.6077 - val_loss: 0.5848 - val_mae: 0.6198 - val_root_mean_squared_error: 0.8079 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4593 - mae: 0.4438 - root_mean_squared_error: 0.5777 - val_loss: 0.5690 - val_mae: 0.6093 - val_root_mean_squared_error: 0.7973 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4556 - mae: 0.4509 - root_mean_squared_error: 0.5951 - val_loss: 0.5542 - val_mae: 0.5989 - val_root_mean_squared_error: 0.7847 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4423 - mae: 0.4439 - root_mean_squared_error: 0.5810 - val_loss: 0.5530 - val_mae: 0.6137 - val_root_mean_squared_error: 0.7978 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4112 - mae: 0.4069 - root_mean_squared_error: 0.5406 - val_loss: 0.5382 - val_mae: 0.5998 - val_root_mean_squared_error: 0.7860 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4072 - mae: 0.4224 - root_mean_squared_error: 0.5436 - val_loss: 0.5290 - val_mae: 0.5978 - val_root_mean_squared_error: 0.7836 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3981 - mae: 0.4173 - root_mean_squared_error: 0.5423 - val_loss: 0.5296 - val_mae: 0.6091 - val_root_mean_squared_error: 0.7981 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3816 - mae: 0.4049 - root_mean_squared_error: 0.5270 - val_loss: 0.5179 - val_mae: 0.6078 - val_root_mean_squared_error: 0.7910 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3615 - mae: 0.3835 - root_mean_squared_error: 0.4987 - val_loss: 0.5029 - val_mae: 0.5945 - val_root_mean_squared_error: 0.7803 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3564 - mae: 0.3919 - root_mean_squared_error: 0.5028 - val_loss: 0.4965 - val_mae: 0.5952 - val_root_mean_squared_error: 0.7801 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3423 - mae: 0.3790 - root_mean_squared_error: 0.4912 - val_loss: 0.4957 - val_mae: 0.6056 - val_root_mean_squared_error: 0.7903 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3261 - mae: 0.3603 - root_mean_squared_error: 0.4729 - val_loss: 0.4901 - val_mae: 0.6067 - val_root_mean_squared_error: 0.7929 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3191 - mae: 0.3612 - root_mean_squared_error: 0.4708 - val_loss: 0.4840 - val_mae: 0.6072 - val_root_mean_squared_error: 0.7920 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3052 - mae: 0.3470 - root_mean_squared_error: 0.4533 - val_loss: 0.4863 - val_mae: 0.6213 - val_root_mean_squared_error: 0.8066 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2996 - mae: 0.3497 - root_mean_squared_error: 0.4540 - val_loss: 0.4677 - val_mae: 0.6100 - val_root_mean_squared_error: 0.7853 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2849 - mae: 0.3377 - root_mean_squared_error: 0.4333 - val_loss: 0.4719 - val_mae: 0.6201 - val_root_mean_squared_error: 0.8038 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2832 - mae: 0.3435 - root_mean_squared_error: 0.4476 - val_loss: 0.4601 - val_mae: 0.6067 - val_root_mean_squared_error: 0.7945 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2779 - mae: 0.3408 - root_mean_squared_error: 0.4456 - val_loss: 0.4552 - val_mae: 0.6096 - val_root_mean_squared_error: 0.7944 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2639 - mae: 0.3277 - root_mean_squared_error: 0.4255 - val_loss: 0.4448 - val_mae: 0.6031 - val_root_mean_squared_error: 0.7884 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2569 - mae: 0.3242 - root_mean_squared_error: 0.4224 - val_loss: 0.4436 - val_mae: 0.6085 - val_root_mean_squared_error: 0.7967 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2469 - mae: 0.3198 - root_mean_squared_error: 0.4082 - val_loss: 0.4470 - val_mae: 0.6192 - val_root_mean_squared_error: 0.8100 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2407 - mae: 0.3150 - root_mean_squared_error: 0.4070 - val_loss: 0.4554 - val_mae: 0.6337 - val_root_mean_squared_error: 0.8373 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2344 - mae: 0.3121 - root_mean_squared_error: 0.4019 - val_loss: 0.4314 - val_mae: 0.6089 - val_root_mean_squared_error: 0.7987 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2369 - mae: 0.3252 - root_mean_squared_error: 0.4224 - val_loss: 0.4319 - val_mae: 0.6139 - val_root_mean_squared_error: 0.8102 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2281 - mae: 0.3150 - root_mean_squared_error: 0.4081 - val_loss: 0.4209 - val_mae: 0.6039 - val_root_mean_squared_error: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2155 - mae: 0.2972 - root_mean_squared_error: 0.3877 - val_loss: 0.4212 - val_mae: 0.6137 - val_root_mean_squared_error: 0.8065 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2106 - mae: 0.3018 - root_mean_squared_error: 0.3857 - val_loss: 0.4162 - val_mae: 0.6071 - val_root_mean_squared_error: 0.8086 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2061 - mae: 0.3029 - root_mean_squared_error: 0.3839 - val_loss: 0.4116 - val_mae: 0.6090 - val_root_mean_squared_error: 0.8055 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2033 - mae: 0.2984 - root_mean_squared_error: 0.3876 - val_loss: 0.4049 - val_mae: 0.6075 - val_root_mean_squared_error: 0.7949 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2014 - mae: 0.3053 - root_mean_squared_error: 0.3901 - val_loss: 0.4114 - val_mae: 0.6177 - val_root_mean_squared_error: 0.8173 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1943 - mae: 0.2965 - root_mean_squared_error: 0.3806 - val_loss: 0.4004 - val_mae: 0.6054 - val_root_mean_squared_error: 0.8081 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1926 - mae: 0.2955 - root_mean_squared_error: 0.3857 - val_loss: 0.4025 - val_mae: 0.6162 - val_root_mean_squared_error: 0.8106 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1905 - mae: 0.2972 - root_mean_squared_error: 0.3871 - val_loss: 0.3857 - val_mae: 0.5989 - val_root_mean_squared_error: 0.7884 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1798 - mae: 0.2855 - root_mean_squared_error: 0.3662 - val_loss: 0.3819 - val_mae: 0.5944 - val_root_mean_squared_error: 0.7837 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1787 - mae: 0.2844 - root_mean_squared_error: 0.3709 - val_loss: 0.3837 - val_mae: 0.6067 - val_root_mean_squared_error: 0.7979 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1732 - mae: 0.2817 - root_mean_squared_error: 0.3634 - val_loss: 0.3803 - val_mae: 0.5973 - val_root_mean_squared_error: 0.7937 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1670 - mae: 0.2745 - root_mean_squared_error: 0.3535 - val_loss: 0.3789 - val_mae: 0.6046 - val_root_mean_squared_error: 0.7952 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1667 - mae: 0.2809 - root_mean_squared_error: 0.3606 - val_loss: 0.3792 - val_mae: 0.6069 - val_root_mean_squared_error: 0.7999 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1615 - mae: 0.2731 - root_mean_squared_error: 0.3520 - val_loss: 0.3768 - val_mae: 0.6048 - val_root_mean_squared_error: 0.7999 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1599 - mae: 0.2760 - root_mean_squared_error: 0.3541 - val_loss: 0.3689 - val_mae: 0.5962 - val_root_mean_squared_error: 0.7920 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1601 - mae: 0.2776 - root_mean_squared_error: 0.3608 - val_loss: 0.3701 - val_mae: 0.6045 - val_root_mean_squared_error: 0.7978 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1544 - mae: 0.2721 - root_mean_squared_error: 0.3501 - val_loss: 0.3837 - val_mae: 0.6252 - val_root_mean_squared_error: 0.8242 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1549 - mae: 0.2770 - root_mean_squared_error: 0.3575 - val_loss: 0.3620 - val_mae: 0.5981 - val_root_mean_squared_error: 0.7881 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1497 - mae: 0.2668 - root_mean_squared_error: 0.3467 - val_loss: 0.3586 - val_mae: 0.5943 - val_root_mean_squared_error: 0.7850 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1474 - mae: 0.2677 - root_mean_squared_error: 0.3469 - val_loss: 0.3568 - val_mae: 0.5912 - val_root_mean_squared_error: 0.7853 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1442 - mae: 0.2656 - root_mean_squared_error: 0.3411 - val_loss: 0.3579 - val_mae: 0.5966 - val_root_mean_squared_error: 0.7902 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1448 - mae: 0.2697 - root_mean_squared_error: 0.3485 - val_loss: 0.3581 - val_mae: 0.5967 - val_root_mean_squared_error: 0.7963 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1449 - mae: 0.2752 - root_mean_squared_error: 0.3544 - val_loss: 0.3615 - val_mae: 0.6045 - val_root_mean_squared_error: 0.7992 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1429 - mae: 0.2730 - root_mean_squared_error: 0.3520 - val_loss: 0.3609 - val_mae: 0.6077 - val_root_mean_squared_error: 0.8060 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1373 - mae: 0.2630 - root_mean_squared_error: 0.3397 - val_loss: 0.3562 - val_mae: 0.6035 - val_root_mean_squared_error: 0.7968 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1335 - mae: 0.2543 - root_mean_squared_error: 0.3309 - val_loss: 0.3496 - val_mae: 0.5895 - val_root_mean_squared_error: 0.7882 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1329 - mae: 0.2600 - root_mean_squared_error: 0.3334 - val_loss: 0.3496 - val_mae: 0.5958 - val_root_mean_squared_error: 0.7891 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1337 - mae: 0.2613 - root_mean_squared_error: 0.3412 - val_loss: 0.3477 - val_mae: 0.5901 - val_root_mean_squared_error: 0.7853 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1352 - mae: 0.2674 - root_mean_squared_error: 0.3475 - val_loss: 0.3407 - val_mae: 0.5868 - val_root_mean_squared_error: 0.7792 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1310 - mae: 0.2661 - root_mean_squared_error: 0.3383 - val_loss: 0.3410 - val_mae: 0.5921 - val_root_mean_squared_error: 0.7765 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1321 - mae: 0.2635 - root_mean_squared_error: 0.3439 - val_loss: 0.3517 - val_mae: 0.6024 - val_root_mean_squared_error: 0.7989 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1389 - mae: 0.2757 - root_mean_squared_error: 0.3682 - val_loss: 0.3439 - val_mae: 0.5961 - val_root_mean_squared_error: 0.7870 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1390 - mae: 0.2732 - root_mean_squared_error: 0.3715 - val_loss: 0.3464 - val_mae: 0.6010 - val_root_mean_squared_error: 0.7922 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1260 - mae: 0.2535 - root_mean_squared_error: 0.3304 - val_loss: 0.3503 - val_mae: 0.6006 - val_root_mean_squared_error: 0.8017 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1226 - mae: 0.2468 - root_mean_squared_error: 0.3224 - val_loss: 0.3495 - val_mae: 0.6021 - val_root_mean_squared_error: 0.8056 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "\u001b[1m21/33\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1268 - mae: 0.2595 - root_mean_squared_error: 0.3382 \n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1263 - mae: 0.2580 - root_mean_squared_error: 0.3371 - val_loss: 0.3534 - val_mae: 0.6133 - val_root_mean_squared_error: 0.8115 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1163 - mae: 0.2367 - root_mean_squared_error: 0.3071 - val_loss: 0.3455 - val_mae: 0.6014 - val_root_mean_squared_error: 0.7996 - learning_rate: 3.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1087 - mae: 0.2224 - root_mean_squared_error: 0.2842 - val_loss: 0.3392 - val_mae: 0.5926 - val_root_mean_squared_error: 0.7905 - learning_rate: 3.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1061 - mae: 0.2114 - root_mean_squared_error: 0.2788 - val_loss: 0.3447 - val_mae: 0.5990 - val_root_mean_squared_error: 0.8018 - learning_rate: 3.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1059 - mae: 0.2165 - root_mean_squared_error: 0.2819 - val_loss: 0.3433 - val_mae: 0.6012 - val_root_mean_squared_error: 0.8006 - learning_rate: 3.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1004 - mae: 0.2076 - root_mean_squared_error: 0.2650 - val_loss: 0.3399 - val_mae: 0.5971 - val_root_mean_squared_error: 0.7963 - learning_rate: 3.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1003 - mae: 0.2075 - root_mean_squared_error: 0.2686 - val_loss: 0.3383 - val_mae: 0.5980 - val_root_mean_squared_error: 0.7944 - learning_rate: 3.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0993 - mae: 0.2072 - root_mean_squared_error: 0.2683 - val_loss: 0.3344 - val_mae: 0.5930 - val_root_mean_squared_error: 0.7894 - learning_rate: 3.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0944 - mae: 0.1969 - root_mean_squared_error: 0.2530 - val_loss: 0.3342 - val_mae: 0.5925 - val_root_mean_squared_error: 0.7915 - learning_rate: 3.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0957 - mae: 0.2020 - root_mean_squared_error: 0.2616 - val_loss: 0.3345 - val_mae: 0.5946 - val_root_mean_squared_error: 0.7931 - learning_rate: 3.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0919 - mae: 0.1936 - root_mean_squared_error: 0.2504 - val_loss: 0.3353 - val_mae: 0.5984 - val_root_mean_squared_error: 0.7956 - learning_rate: 3.0000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0899 - mae: 0.1935 - root_mean_squared_error: 0.2463 - val_loss: 0.3354 - val_mae: 0.5996 - val_root_mean_squared_error: 0.7977 - learning_rate: 3.0000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0902 - mae: 0.1949 - root_mean_squared_error: 0.2513 - val_loss: 0.3328 - val_mae: 0.5983 - val_root_mean_squared_error: 0.7943 - learning_rate: 3.0000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0901 - mae: 0.1908 - root_mean_squared_error: 0.2544 - val_loss: 0.3329 - val_mae: 0.5983 - val_root_mean_squared_error: 0.7967 - learning_rate: 3.0000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0876 - mae: 0.1911 - root_mean_squared_error: 0.2475 - val_loss: 0.3332 - val_mae: 0.5997 - val_root_mean_squared_error: 0.7979 - learning_rate: 3.0000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0861 - mae: 0.1891 - root_mean_squared_error: 0.2448 - val_loss: 0.3327 - val_mae: 0.5979 - val_root_mean_squared_error: 0.7983 - learning_rate: 3.0000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0831 - mae: 0.1833 - root_mean_squared_error: 0.2360 - val_loss: 0.3319 - val_mae: 0.6003 - val_root_mean_squared_error: 0.7973 - learning_rate: 3.0000e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0852 - mae: 0.1916 - root_mean_squared_error: 0.2481 - val_loss: 0.3281 - val_mae: 0.5963 - val_root_mean_squared_error: 0.7923 - learning_rate: 3.0000e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0820 - mae: 0.1844 - root_mean_squared_error: 0.2386 - val_loss: 0.3258 - val_mae: 0.5940 - val_root_mean_squared_error: 0.7902 - learning_rate: 3.0000e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0803 - mae: 0.1833 - root_mean_squared_error: 0.2346 - val_loss: 0.3239 - val_mae: 0.5937 - val_root_mean_squared_error: 0.7876 - learning_rate: 3.0000e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0826 - mae: 0.1933 - root_mean_squared_error: 0.2476 - val_loss: 0.3238 - val_mae: 0.5942 - val_root_mean_squared_error: 0.7909 - learning_rate: 3.0000e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0790 - mae: 0.1817 - root_mean_squared_error: 0.2356 - val_loss: 0.3233 - val_mae: 0.5945 - val_root_mean_squared_error: 0.7905 - learning_rate: 3.0000e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0833 - mae: 0.1989 - root_mean_squared_error: 0.2562 - val_loss: 0.3199 - val_mae: 0.5915 - val_root_mean_squared_error: 0.7860 - learning_rate: 3.0000e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0763 - mae: 0.1777 - root_mean_squared_error: 0.2305 - val_loss: 0.3220 - val_mae: 0.5946 - val_root_mean_squared_error: 0.7920 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 99.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Resultats 100D - Pearson: 0.418, MSE: 0.618, MAE: 0.592\n",
      "Prediccions - Min: 1.054, Max: 4.296, Mean: 2.526\n",
      "Targets - Min: 0.000, Max: 5.000, Mean: 2.575\n",
      "\n",
      "=== Entrenant Model Agregat 150D ===\n",
      "Forma de les dades: X1_train=(2073, 150), Y_train=(2073,)\n",
      "Rang Y_train: [0.00, 5.00]\n",
      "Epoch 1/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.2718 - mae: 1.1607 - root_mean_squared_error: 1.4215 - val_loss: 0.8313 - val_mae: 0.6596 - val_root_mean_squared_error: 0.8534 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0113 - mae: 0.8810 - root_mean_squared_error: 1.1026 - val_loss: 0.8153 - val_mae: 0.6540 - val_root_mean_squared_error: 0.8491 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8946 - mae: 0.7619 - root_mean_squared_error: 0.9654 - val_loss: 0.8000 - val_mae: 0.6515 - val_root_mean_squared_error: 0.8484 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8596 - mae: 0.7338 - root_mean_squared_error: 0.9339 - val_loss: 0.7792 - val_mae: 0.6533 - val_root_mean_squared_error: 0.8440 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7890 - mae: 0.6745 - root_mean_squared_error: 0.8519 - val_loss: 0.7620 - val_mae: 0.6420 - val_root_mean_squared_error: 0.8384 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7332 - mae: 0.6192 - root_mean_squared_error: 0.7958 - val_loss: 0.7467 - val_mae: 0.6543 - val_root_mean_squared_error: 0.8426 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7209 - mae: 0.6221 - root_mean_squared_error: 0.7953 - val_loss: 0.7297 - val_mae: 0.6439 - val_root_mean_squared_error: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6646 - mae: 0.5652 - root_mean_squared_error: 0.7298 - val_loss: 0.7187 - val_mae: 0.6393 - val_root_mean_squared_error: 0.8391 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6314 - mae: 0.5373 - root_mean_squared_error: 0.6960 - val_loss: 0.7039 - val_mae: 0.6325 - val_root_mean_squared_error: 0.8354 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6230 - mae: 0.5460 - root_mean_squared_error: 0.7025 - val_loss: 0.6767 - val_mae: 0.6177 - val_root_mean_squared_error: 0.8146 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5761 - mae: 0.4992 - root_mean_squared_error: 0.6430 - val_loss: 0.6635 - val_mae: 0.6171 - val_root_mean_squared_error: 0.8143 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5655 - mae: 0.5021 - root_mean_squared_error: 0.6521 - val_loss: 0.6425 - val_mae: 0.6179 - val_root_mean_squared_error: 0.8051 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5504 - mae: 0.4893 - root_mean_squared_error: 0.6499 - val_loss: 0.6262 - val_mae: 0.6163 - val_root_mean_squared_error: 0.8006 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4973 - mae: 0.4319 - root_mean_squared_error: 0.5653 - val_loss: 0.6147 - val_mae: 0.6192 - val_root_mean_squared_error: 0.8022 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4851 - mae: 0.4351 - root_mean_squared_error: 0.5651 - val_loss: 0.6026 - val_mae: 0.6162 - val_root_mean_squared_error: 0.8013 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4710 - mae: 0.4277 - root_mean_squared_error: 0.5609 - val_loss: 0.5901 - val_mae: 0.6205 - val_root_mean_squared_error: 0.7986 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4536 - mae: 0.4205 - root_mean_squared_error: 0.5479 - val_loss: 0.5836 - val_mae: 0.6271 - val_root_mean_squared_error: 0.8051 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4275 - mae: 0.3977 - root_mean_squared_error: 0.5161 - val_loss: 0.5635 - val_mae: 0.6113 - val_root_mean_squared_error: 0.7860 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4077 - mae: 0.3832 - root_mean_squared_error: 0.4972 - val_loss: 0.5548 - val_mae: 0.6110 - val_root_mean_squared_error: 0.7893 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3994 - mae: 0.3869 - root_mean_squared_error: 0.5002 - val_loss: 0.5481 - val_mae: 0.6126 - val_root_mean_squared_error: 0.7940 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3815 - mae: 0.3740 - root_mean_squared_error: 0.4849 - val_loss: 0.5356 - val_mae: 0.6082 - val_root_mean_squared_error: 0.7908 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3781 - mae: 0.3837 - root_mean_squared_error: 0.4969 - val_loss: 0.5174 - val_mae: 0.5988 - val_root_mean_squared_error: 0.7775 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3564 - mae: 0.3604 - root_mean_squared_error: 0.4719 - val_loss: 0.5161 - val_mae: 0.6076 - val_root_mean_squared_error: 0.7895 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3454 - mae: 0.3510 - root_mean_squared_error: 0.4662 - val_loss: 0.5119 - val_mae: 0.6158 - val_root_mean_squared_error: 0.7950 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3351 - mae: 0.3534 - root_mean_squared_error: 0.4604 - val_loss: 0.5006 - val_mae: 0.6092 - val_root_mean_squared_error: 0.7927 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3200 - mae: 0.3443 - root_mean_squared_error: 0.4449 - val_loss: 0.4954 - val_mae: 0.6129 - val_root_mean_squared_error: 0.7959 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3134 - mae: 0.3496 - root_mean_squared_error: 0.4463 - val_loss: 0.4946 - val_mae: 0.6231 - val_root_mean_squared_error: 0.8105 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2972 - mae: 0.3328 - root_mean_squared_error: 0.4254 - val_loss: 0.4777 - val_mae: 0.6076 - val_root_mean_squared_error: 0.7911 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2877 - mae: 0.3254 - root_mean_squared_error: 0.4203 - val_loss: 0.4753 - val_mae: 0.6155 - val_root_mean_squared_error: 0.7949 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2772 - mae: 0.3202 - root_mean_squared_error: 0.4125 - val_loss: 0.4679 - val_mae: 0.6081 - val_root_mean_squared_error: 0.7970 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2706 - mae: 0.3191 - root_mean_squared_error: 0.4149 - val_loss: 0.4617 - val_mae: 0.6121 - val_root_mean_squared_error: 0.8002 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2613 - mae: 0.3139 - root_mean_squared_error: 0.4064 - val_loss: 0.4658 - val_mae: 0.6261 - val_root_mean_squared_error: 0.8123 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2579 - mae: 0.3215 - root_mean_squared_error: 0.4126 - val_loss: 0.4589 - val_mae: 0.6245 - val_root_mean_squared_error: 0.8119 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2437 - mae: 0.3012 - root_mean_squared_error: 0.3940 - val_loss: 0.4438 - val_mae: 0.6089 - val_root_mean_squared_error: 0.7964 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2377 - mae: 0.3056 - root_mean_squared_error: 0.3903 - val_loss: 0.4329 - val_mae: 0.6061 - val_root_mean_squared_error: 0.7910 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2313 - mae: 0.2965 - root_mean_squared_error: 0.3899 - val_loss: 0.4355 - val_mae: 0.6144 - val_root_mean_squared_error: 0.8066 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2225 - mae: 0.2968 - root_mean_squared_error: 0.3796 - val_loss: 0.4286 - val_mae: 0.6144 - val_root_mean_squared_error: 0.8007 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2250 - mae: 0.3055 - root_mean_squared_error: 0.4018 - val_loss: 0.4214 - val_mae: 0.6108 - val_root_mean_squared_error: 0.7974 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2161 - mae: 0.2969 - root_mean_squared_error: 0.3910 - val_loss: 0.4197 - val_mae: 0.6146 - val_root_mean_squared_error: 0.8028 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2080 - mae: 0.2895 - root_mean_squared_error: 0.3786 - val_loss: 0.4228 - val_mae: 0.6265 - val_root_mean_squared_error: 0.8150 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2025 - mae: 0.2893 - root_mean_squared_error: 0.3742 - val_loss: 0.4158 - val_mae: 0.6210 - val_root_mean_squared_error: 0.8050 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1982 - mae: 0.2861 - root_mean_squared_error: 0.3732 - val_loss: 0.4118 - val_mae: 0.6196 - val_root_mean_squared_error: 0.8090 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1927 - mae: 0.2822 - root_mean_squared_error: 0.3701 - val_loss: 0.4042 - val_mae: 0.6111 - val_root_mean_squared_error: 0.8048 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1908 - mae: 0.2856 - root_mean_squared_error: 0.3744 - val_loss: 0.4049 - val_mae: 0.6220 - val_root_mean_squared_error: 0.8111 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1863 - mae: 0.2863 - root_mean_squared_error: 0.3714 - val_loss: 0.4056 - val_mae: 0.6193 - val_root_mean_squared_error: 0.8153 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1726 - mae: 0.2640 - root_mean_squared_error: 0.3417 - val_loss: 0.3947 - val_mae: 0.6162 - val_root_mean_squared_error: 0.7996 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1734 - mae: 0.2686 - root_mean_squared_error: 0.3534 - val_loss: 0.3950 - val_mae: 0.6190 - val_root_mean_squared_error: 0.8083 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1676 - mae: 0.2677 - root_mean_squared_error: 0.3453 - val_loss: 0.3998 - val_mae: 0.6292 - val_root_mean_squared_error: 0.8217 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1607 - mae: 0.2589 - root_mean_squared_error: 0.3335 - val_loss: 0.3952 - val_mae: 0.6262 - val_root_mean_squared_error: 0.8192 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1583 - mae: 0.2601 - root_mean_squared_error: 0.3356 - val_loss: 0.3723 - val_mae: 0.5999 - val_root_mean_squared_error: 0.7840 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1563 - mae: 0.2621 - root_mean_squared_error: 0.3383 - val_loss: 0.3760 - val_mae: 0.6045 - val_root_mean_squared_error: 0.7966 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1610 - mae: 0.2700 - root_mean_squared_error: 0.3612 - val_loss: 0.3799 - val_mae: 0.6196 - val_root_mean_squared_error: 0.8077 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1591 - mae: 0.2770 - root_mean_squared_error: 0.3603 - val_loss: 0.3790 - val_mae: 0.6178 - val_root_mean_squared_error: 0.8062 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1490 - mae: 0.2626 - root_mean_squared_error: 0.3355 - val_loss: 0.3771 - val_mae: 0.6207 - val_root_mean_squared_error: 0.8064 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1475 - mae: 0.2581 - root_mean_squared_error: 0.3373 - val_loss: 0.3791 - val_mae: 0.6276 - val_root_mean_squared_error: 0.8107 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1455 - mae: 0.2571 - root_mean_squared_error: 0.3362 - val_loss: 0.3797 - val_mae: 0.6310 - val_root_mean_squared_error: 0.8146 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m26/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1475 - mae: 0.2692 - root_mean_squared_error: 0.3471 \n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1483 - mae: 0.2708 - root_mean_squared_error: 0.3499 - val_loss: 0.3776 - val_mae: 0.6283 - val_root_mean_squared_error: 0.8167 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1380 - mae: 0.2479 - root_mean_squared_error: 0.3232 - val_loss: 0.3733 - val_mae: 0.6264 - val_root_mean_squared_error: 0.8090 - learning_rate: 3.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1295 - mae: 0.2301 - root_mean_squared_error: 0.2996 - val_loss: 0.3651 - val_mae: 0.6167 - val_root_mean_squared_error: 0.7975 - learning_rate: 3.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1291 - mae: 0.2335 - root_mean_squared_error: 0.3028 - val_loss: 0.3689 - val_mae: 0.6256 - val_root_mean_squared_error: 0.8063 - learning_rate: 3.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1196 - mae: 0.2127 - root_mean_squared_error: 0.2751 - val_loss: 0.3657 - val_mae: 0.6213 - val_root_mean_squared_error: 0.8043 - learning_rate: 3.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1178 - mae: 0.2142 - root_mean_squared_error: 0.2737 - val_loss: 0.3661 - val_mae: 0.6260 - val_root_mean_squared_error: 0.8077 - learning_rate: 3.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1143 - mae: 0.2028 - root_mean_squared_error: 0.2656 - val_loss: 0.3629 - val_mae: 0.6236 - val_root_mean_squared_error: 0.8050 - learning_rate: 3.0000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1115 - mae: 0.1991 - root_mean_squared_error: 0.2607 - val_loss: 0.3597 - val_mae: 0.6193 - val_root_mean_squared_error: 0.8013 - learning_rate: 3.0000e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1101 - mae: 0.2039 - root_mean_squared_error: 0.2603 - val_loss: 0.3593 - val_mae: 0.6190 - val_root_mean_squared_error: 0.8036 - learning_rate: 3.0000e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1111 - mae: 0.2055 - root_mean_squared_error: 0.2694 - val_loss: 0.3552 - val_mae: 0.6153 - val_root_mean_squared_error: 0.7986 - learning_rate: 3.0000e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1083 - mae: 0.2054 - root_mean_squared_error: 0.2635 - val_loss: 0.3585 - val_mae: 0.6226 - val_root_mean_squared_error: 0.8073 - learning_rate: 3.0000e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1043 - mae: 0.1940 - root_mean_squared_error: 0.2540 - val_loss: 0.3540 - val_mae: 0.6176 - val_root_mean_squared_error: 0.7998 - learning_rate: 3.0000e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1051 - mae: 0.1987 - root_mean_squared_error: 0.2613 - val_loss: 0.3524 - val_mae: 0.6189 - val_root_mean_squared_error: 0.8001 - learning_rate: 3.0000e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1048 - mae: 0.2034 - root_mean_squared_error: 0.2642 - val_loss: 0.3519 - val_mae: 0.6211 - val_root_mean_squared_error: 0.8008 - learning_rate: 3.0000e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1029 - mae: 0.2002 - root_mean_squared_error: 0.2624 - val_loss: 0.3501 - val_mae: 0.6196 - val_root_mean_squared_error: 0.7994 - learning_rate: 3.0000e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1008 - mae: 0.1981 - root_mean_squared_error: 0.2583 - val_loss: 0.3512 - val_mae: 0.6234 - val_root_mean_squared_error: 0.8025 - learning_rate: 3.0000e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1016 - mae: 0.2034 - root_mean_squared_error: 0.2655 - val_loss: 0.3490 - val_mae: 0.6199 - val_root_mean_squared_error: 0.8017 - learning_rate: 3.0000e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0944 - mae: 0.1861 - root_mean_squared_error: 0.2418 - val_loss: 0.3458 - val_mae: 0.6176 - val_root_mean_squared_error: 0.8001 - learning_rate: 3.0000e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0946 - mae: 0.1914 - root_mean_squared_error: 0.2471 - val_loss: 0.3448 - val_mae: 0.6185 - val_root_mean_squared_error: 0.7983 - learning_rate: 3.0000e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0935 - mae: 0.1922 - root_mean_squared_error: 0.2471 - val_loss: 0.3522 - val_mae: 0.6290 - val_root_mean_squared_error: 0.8112 - learning_rate: 3.0000e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0911 - mae: 0.1871 - root_mean_squared_error: 0.2416 - val_loss: 0.3515 - val_mae: 0.6277 - val_root_mean_squared_error: 0.8118 - learning_rate: 3.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0890 - mae: 0.1826 - root_mean_squared_error: 0.2378 - val_loss: 0.3430 - val_mae: 0.6165 - val_root_mean_squared_error: 0.8019 - learning_rate: 3.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0901 - mae: 0.1850 - root_mean_squared_error: 0.2468 - val_loss: 0.3434 - val_mae: 0.6182 - val_root_mean_squared_error: 0.8031 - learning_rate: 3.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0893 - mae: 0.1899 - root_mean_squared_error: 0.2473 - val_loss: 0.3433 - val_mae: 0.6193 - val_root_mean_squared_error: 0.8043 - learning_rate: 3.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0902 - mae: 0.1941 - root_mean_squared_error: 0.2546 - val_loss: 0.3439 - val_mae: 0.6210 - val_root_mean_squared_error: 0.8085 - learning_rate: 3.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0858 - mae: 0.1878 - root_mean_squared_error: 0.2406 - val_loss: 0.3409 - val_mae: 0.6172 - val_root_mean_squared_error: 0.8047 - learning_rate: 3.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0888 - mae: 0.1964 - root_mean_squared_error: 0.2562 - val_loss: 0.3390 - val_mae: 0.6165 - val_root_mean_squared_error: 0.8015 - learning_rate: 3.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0851 - mae: 0.1930 - root_mean_squared_error: 0.2454 - val_loss: 0.3462 - val_mae: 0.6267 - val_root_mean_squared_error: 0.8176 - learning_rate: 3.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0843 - mae: 0.1924 - root_mean_squared_error: 0.2459 - val_loss: 0.3384 - val_mae: 0.6180 - val_root_mean_squared_error: 0.8057 - learning_rate: 3.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0838 - mae: 0.1945 - root_mean_squared_error: 0.2475 - val_loss: 0.3396 - val_mae: 0.6211 - val_root_mean_squared_error: 0.8080 - learning_rate: 3.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0798 - mae: 0.1772 - root_mean_squared_error: 0.2344 - val_loss: 0.3359 - val_mae: 0.6164 - val_root_mean_squared_error: 0.8054 - learning_rate: 3.0000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0816 - mae: 0.1881 - root_mean_squared_error: 0.2457 - val_loss: 0.3338 - val_mae: 0.6167 - val_root_mean_squared_error: 0.8012 - learning_rate: 3.0000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0783 - mae: 0.1820 - root_mean_squared_error: 0.2350 - val_loss: 0.3298 - val_mae: 0.6149 - val_root_mean_squared_error: 0.7952 - learning_rate: 3.0000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0811 - mae: 0.1935 - root_mean_squared_error: 0.2501 - val_loss: 0.3316 - val_mae: 0.6151 - val_root_mean_squared_error: 0.8008 - learning_rate: 3.0000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0808 - mae: 0.1936 - root_mean_squared_error: 0.2523 - val_loss: 0.3308 - val_mae: 0.6118 - val_root_mean_squared_error: 0.8045 - learning_rate: 3.0000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0768 - mae: 0.1848 - root_mean_squared_error: 0.2386 - val_loss: 0.3283 - val_mae: 0.6130 - val_root_mean_squared_error: 0.7966 - learning_rate: 3.0000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0790 - mae: 0.1906 - root_mean_squared_error: 0.2509 - val_loss: 0.3279 - val_mae: 0.6136 - val_root_mean_squared_error: 0.7985 - learning_rate: 3.0000e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0721 - mae: 0.1746 - root_mean_squared_error: 0.2245 - val_loss: 0.3359 - val_mae: 0.6225 - val_root_mean_squared_error: 0.8127 - learning_rate: 3.0000e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0737 - mae: 0.1833 - root_mean_squared_error: 0.2348 - val_loss: 0.3301 - val_mae: 0.6164 - val_root_mean_squared_error: 0.8060 - learning_rate: 3.0000e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0714 - mae: 0.1766 - root_mean_squared_error: 0.2277 - val_loss: 0.3293 - val_mae: 0.6209 - val_root_mean_squared_error: 0.8047 - learning_rate: 3.0000e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0729 - mae: 0.1832 - root_mean_squared_error: 0.2372 - val_loss: 0.3309 - val_mae: 0.6215 - val_root_mean_squared_error: 0.8096 - learning_rate: 3.0000e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0745 - mae: 0.1899 - root_mean_squared_error: 0.2463 - val_loss: 0.3310 - val_mae: 0.6230 - val_root_mean_squared_error: 0.8098 - learning_rate: 3.0000e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0701 - mae: 0.1775 - root_mean_squared_error: 0.2306 - val_loss: 0.3311 - val_mae: 0.6201 - val_root_mean_squared_error: 0.8127 - learning_rate: 3.0000e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m22/33\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0702 - mae: 0.1787 - root_mean_squared_error: 0.2333\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0707 - mae: 0.1804 - root_mean_squared_error: 0.2357 - val_loss: 0.3365 - val_mae: 0.6267 - val_root_mean_squared_error: 0.8244 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 92.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Resultats 150D - Pearson: 0.412, MSE: 0.635, MAE: 0.613\n",
      "Prediccions - Min: 0.962, Max: 4.558, Mean: 2.488\n",
      "Targets - Min: 0.000, Max: 5.000, Mean: 2.575\n",
      "\n",
      "=== Entrenant Model Agregat 300D ===\n",
      "Forma de les dades: X1_train=(2073, 300), Y_train=(2073,)\n",
      "Rang Y_train: [0.00, 5.00]\n",
      "Epoch 1/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 1.2486 - mae: 1.0639 - root_mean_squared_error: 1.3157 - val_loss: 0.9051 - val_mae: 0.6950 - val_root_mean_squared_error: 0.8822 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0421 - mae: 0.8554 - root_mean_squared_error: 1.0665 - val_loss: 0.8697 - val_mae: 0.6735 - val_root_mean_squared_error: 0.8595 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9244 - mae: 0.7431 - root_mean_squared_error: 0.9295 - val_loss: 0.8543 - val_mae: 0.6848 - val_root_mean_squared_error: 0.8701 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8592 - mae: 0.6861 - root_mean_squared_error: 0.8676 - val_loss: 0.8426 - val_mae: 0.6959 - val_root_mean_squared_error: 0.8837 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8155 - mae: 0.6468 - root_mean_squared_error: 0.8319 - val_loss: 0.7998 - val_mae: 0.6600 - val_root_mean_squared_error: 0.8451 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7842 - mae: 0.6322 - root_mean_squared_error: 0.8140 - val_loss: 0.7772 - val_mae: 0.6490 - val_root_mean_squared_error: 0.8339 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7280 - mae: 0.5773 - root_mean_squared_error: 0.7582 - val_loss: 0.7545 - val_mae: 0.6398 - val_root_mean_squared_error: 0.8237 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7127 - mae: 0.5794 - root_mean_squared_error: 0.7628 - val_loss: 0.7439 - val_mae: 0.6551 - val_root_mean_squared_error: 0.8356 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6641 - mae: 0.5353 - root_mean_squared_error: 0.7074 - val_loss: 0.7237 - val_mae: 0.6519 - val_root_mean_squared_error: 0.8313 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6239 - mae: 0.5061 - root_mean_squared_error: 0.6618 - val_loss: 0.7006 - val_mae: 0.6416 - val_root_mean_squared_error: 0.8224 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5909 - mae: 0.4862 - root_mean_squared_error: 0.6340 - val_loss: 0.6754 - val_mae: 0.6272 - val_root_mean_squared_error: 0.8084 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5637 - mae: 0.4592 - root_mean_squared_error: 0.6160 - val_loss: 0.6557 - val_mae: 0.6192 - val_root_mean_squared_error: 0.8013 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5281 - mae: 0.4385 - root_mean_squared_error: 0.5735 - val_loss: 0.6338 - val_mae: 0.6011 - val_root_mean_squared_error: 0.7887 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5177 - mae: 0.4430 - root_mean_squared_error: 0.5835 - val_loss: 0.6153 - val_mae: 0.6029 - val_root_mean_squared_error: 0.7850 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4871 - mae: 0.4282 - root_mean_squared_error: 0.5495 - val_loss: 0.6030 - val_mae: 0.6062 - val_root_mean_squared_error: 0.7887 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4692 - mae: 0.4144 - root_mean_squared_error: 0.5451 - val_loss: 0.5892 - val_mae: 0.6027 - val_root_mean_squared_error: 0.7889 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4463 - mae: 0.3966 - root_mean_squared_error: 0.5323 - val_loss: 0.5649 - val_mae: 0.5890 - val_root_mean_squared_error: 0.7707 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4144 - mae: 0.3700 - root_mean_squared_error: 0.4849 - val_loss: 0.5725 - val_mae: 0.6151 - val_root_mean_squared_error: 0.8070 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4051 - mae: 0.3789 - root_mean_squared_error: 0.4890 - val_loss: 0.5528 - val_mae: 0.5994 - val_root_mean_squared_error: 0.7892 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3939 - mae: 0.3774 - root_mean_squared_error: 0.5006 - val_loss: 0.5423 - val_mae: 0.5996 - val_root_mean_squared_error: 0.7948 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3795 - mae: 0.3734 - root_mean_squared_error: 0.4873 - val_loss: 0.5235 - val_mae: 0.5922 - val_root_mean_squared_error: 0.7791 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3601 - mae: 0.3550 - root_mean_squared_error: 0.4640 - val_loss: 0.5191 - val_mae: 0.5981 - val_root_mean_squared_error: 0.7875 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3471 - mae: 0.3496 - root_mean_squared_error: 0.4586 - val_loss: 0.5074 - val_mae: 0.6003 - val_root_mean_squared_error: 0.7849 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3273 - mae: 0.3361 - root_mean_squared_error: 0.4344 - val_loss: 0.5005 - val_mae: 0.6067 - val_root_mean_squared_error: 0.7910 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3229 - mae: 0.3411 - root_mean_squared_error: 0.4488 - val_loss: 0.4895 - val_mae: 0.6007 - val_root_mean_squared_error: 0.7854 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3074 - mae: 0.3342 - root_mean_squared_error: 0.4310 - val_loss: 0.4789 - val_mae: 0.5986 - val_root_mean_squared_error: 0.7784 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2892 - mae: 0.3101 - root_mean_squared_error: 0.4077 - val_loss: 0.4762 - val_mae: 0.5923 - val_root_mean_squared_error: 0.7950 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2913 - mae: 0.3318 - root_mean_squared_error: 0.4324 - val_loss: 0.4660 - val_mae: 0.5984 - val_root_mean_squared_error: 0.7882 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2792 - mae: 0.3211 - root_mean_squared_error: 0.4202 - val_loss: 0.4656 - val_mae: 0.6120 - val_root_mean_squared_error: 0.8007 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2669 - mae: 0.3144 - root_mean_squared_error: 0.4065 - val_loss: 0.4543 - val_mae: 0.6008 - val_root_mean_squared_error: 0.7903 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2565 - mae: 0.3050 - root_mean_squared_error: 0.3970 - val_loss: 0.4478 - val_mae: 0.5989 - val_root_mean_squared_error: 0.7919 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2489 - mae: 0.3008 - root_mean_squared_error: 0.3955 - val_loss: 0.4517 - val_mae: 0.6118 - val_root_mean_squared_error: 0.8044 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2445 - mae: 0.3022 - root_mean_squared_error: 0.3990 - val_loss: 0.4357 - val_mae: 0.5994 - val_root_mean_squared_error: 0.7899 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2309 - mae: 0.2923 - root_mean_squared_error: 0.3762 - val_loss: 0.4373 - val_mae: 0.6123 - val_root_mean_squared_error: 0.7993 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2294 - mae: 0.2984 - root_mean_squared_error: 0.3877 - val_loss: 0.4348 - val_mae: 0.6143 - val_root_mean_squared_error: 0.8085 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2218 - mae: 0.2949 - root_mean_squared_error: 0.3798 - val_loss: 0.4250 - val_mae: 0.6080 - val_root_mean_squared_error: 0.7976 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2109 - mae: 0.2836 - root_mean_squared_error: 0.3645 - val_loss: 0.4194 - val_mae: 0.6032 - val_root_mean_squared_error: 0.7965 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2067 - mae: 0.2829 - root_mean_squared_error: 0.3656 - val_loss: 0.4109 - val_mae: 0.5997 - val_root_mean_squared_error: 0.7887 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2041 - mae: 0.2855 - root_mean_squared_error: 0.3703 - val_loss: 0.4157 - val_mae: 0.6147 - val_root_mean_squared_error: 0.8024 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1998 - mae: 0.2850 - root_mean_squared_error: 0.3672 - val_loss: 0.4048 - val_mae: 0.6008 - val_root_mean_squared_error: 0.7917 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1935 - mae: 0.2726 - root_mean_squared_error: 0.3615 - val_loss: 0.4030 - val_mae: 0.6019 - val_root_mean_squared_error: 0.7969 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1900 - mae: 0.2764 - root_mean_squared_error: 0.3632 - val_loss: 0.3897 - val_mae: 0.5862 - val_root_mean_squared_error: 0.7816 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1850 - mae: 0.2731 - root_mean_squared_error: 0.3586 - val_loss: 0.3925 - val_mae: 0.6028 - val_root_mean_squared_error: 0.7883 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1771 - mae: 0.2661 - root_mean_squared_error: 0.3453 - val_loss: 0.3844 - val_mae: 0.5979 - val_root_mean_squared_error: 0.7799 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1755 - mae: 0.2669 - root_mean_squared_error: 0.3497 - val_loss: 0.3831 - val_mae: 0.5885 - val_root_mean_squared_error: 0.7883 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1682 - mae: 0.2588 - root_mean_squared_error: 0.3360 - val_loss: 0.3902 - val_mae: 0.6066 - val_root_mean_squared_error: 0.8040 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1736 - mae: 0.2774 - root_mean_squared_error: 0.3618 - val_loss: 0.3852 - val_mae: 0.6041 - val_root_mean_squared_error: 0.7974 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1674 - mae: 0.2610 - root_mean_squared_error: 0.3547 - val_loss: 0.3953 - val_mae: 0.6171 - val_root_mean_squared_error: 0.8151 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1699 - mae: 0.2805 - root_mean_squared_error: 0.3629 - val_loss: 0.3765 - val_mae: 0.6015 - val_root_mean_squared_error: 0.7885 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1759 - mae: 0.2916 - root_mean_squared_error: 0.3864 - val_loss: 0.3903 - val_mae: 0.6105 - val_root_mean_squared_error: 0.8169 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1632 - mae: 0.2664 - root_mean_squared_error: 0.3508 - val_loss: 0.3671 - val_mae: 0.5842 - val_root_mean_squared_error: 0.7817 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1560 - mae: 0.2537 - root_mean_squared_error: 0.3365 - val_loss: 0.3922 - val_mae: 0.6240 - val_root_mean_squared_error: 0.8237 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1627 - mae: 0.2774 - root_mean_squared_error: 0.3627 - val_loss: 0.3770 - val_mae: 0.6021 - val_root_mean_squared_error: 0.8025 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1637 - mae: 0.2776 - root_mean_squared_error: 0.3674 - val_loss: 0.3751 - val_mae: 0.6031 - val_root_mean_squared_error: 0.8007 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1641 - mae: 0.2853 - root_mean_squared_error: 0.3724 - val_loss: 0.3728 - val_mae: 0.6017 - val_root_mean_squared_error: 0.8008 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1493 - mae: 0.2556 - root_mean_squared_error: 0.3374 - val_loss: 0.3700 - val_mae: 0.6016 - val_root_mean_squared_error: 0.7983 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1505 - mae: 0.2628 - root_mean_squared_error: 0.3448 - val_loss: 0.3666 - val_mae: 0.6012 - val_root_mean_squared_error: 0.7921 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1459 - mae: 0.2575 - root_mean_squared_error: 0.3354 - val_loss: 0.3619 - val_mae: 0.5994 - val_root_mean_squared_error: 0.7924 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1465 - mae: 0.2633 - root_mean_squared_error: 0.3423 - val_loss: 0.3590 - val_mae: 0.5859 - val_root_mean_squared_error: 0.7933 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1498 - mae: 0.2659 - root_mean_squared_error: 0.3583 - val_loss: 0.3703 - val_mae: 0.6078 - val_root_mean_squared_error: 0.8080 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1460 - mae: 0.2696 - root_mean_squared_error: 0.3451 - val_loss: 0.3711 - val_mae: 0.6105 - val_root_mean_squared_error: 0.8102 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1467 - mae: 0.2619 - root_mean_squared_error: 0.3513 - val_loss: 0.3610 - val_mae: 0.5945 - val_root_mean_squared_error: 0.7946 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1457 - mae: 0.2633 - root_mean_squared_error: 0.3490 - val_loss: 0.3565 - val_mae: 0.5940 - val_root_mean_squared_error: 0.7917 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1381 - mae: 0.2473 - root_mean_squared_error: 0.3281 - val_loss: 0.3586 - val_mae: 0.5983 - val_root_mean_squared_error: 0.7917 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1408 - mae: 0.2625 - root_mean_squared_error: 0.3375 - val_loss: 0.3593 - val_mae: 0.6055 - val_root_mean_squared_error: 0.7909 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1351 - mae: 0.2475 - root_mean_squared_error: 0.3240 - val_loss: 0.3602 - val_mae: 0.6060 - val_root_mean_squared_error: 0.8020 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1355 - mae: 0.2451 - root_mean_squared_error: 0.3298 - val_loss: 0.3623 - val_mae: 0.6067 - val_root_mean_squared_error: 0.8002 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1433 - mae: 0.2644 - root_mean_squared_error: 0.3536 - val_loss: 0.3662 - val_mae: 0.6063 - val_root_mean_squared_error: 0.8081 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1395 - mae: 0.2588 - root_mean_squared_error: 0.3397 - val_loss: 0.3473 - val_mae: 0.5839 - val_root_mean_squared_error: 0.7768 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1349 - mae: 0.2502 - root_mean_squared_error: 0.3287 - val_loss: 0.3450 - val_mae: 0.5872 - val_root_mean_squared_error: 0.7689 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1415 - mae: 0.2666 - root_mean_squared_error: 0.3511 - val_loss: 0.3575 - val_mae: 0.5922 - val_root_mean_squared_error: 0.7942 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1331 - mae: 0.2463 - root_mean_squared_error: 0.3316 - val_loss: 0.3544 - val_mae: 0.5975 - val_root_mean_squared_error: 0.7891 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1290 - mae: 0.2403 - root_mean_squared_error: 0.3176 - val_loss: 0.3556 - val_mae: 0.5965 - val_root_mean_squared_error: 0.7986 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1283 - mae: 0.2410 - root_mean_squared_error: 0.3186 - val_loss: 0.3493 - val_mae: 0.5934 - val_root_mean_squared_error: 0.7878 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1349 - mae: 0.2588 - root_mean_squared_error: 0.3396 - val_loss: 0.3510 - val_mae: 0.5987 - val_root_mean_squared_error: 0.7873 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1387 - mae: 0.2633 - root_mean_squared_error: 0.3490 - val_loss: 0.3656 - val_mae: 0.6137 - val_root_mean_squared_error: 0.8170 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "\u001b[1m17/33\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1401 - mae: 0.2607 - root_mean_squared_error: 0.3597 \n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1390 - mae: 0.2592 - root_mean_squared_error: 0.3549 - val_loss: 0.3655 - val_mae: 0.6167 - val_root_mean_squared_error: 0.8104 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1324 - mae: 0.2485 - root_mean_squared_error: 0.3308 - val_loss: 0.3561 - val_mae: 0.6046 - val_root_mean_squared_error: 0.7979 - learning_rate: 3.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1149 - mae: 0.2150 - root_mean_squared_error: 0.2783 - val_loss: 0.3499 - val_mae: 0.6017 - val_root_mean_squared_error: 0.7895 - learning_rate: 3.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1102 - mae: 0.2015 - root_mean_squared_error: 0.2669 - val_loss: 0.3479 - val_mae: 0.6019 - val_root_mean_squared_error: 0.7917 - learning_rate: 3.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1090 - mae: 0.2068 - root_mean_squared_error: 0.2686 - val_loss: 0.3472 - val_mae: 0.6059 - val_root_mean_squared_error: 0.7892 - learning_rate: 3.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1076 - mae: 0.2054 - root_mean_squared_error: 0.2704 - val_loss: 0.3413 - val_mae: 0.5987 - val_root_mean_squared_error: 0.7831 - learning_rate: 3.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0972 - mae: 0.1807 - root_mean_squared_error: 0.2358 - val_loss: 0.3384 - val_mae: 0.5920 - val_root_mean_squared_error: 0.7823 - learning_rate: 3.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1019 - mae: 0.2013 - root_mean_squared_error: 0.2616 - val_loss: 0.3371 - val_mae: 0.5975 - val_root_mean_squared_error: 0.7816 - learning_rate: 3.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0950 - mae: 0.1852 - root_mean_squared_error: 0.2403 - val_loss: 0.3382 - val_mae: 0.5984 - val_root_mean_squared_error: 0.7875 - learning_rate: 3.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0921 - mae: 0.1810 - root_mean_squared_error: 0.2350 - val_loss: 0.3339 - val_mae: 0.5990 - val_root_mean_squared_error: 0.7828 - learning_rate: 3.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0925 - mae: 0.1860 - root_mean_squared_error: 0.2427 - val_loss: 0.3374 - val_mae: 0.6015 - val_root_mean_squared_error: 0.7913 - learning_rate: 3.0000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0854 - mae: 0.1709 - root_mean_squared_error: 0.2188 - val_loss: 0.3338 - val_mae: 0.5993 - val_root_mean_squared_error: 0.7880 - learning_rate: 3.0000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0880 - mae: 0.1812 - root_mean_squared_error: 0.2369 - val_loss: 0.3312 - val_mae: 0.5968 - val_root_mean_squared_error: 0.7858 - learning_rate: 3.0000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0835 - mae: 0.1743 - root_mean_squared_error: 0.2232 - val_loss: 0.3313 - val_mae: 0.6008 - val_root_mean_squared_error: 0.7875 - learning_rate: 3.0000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0838 - mae: 0.1774 - root_mean_squared_error: 0.2309 - val_loss: 0.3299 - val_mae: 0.6021 - val_root_mean_squared_error: 0.7858 - learning_rate: 3.0000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0812 - mae: 0.1740 - root_mean_squared_error: 0.2249 - val_loss: 0.3243 - val_mae: 0.5954 - val_root_mean_squared_error: 0.7802 - learning_rate: 3.0000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0801 - mae: 0.1719 - root_mean_squared_error: 0.2254 - val_loss: 0.3239 - val_mae: 0.5980 - val_root_mean_squared_error: 0.7810 - learning_rate: 3.0000e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0794 - mae: 0.1777 - root_mean_squared_error: 0.2278 - val_loss: 0.3324 - val_mae: 0.6104 - val_root_mean_squared_error: 0.7976 - learning_rate: 3.0000e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0762 - mae: 0.1669 - root_mean_squared_error: 0.2185 - val_loss: 0.3290 - val_mae: 0.6029 - val_root_mean_squared_error: 0.7943 - learning_rate: 3.0000e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0761 - mae: 0.1687 - root_mean_squared_error: 0.2229 - val_loss: 0.3259 - val_mae: 0.6011 - val_root_mean_squared_error: 0.7926 - learning_rate: 3.0000e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0737 - mae: 0.1714 - root_mean_squared_error: 0.2175 - val_loss: 0.3227 - val_mae: 0.5986 - val_root_mean_squared_error: 0.7880 - learning_rate: 3.0000e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0749 - mae: 0.1764 - root_mean_squared_error: 0.2279 - val_loss: 0.3174 - val_mae: 0.5952 - val_root_mean_squared_error: 0.7833 - learning_rate: 3.0000e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0724 - mae: 0.1665 - root_mean_squared_error: 0.2209 - val_loss: 0.3247 - val_mae: 0.6044 - val_root_mean_squared_error: 0.7953 - learning_rate: 3.0000e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0725 - mae: 0.1720 - root_mean_squared_error: 0.2250 - val_loss: 0.3232 - val_mae: 0.6007 - val_root_mean_squared_error: 0.7935 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 98.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Resultats 300D - Pearson: 0.440, MSE: 0.614, MAE: 0.595\n",
      "Prediccions - Min: 0.931, Max: 4.271, Mean: 2.463\n",
      "Targets - Min: 0.000, Max: 5.000, Mean: 2.575\n"
     ]
    }
   ],
   "source": [
    "def build_model_aggregated(embedding_dim: int, hidden_size: int = 256, \n",
    "                          dropout_rate: float = 0.3) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Model amb millor arquitectura\n",
    "    \"\"\"\n",
    "    input_1 = tf.keras.Input(shape=(embedding_dim,), name=\"input_vector_1\")\n",
    "    input_2 = tf.keras.Input(shape=(embedding_dim,), name=\"input_vector_2\")\n",
    "    \n",
    "    # Normalitzar inputs (L2 normalization)\n",
    "    norm_1 = tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(input_1)\n",
    "    norm_2 = tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(input_2)\n",
    "    \n",
    "    # Múltiples formes de combinar els vectors\n",
    "    concatenated = tf.keras.layers.Concatenate(axis=-1)([norm_1, norm_2])\n",
    "    \n",
    "    # Diferència absoluta (captura dissimilitud)\n",
    "    abs_diff = tf.keras.layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([norm_1, norm_2])\n",
    "    \n",
    "    # Producte element-wise (captura similitud)\n",
    "    element_wise = tf.keras.layers.Multiply()([norm_1, norm_2])\n",
    "    \n",
    "    # Combinar totes les representacions\n",
    "    combined = tf.keras.layers.Concatenate(axis=-1)([concatenated, abs_diff, element_wise])\n",
    "    \n",
    "    # Arquitectura més robusta\n",
    "    x = tf.keras.layers.BatchNormalization()(combined)\n",
    "    x = tf.keras.layers.Dense(hidden_size, activation='relu', \n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(hidden_size // 2, activation='relu',\n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(hidden_size // 4, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate / 2)(x)\n",
    "    \n",
    "    # Capa de sortida amb activació sigmoid escalada a [0,5]\n",
    "    output_raw = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    output = tf.keras.layers.Lambda(lambda x: x * 5.0)(output_raw)  # Escalar a [0,5]\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    \n",
    "    # Utilitzar Huber loss (més robust a outliers)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.Huber(delta=1.0),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "        metrics=['mae', tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_aggregated_data(df: pd.DataFrame, embeddings_dict: Dict[str, np.ndarray], \n",
    "                           vector_size: int, use_tfidf: bool = False,\n",
    "                           tfidf_vectorizer=None, feature_names=None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \n",
    "    X1, X2, Y = [], [], []\n",
    "    invalid_count = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        sent1, sent2, label = row['sentence_1'], row['sentence_2'], row['label']\n",
    "        \n",
    "        if use_tfidf and tfidf_vectorizer is not None:\n",
    "            vec1 = get_sentence_embedding_tfidf(sent1, embeddings_dict, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "            vec2 = get_sentence_embedding_tfidf(sent2, embeddings_dict, tfidf_vectorizer, \n",
    "                                              feature_names, vector_size)\n",
    "        else:\n",
    "            vec1 = get_sentence_embedding_simple(sent1, embeddings_dict, vector_size)\n",
    "            vec2 = get_sentence_embedding_simple(sent2, embeddings_dict, vector_size)\n",
    "        \n",
    "        # Validar embeddings\n",
    "        if np.any(np.isnan(vec1)) or np.any(np.isnan(vec2)):\n",
    "            invalid_count += 1\n",
    "            continue\n",
    "            \n",
    "        X1.append(vec1)\n",
    "        X2.append(vec2)\n",
    "        Y.append(label)\n",
    "    \n",
    "    if invalid_count > 0:\n",
    "        print(f\"Warning: {invalid_count} mostres amb embeddings invàlids eliminades\")\n",
    "    \n",
    "    return np.array(X1), np.array(X2), np.array(Y)\n",
    "\n",
    "# Codi d'entrenament\n",
    "if kv_model is not None:\n",
    "    aggregated_results = {}\n",
    "    \n",
    "    for dim in [50, 100, 150, 300]:\n",
    "        if dim in truncated_embeddings:\n",
    "            print(f\"\\n=== Entrenant Model Agregat {dim}D ===\")\n",
    "            \n",
    "            # Preparar dades\n",
    "            X1_train, X2_train, Y_train = prepare_aggregated_data(\n",
    "                train_df, truncated_embeddings[dim], dim\n",
    "            )\n",
    "            X1_val, X2_val, Y_val = prepare_aggregated_data(\n",
    "                val_df, truncated_embeddings[dim], dim\n",
    "            )\n",
    "            \n",
    "            print(f\"Forma de les dades: X1_train={X1_train.shape}, Y_train={Y_train.shape}\")\n",
    "            print(f\"Rang Y_train: [{Y_train.min():.2f}, {Y_train.max():.2f}]\")\n",
    "            \n",
    "            # Construir model\n",
    "            model = build_model_aggregated(embedding_dim=dim, hidden_size=256)\n",
    "            \n",
    "            # Callbacks\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=15, restore_best_weights=True, \n",
    "                verbose=1, min_delta=0.001\n",
    "            )\n",
    "            \n",
    "            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.3, patience=7, min_lr=1e-7, verbose=1\n",
    "            )\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "                lambda epoch: 0.001 * (0.9 ** epoch)\n",
    "            )\n",
    "            \n",
    "            # Entrenament\n",
    "            history = model.fit(\n",
    "                [X1_train, X2_train], Y_train,\n",
    "                validation_data=([X1_val, X2_val], Y_val),\n",
    "                epochs=100,  # Més èpoques\n",
    "                batch_size=64,  # Batch size més gran\n",
    "                callbacks=[early_stopping, reduce_lr],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Avaluació\n",
    "            Y_pred = model.predict([X1_val, X2_val]).flatten()\n",
    "            \n",
    "            # Clips predictions to valid range\n",
    "            Y_pred = np.clip(Y_pred, 0, 5)\n",
    "            \n",
    "            pearson_corr, _ = pearsonr(Y_val, Y_pred)\n",
    "            mse = mean_squared_error(Y_val, Y_pred)\n",
    "            mae = mean_absolute_error(Y_val, Y_pred)\n",
    "            \n",
    "            aggregated_results[dim] = {\n",
    "                'model': model,\n",
    "                'history': history,\n",
    "                'pearson': pearson_corr,\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'predictions': Y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"Resultats {dim}D - Pearson: {pearson_corr:.3f}, MSE: {mse:.3f}, MAE: {mae:.3f}\")\n",
    "            \n",
    "            # Diagnosis adicional\n",
    "            print(f\"Prediccions - Min: {Y_pred.min():.3f}, Max: {Y_pred.max():.3f}, Mean: {Y_pred.mean():.3f}\")\n",
    "            print(f\"Targets - Min: {Y_val.min():.3f}, Max: {Y_val.max():.3f}, Mean: {Y_val.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363fef0",
   "metadata": {},
   "source": [
    "A continuació fem una comparativa dels models agregats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c4195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARACIÓ MODELS AGREGATS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_2b3fa\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_2b3fa_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_2b3fa_level0_col1\" class=\"col_heading level0 col1\" >Dimensions</th>\n",
       "      <th id=\"T_2b3fa_level0_col2\" class=\"col_heading level0 col2\" >Pearson</th>\n",
       "      <th id=\"T_2b3fa_level0_col3\" class=\"col_heading level0 col3\" >MSE</th>\n",
       "      <th id=\"T_2b3fa_level0_col4\" class=\"col_heading level0 col4\" >MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_2b3fa_row0_col0\" class=\"data row0 col0\" >Model Agregat</td>\n",
       "      <td id=\"T_2b3fa_row0_col1\" class=\"data row0 col1\" >50D</td>\n",
       "      <td id=\"T_2b3fa_row0_col2\" class=\"data row0 col2\" >0.388917</td>\n",
       "      <td id=\"T_2b3fa_row0_col3\" class=\"data row0 col3\" >0.651528</td>\n",
       "      <td id=\"T_2b3fa_row0_col4\" class=\"data row0 col4\" >0.602588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b3fa_row1_col0\" class=\"data row1 col0\" >Model Agregat</td>\n",
       "      <td id=\"T_2b3fa_row1_col1\" class=\"data row1 col1\" >100D</td>\n",
       "      <td id=\"T_2b3fa_row1_col2\" class=\"data row1 col2\" >0.418242</td>\n",
       "      <td id=\"T_2b3fa_row1_col3\" class=\"data row1 col3\" >0.617774</td>\n",
       "      <td id=\"T_2b3fa_row1_col4\" class=\"data row1 col4\" >0.591511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b3fa_row2_col0\" class=\"data row2 col0\" >Model Agregat</td>\n",
       "      <td id=\"T_2b3fa_row2_col1\" class=\"data row2 col1\" >150D</td>\n",
       "      <td id=\"T_2b3fa_row2_col2\" class=\"data row2 col2\" >0.411732</td>\n",
       "      <td id=\"T_2b3fa_row2_col3\" class=\"data row2 col3\" >0.634592</td>\n",
       "      <td id=\"T_2b3fa_row2_col4\" class=\"data row2 col4\" >0.612983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b3fa_row3_col0\" class=\"data row3 col0\" >Model Agregat</td>\n",
       "      <td id=\"T_2b3fa_row3_col1\" class=\"data row3 col1\" >300D</td>\n",
       "      <td id=\"T_2b3fa_row3_col2\" class=\"data row3 col2\" >0.440120</td>\n",
       "      <td id=\"T_2b3fa_row3_col3\" class=\"data row3 col3\" >0.613584</td>\n",
       "      <td id=\"T_2b3fa_row3_col4\" class=\"data row3 col4\" >0.595229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fcc176ac90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n=== COMPARACIÓ MODELS AGREGATS ===\")\n",
    "# Crear un DataFrame amb els resultats dels models agregats\n",
    "aggr_data = []\n",
    "for dim in [50, 100, 150, 300]:\n",
    "    if dim in aggregated_results:\n",
    "        aggr_data.append({\n",
    "            'Model': 'Model Agregat',\n",
    "            'Dimensions': f'{dim}D',\n",
    "            'Pearson': aggregated_results[dim]['pearson'],\n",
    "            'MSE': aggregated_results[dim]['mse'],\n",
    "            'MAE': aggregated_results[dim]['mae']\n",
    "        })\n",
    "\n",
    "df_aggregated = pd.DataFrame(aggr_data)\n",
    "df_results = pd.concat([df_baseline_results, df_aggregated], ignore_index=True)\n",
    "display(df_aggregated.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd033a",
   "metadata": {},
   "source": [
    "### Anàlisi dels Models Agregats\n",
    "\n",
    "Els models agregats mostren una millora significativa respecte als resultats anteriors, amb correlacions de Pearson que oscil·len entre 0.39 i 0.44, superant clarament el llindar de referència establert pels baselines (0.356). El millor resultat s'obté amb 300D (Pearson: 0.440), seguint un patró de millora gradual amb l'augment de dimensions fins als 100D, després es manté relativament estable.\n",
    "\n",
    "Els errors MSE es mantenen consistents al voltant de 0.61-0.65, mentre que els MAE oscil·len entre 0.59-0.61, indicant prediccions més precises que les versions anteriors. La millora més notable es produeix entre 50D i 100D (increment del 7.5% en Pearson), mentre que l'augment posterior és més moderat.\n",
    "\n",
    "Aquests resultats demostren que les millores implementades en l'arquitectura del model (normalització L2, múltiples formes de combinació de vectors, regularització Huber loss) han tingut un impacte positiu substancial, convertint els models agregats en una alternativa viable que supera els baselines tradicionals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7c417",
   "metadata": {},
   "source": [
    "## 6. Model 2: Seqüència d'Embeddings amb Atenció\n",
    "\n",
    "Aquest segon model neuronal representa una aproximació més sofisticada que processa les frases com a seqüències d'embeddings de paraules, incorporant un mecanisme d'atenció per capturar millor les dependències entre paraules i la importància relativa de cada terme en el context de la similitud semàntica.\n",
    "\n",
    "### Arquitectura i Metodologia\n",
    "\n",
    "L'enfocament de seqüències d'embeddings manté l'ordre i la informació posicional de les paraules dins de cada frase, contrastant amb l'agregació simple del Model 1. L'arquitectura utilitza una capa d'embedding compartida que mapeja els índexs de paraules a vectors densos, seguida d'un mecanisme d'atenció que aprèn a ponderar automàticament la importància de cada paraula per a la tasca de similitud.\n",
    "\n",
    "La **capa d'atenció simple** implementada calcula pesos d'atenció mitjançant una xarxa neuronal de dues capes: primer transforma els embeddings d'entrada amb una capa densa amb activació `tanh`, després computa puntuacions d'atenció amb una segona capa densa. Els pesos resultants s'apliquen per generar un vector de context que representa tota la seqüència.\n",
    "\n",
    "El model calcula la **similitud cosinus** entre els vectors de context normalitzats de les dues frases, escalant el resultat de `[-1,1]` a `[0,1]` per coincidir amb les etiquetes del dataset. Aquest enfocament és més principiat que la regressió directa, ja que explota la naturalesa geomètrica de la similitud semàntica.\n",
    "\n",
    "### Experimentació amb Embeddings Entrenables\n",
    "\n",
    "Seguint els requeriments de la pràctica, s'implementen tres configuracions d'embeddings per analitzar l'impacte del fine-tuning:\n",
    "\n",
    "1. **Embeddings Pre-entrenats Frozen**: Els pesos de Word2Vec es mantenen fixos durant l'entrenament, preservant el coneixement semàntic original.\n",
    "\n",
    "2. **Embeddings Pre-entrenats Trainable**: Els embeddings s'inicialitzen amb Word2Vec però s'actualitzen durant l'entrenament, permetent l'adaptació a la tasca específica.\n",
    "\n",
    "3. **Embeddings Aleatoris**: Inicialització aleatòria dels embeddings, entrenant-los completament des de zero.\n",
    "\n",
    "Durant l'entrenament, es proven aquestes configuracions en diferents dimensions d'embedding (50D, 100D, 150D, 300D) per analitzar com la dimensionalitat interactua amb el fine-tuning dels embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "class SimpleAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Capa d'atenció simple per agregar seqüències d'embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, units: int, **kwargs):\n",
    "        super(SimpleAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dropout_s1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout_s2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W_s1 = tf.keras.layers.Dense(units, activation='tanh', use_bias=True, name=\"attention_transform\")\n",
    "        # Capa densa per calcular puntuacions d'atenció (vector de context)\n",
    "        self.W_s2 = tf.keras.layers.Dense(1, use_bias=False, name=\"attention_scorer\")\n",
    "        self.supports_masking = True  # Declara que aquesta capa suporta masking\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n",
    "        # forma dels inputs: (batch_size, sequence_length, embedding_dim)\n",
    "        # forma de la màscara: (batch_size, sequence_length) tensor booleà\n",
    "\n",
    "        # Estats ocults d'atenció\n",
    "        hidden_states = self.dropout_s1(self.W_s1(inputs))\n",
    "\n",
    "        # Calcular puntuacions d'atenció\n",
    "        scores = self.dropout_s2(self.W_s2(hidden_states))\n",
    "\n",
    "        if mask is not None:\n",
    "            # Aplicar la màscara a les puntuacions abans del softmax\n",
    "            expanded_mask = tf.expand_dims(tf.cast(mask, dtype=tf.float32), axis=-1)\n",
    "            # Afegir un número negatiu gran a les puntuacions emmascerades (padding)\n",
    "            scores += (1.0 - expanded_mask) * -1e9\n",
    "\n",
    "        # Calcular pesos d'atenció\n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "\n",
    "        # Calcular el vector de context (suma ponderada dels embeddings d'entrada)\n",
    "        context_vector = tf.reduce_sum(inputs * attention_weights, axis=1)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = super(SimpleAttention, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "\n",
    "    def compute_mask(self, inputs: tf.Tensor, mask: Optional[tf.Tensor] = None) -> Optional[tf.Tensor]:\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_model_sequence(vocab_size: int = 1000, \n",
    "                        embedding_dim: int = 300,\n",
    "                        sequence_length: int = 32,\n",
    "                        learning_rate: float = 0.001,\n",
    "                        trainable_embeddings: bool = False,\n",
    "                        pretrained_weights: Optional[np.ndarray] = None,\n",
    "                        use_attention: bool = True,\n",
    "                        attention_units: int = 4) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Model de seqüència millorat basat en build_and_compile_model_2\n",
    "    Incorpora similitud cosinus i escalat adequat per STS\n",
    "    \"\"\"\n",
    "    input_1 = tf.keras.Input((sequence_length,), dtype=tf.int32, name=\"input_1\")\n",
    "    input_2 = tf.keras.Input((sequence_length,), dtype=tf.int32, name=\"input_2\")\n",
    "\n",
    "    # Determinar paràmetres efectius d'embedding\n",
    "    if pretrained_weights is not None:\n",
    "        effective_dictionary_size = pretrained_weights.shape[0]\n",
    "        effective_embedding_size = pretrained_weights.shape[1]\n",
    "        embedding_initializer = tf.keras.initializers.Constant(pretrained_weights)\n",
    "        is_embedding_trainable = trainable_embeddings\n",
    "        embedding_layer_name = \"embedding_pretrained\"\n",
    "    else:\n",
    "        effective_dictionary_size = vocab_size\n",
    "        effective_embedding_size = embedding_dim\n",
    "        embedding_initializer = 'uniform'\n",
    "        is_embedding_trainable = True\n",
    "        embedding_layer_name = \"embedding\"\n",
    "\n",
    "    # Capa d'Embedding Compartida\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=effective_dictionary_size,\n",
    "        output_dim=effective_embedding_size,\n",
    "        input_length=sequence_length,\n",
    "        mask_zero=True,\n",
    "        embeddings_initializer=embedding_initializer,\n",
    "        trainable=is_embedding_trainable,\n",
    "        name=embedding_layer_name\n",
    "    )\n",
    "\n",
    "    # Aplicar la capa d'embedding a ambdues entrades\n",
    "    embedded_1 = embedding_layer(input_1)  # Forma: (batch_size, sequence_length, effective_embedding_size)\n",
    "    embedded_2 = embedding_layer(input_2)  # Forma: (batch_size, sequence_length, effective_embedding_size)\n",
    "\n",
    "    # Capa compartida de pooling/atenció\n",
    "    if use_attention:\n",
    "        # Utilitzar mecanisme d'atenció\n",
    "        sentence_pooling_layer = SimpleAttention(units=attention_units, name=\"sentence_attention\")\n",
    "    else:\n",
    "        # Utilitzar pooling mitjà global simple\n",
    "        sentence_pooling_layer = tf.keras.layers.GlobalAveragePooling1D(name=\"sentence_attention_layer\")\n",
    "\n",
    "    # Aplicar pooling/atenció per obtenir vectors de frase\n",
    "    sentence_vector_1 = sentence_pooling_layer(embedded_1)\n",
    "    sentence_vector_2 = sentence_pooling_layer(embedded_2)\n",
    "\n",
    "    # Capa de projecció\n",
    "    first_projection_layer = tf.keras.layers.Dense(\n",
    "        effective_embedding_size,\n",
    "        activation='tanh',\n",
    "        kernel_initializer=tf.keras.initializers.Identity(),\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "        name=\"projection_layer\"\n",
    "    )\n",
    "    dropout = tf.keras.layers.Dropout(0.2, name=\"projection_dropout\")\n",
    "    projected_1 = dropout(first_projection_layer(sentence_vector_1))\n",
    "    projected_2 = dropout(first_projection_layer(sentence_vector_2))\n",
    "\n",
    "    # Normalitzar els vectors projectats (normalització L2)\n",
    "    normalized_1 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1), name=\"normalize_1\"\n",
    "    )(projected_1)\n",
    "    normalized_2 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1), name=\"normalize_2\"\n",
    "    )(projected_2)\n",
    "\n",
    "    # Calcular Similitud Cosinus\n",
    "    similarity_score = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.reduce_sum(x[0] * x[1], axis=1, keepdims=True), name=\"cosine_similarity\"\n",
    "    )([normalized_1, normalized_2])\n",
    "\n",
    "    # Escalar similitud\n",
    "    output_layer = tf.keras.layers.Lambda(\n",
    "        lambda x: 0.5 * (1.0 + x), name=\"output_scaling\"\n",
    "    )(similarity_score)\n",
    "\n",
    "    # Definir el Model de Keras\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_1, input_2],\n",
    "        outputs=output_layer,\n",
    "        name=\"sequence_similarity_attention_model\"\n",
    "    )\n",
    "\n",
    "    # Compilar el model\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=['mae'],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ccb5fd",
   "metadata": {},
   "source": [
    "\n",
    "Aquestes són les funcions que es defineixen i seràn utilitzades posteriorment:\n",
    "\n",
    "*   `create_vocabulary_mapping`: Crea un diccionari que assigna a cada paraula un índex únic i viceversa. Això és essencial per convertir text en dades numèriques que els models poden processar.\n",
    "\n",
    "*   `sentence_to_sequence`: Transforma una frase en una seqüència d'índexs utilitzant el diccionari creat anteriorment. Aquesta funció també s'encarrega de fer padding o truncament per assegurar que totes les seqüències tinguin la mateixa longitud.\n",
    "\n",
    "*   `create_pretrained_embedding_matrix`: Crea una matriu d'embeddings pre-entrenats on cada fila correspon a l'embedding d'una paraula del vocabulari. Si una paraula no es troba en els embeddings pre-entrenats, el seu vector es manté a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6139a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Preparació de dades per al model de seqüències\n",
    "def create_vocabulary_mapping(sentences: List[str], max_vocab_size: int = 10000) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Crea un mapatge de vocabulari paraula->índex i índex->paraula\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    for sentence in sentences:\n",
    "        words = preprocess_sentence(sentence)\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    # Ordenar per freqüència i prendre les més comunes\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Reservar índexs especials: 0=PAD, 1=UNK\n",
    "    word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    idx_to_word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "    \n",
    "    for word, count in sorted_words[:max_vocab_size-2]:  # -2 per PAD i UNK\n",
    "        idx = len(word_to_idx)\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "    \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def sentence_to_sequence(sentence: str, word_to_idx: Dict[str, int], \n",
    "                        max_length: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converteix una frase a seqüència d'índexs\n",
    "    \"\"\"\n",
    "    words = preprocess_sentence(sentence)\n",
    "    sequence = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            sequence.append(word_to_idx[word])\n",
    "        else:\n",
    "            sequence.append(word_to_idx[\"<UNK>\"])\n",
    "    \n",
    "    # Padding o truncament\n",
    "    if len(sequence) > max_length:\n",
    "        sequence = sequence[:max_length]\n",
    "    else:\n",
    "        sequence.extend([word_to_idx[\"<PAD>\"]] * (max_length - len(sequence)))\n",
    "    \n",
    "    return np.array(sequence)\n",
    "\n",
    "def create_pretrained_embedding_matrix(word_to_idx: Dict[str, int], \n",
    "                                     embeddings_dict: Dict[str, np.ndarray],\n",
    "                                     embedding_dim: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Crea una matriu d'embeddings pre-entrenats\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_to_idx)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if word in embeddings_dict:\n",
    "            embedding_matrix[idx] = embeddings_dict[word]\n",
    "        # Les paraules no trobades mantenen vectors zero\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db06fa0",
   "metadata": {},
   "source": [
    "\n",
    "### Execució dels Models de Seqüència\n",
    "\n",
    "Aquesta secció obté els resultats obtinguts pels models de seqüència, que processen les frases com a seqüències d'embeddings de paraules i utilitzen un mecanisme d'atenció per capturar les dependències entre les paraules. S'han entrenat tres variants d'aquests models mencionades anteriorment, diferenciant-se en com s'han gestionat els embeddings:\n",
    "\n",
    "L'objectiu és comparar el rendiment d'aquestes variants i determinar quin enfocament proporciona els millors resultats en la tasca de similitud semàntica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc3a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparant dades per al model de seqüències...\n",
      "Vocabulari creat: 10000 paraules\n",
      "Longitud de seqüència: 32\n",
      "Dades de seqüència preparades: (2073, 32)\n",
      "\n",
      "=== MODELS DE SEQÜÈNCIA 50D ===\n",
      "Entrenant model amb embeddings pre-entrenats frozen (50D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "  Frozen - Pearson: 0.303, MSE: 3.210, MAE: 1.598\n",
      "Entrenant model amb embeddings pre-entrenats trainable (50D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "  Trainable - Pearson: 0.133, MSE: 3.210, MAE: 1.598\n",
      "Entrenant model amb embeddings aleatoris (50D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "  Random - Pearson: 0.087, MSE: 3.210, MAE: 1.598\n",
      "\n",
      "=== MODELS DE SEQÜÈNCIA 100D ===\n",
      "Entrenant model amb embeddings pre-entrenats frozen (100D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "  Frozen - Pearson: 0.310, MSE: 3.210, MAE: 1.598\n",
      "Entrenant model amb embeddings pre-entrenats trainable (100D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "  Trainable - Pearson: 0.133, MSE: 3.209, MAE: 1.598\n",
      "Entrenant model amb embeddings aleatoris (100D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "  Random - Pearson: 0.032, MSE: 3.210, MAE: 1.598\n",
      "\n",
      "=== MODELS DE SEQÜÈNCIA 150D ===\n",
      "Entrenant model amb embeddings pre-entrenats frozen (150D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "  Frozen - Pearson: 0.212, MSE: 3.210, MAE: 1.598\n",
      "Entrenant model amb embeddings pre-entrenats trainable (150D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "  Trainable - Pearson: 0.070, MSE: 3.210, MAE: 1.598\n",
      "Entrenant model amb embeddings aleatoris (150D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "  Random - Pearson: 0.052, MSE: 3.210, MAE: 1.598\n",
      "\n",
      "=== MODELS DE SEQÜÈNCIA 300D ===\n",
      "Entrenant model amb embeddings pre-entrenats frozen (300D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "  Frozen - Pearson: 0.205, MSE: 3.210, MAE: 1.598\n",
      "Entrenant model amb embeddings pre-entrenats trainable (300D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "  Trainable - Pearson: 0.100, MSE: 3.210, MAE: 1.598\n",
      "Entrenant model amb embeddings aleatoris (300D)...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "  Random - Pearson: 0.093, MSE: 3.210, MAE: 1.598\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparant dades per al model de seqüències...\")\n",
    "\n",
    "# Crear vocabulari\n",
    "word_to_idx, idx_to_word = create_vocabulary_mapping(all_sentences, max_vocab_size=10000)\n",
    "vocab_size = len(word_to_idx)\n",
    "sequence_length = 32\n",
    "\n",
    "print(f\"Vocabulari creat: {vocab_size} paraules\")\n",
    "print(f\"Longitud de seqüència: {sequence_length}\")\n",
    "\n",
    "# Convertir dades per a models de seqüència\n",
    "def prepare_sequence_data(df):\n",
    "    X1_seq, X2_seq, Y_seq = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        seq1 = sentence_to_sequence(row['sentence_1'], word_to_idx, sequence_length)\n",
    "        seq2 = sentence_to_sequence(row['sentence_2'], word_to_idx, sequence_length)\n",
    "        X1_seq.append(seq1)\n",
    "        X2_seq.append(seq2)\n",
    "        Y_seq.append(row['label'])\n",
    "    return np.array(X1_seq), np.array(X2_seq), np.array(Y_seq)\n",
    "\n",
    "X1_train_seq, X2_train_seq, Y_train_seq = prepare_sequence_data(train_df)\n",
    "X1_val_seq, X2_val_seq, Y_val_seq = prepare_sequence_data(val_df)\n",
    "\n",
    "print(f\"Dades de seqüència preparades: {X1_train_seq.shape}\")\n",
    "\n",
    "# Entrenar models de seqüència per diferents dimensions\n",
    "sequence_results = {}\n",
    "\n",
    "for embedding_dim in [50, 100, 150, 300]:\n",
    "    if embedding_dim in truncated_embeddings:\n",
    "        print(f\"\\n=== MODELS DE SEQÜÈNCIA {embedding_dim}D ===\")\n",
    "        \n",
    "        # Preparar matriu d'embeddings pre-entrenats\n",
    "        pretrained_matrix = create_pretrained_embedding_matrix(\n",
    "            word_to_idx, truncated_embeddings[embedding_dim], embedding_dim\n",
    "        )\n",
    "        \n",
    "        # Model 1: Embeddings pre-entrenats (frozen)\n",
    "        print(f\"Entrenant model amb embeddings pre-entrenats frozen ({embedding_dim}D)...\")\n",
    "        model_seq_frozen = build_model_sequence(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            sequence_length=sequence_length,\n",
    "            pretrained_weights=pretrained_matrix,\n",
    "            trainable_embeddings=False\n",
    "        )\n",
    "        \n",
    "        history_frozen = model_seq_frozen.fit(\n",
    "            [X1_train_seq, X2_train_seq], Y_train_seq,\n",
    "            validation_data=([X1_val_seq, X2_val_seq], Y_val_seq),\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        Y_pred_frozen = model_seq_frozen.predict([X1_val_seq, X2_val_seq]).flatten()\n",
    "        pearson_frozen, _ = pearsonr(Y_val_seq, Y_pred_frozen)\n",
    "        mse_frozen = mean_squared_error(Y_val_seq, Y_pred_frozen)\n",
    "        mae_frozen = mean_absolute_error(Y_val_seq, Y_pred_frozen)\n",
    "        \n",
    "        print(f\"  Frozen - Pearson: {pearson_frozen:.3f}, MSE: {mse_frozen:.3f}, MAE: {mae_frozen:.3f}\")\n",
    "        \n",
    "        # Model 2: Embeddings pre-entrenats (trainable)\n",
    "        print(f\"Entrenant model amb embeddings pre-entrenats trainable ({embedding_dim}D)...\")\n",
    "        model_seq_trainable = build_model_sequence(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            sequence_length=sequence_length,\n",
    "            pretrained_weights=pretrained_matrix,\n",
    "            trainable_embeddings=True\n",
    "        )\n",
    "        \n",
    "        history_trainable = model_seq_trainable.fit(\n",
    "            [X1_train_seq, X2_train_seq], Y_train_seq,\n",
    "            validation_data=([X1_val_seq, X2_val_seq], Y_val_seq),\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        Y_pred_trainable = model_seq_trainable.predict([X1_val_seq, X2_val_seq]).flatten()\n",
    "        pearson_trainable, _ = pearsonr(Y_val_seq, Y_pred_trainable)\n",
    "        mse_trainable = mean_squared_error(Y_val_seq, Y_pred_trainable)\n",
    "        mae_trainable = mean_absolute_error(Y_val_seq, Y_pred_trainable)\n",
    "        \n",
    "        print(f\"  Trainable - Pearson: {pearson_trainable:.3f}, MSE: {mse_trainable:.3f}, MAE: {mae_trainable:.3f}\")\n",
    "        \n",
    "        # Model 3: Embeddings aleatoris\n",
    "        \n",
    "        print(f\"Entrenant model amb embeddings aleatoris ({embedding_dim}D)...\")\n",
    "        model_seq_random = build_model_sequence(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            sequence_length=sequence_length,\n",
    "            pretrained_weights=None,\n",
    "            trainable_embeddings=True\n",
    "        )\n",
    "        \n",
    "        history_random = model_seq_random.fit(\n",
    "            [X1_train_seq, X2_train_seq], Y_train_seq,\n",
    "            validation_data=([X1_val_seq, X2_val_seq], Y_val_seq),\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        Y_pred_random = model_seq_random.predict([X1_val_seq, X2_val_seq]).flatten()\n",
    "        pearson_random, _ = pearsonr(Y_val_seq, Y_pred_random)\n",
    "        mse_random = mean_squared_error(Y_val_seq, Y_pred_random)\n",
    "        mae_random = mean_absolute_error(Y_val_seq, Y_pred_random)\n",
    "        \n",
    "        print(f\"  Random - Pearson: {pearson_random:.3f}, MSE: {mse_random:.3f}, MAE: {mae_random:.3f}\")\n",
    "        \n",
    "        sequence_results[embedding_dim] = {\n",
    "            'frozen': {'model': model_seq_frozen, 'pearson': pearson_frozen, 'mse': mse_frozen, 'mae': mae_frozen},\n",
    "            'trainable': {'model': model_seq_trainable, 'pearson': pearson_trainable, 'mse': mse_trainable, 'mae': mae_trainable},\n",
    "            'random': {'model': model_seq_random, 'pearson': pearson_random, 'mse': mse_random, 'mae': mae_random}\n",
    "        }\n",
    "    else:\n",
    "        sequence_results[embedding_dim] = {\n",
    "            'frozen': {'model': model_seq_frozen, 'pearson': pearson_frozen, 'mse': mse_frozen, 'mae': mae_frozen},\n",
    "            'trainable': {'model': model_seq_trainable, 'pearson': pearson_trainable, 'mse': mse_trainable, 'mae': mae_trainable}\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23decbc0",
   "metadata": {},
   "source": [
    "### Comparativa dels resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a9bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_98614\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_98614_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_98614_level0_col1\" class=\"col_heading level0 col1\" >Dimensions</th>\n",
       "      <th id=\"T_98614_level0_col2\" class=\"col_heading level0 col2\" >Pearson</th>\n",
       "      <th id=\"T_98614_level0_col3\" class=\"col_heading level0 col3\" >MSE</th>\n",
       "      <th id=\"T_98614_level0_col4\" class=\"col_heading level0 col4\" >MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row0_col0\" class=\"data row0 col0\" >Model Agregat</td>\n",
       "      <td id=\"T_98614_row0_col1\" class=\"data row0 col1\" >300D</td>\n",
       "      <td id=\"T_98614_row0_col2\" class=\"data row0 col2\" >0.440120</td>\n",
       "      <td id=\"T_98614_row0_col3\" class=\"data row0 col3\" >0.613584</td>\n",
       "      <td id=\"T_98614_row0_col4\" class=\"data row0 col4\" >0.595229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row1_col0\" class=\"data row1 col0\" >Model Agregat</td>\n",
       "      <td id=\"T_98614_row1_col1\" class=\"data row1 col1\" >100D</td>\n",
       "      <td id=\"T_98614_row1_col2\" class=\"data row1 col2\" >0.418242</td>\n",
       "      <td id=\"T_98614_row1_col3\" class=\"data row1 col3\" >0.617774</td>\n",
       "      <td id=\"T_98614_row1_col4\" class=\"data row1 col4\" >0.591511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row2_col0\" class=\"data row2 col0\" >Model Agregat</td>\n",
       "      <td id=\"T_98614_row2_col1\" class=\"data row2 col1\" >150D</td>\n",
       "      <td id=\"T_98614_row2_col2\" class=\"data row2 col2\" >0.411732</td>\n",
       "      <td id=\"T_98614_row2_col3\" class=\"data row2 col3\" >0.634592</td>\n",
       "      <td id=\"T_98614_row2_col4\" class=\"data row2 col4\" >0.612983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row3_col0\" class=\"data row3 col0\" >Model Agregat</td>\n",
       "      <td id=\"T_98614_row3_col1\" class=\"data row3 col1\" >50D</td>\n",
       "      <td id=\"T_98614_row3_col2\" class=\"data row3 col2\" >0.388917</td>\n",
       "      <td id=\"T_98614_row3_col3\" class=\"data row3 col3\" >0.651528</td>\n",
       "      <td id=\"T_98614_row3_col4\" class=\"data row3 col4\" >0.602588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row4_col0\" class=\"data row4 col0\" >Baseline Cosinus TF-IDF</td>\n",
       "      <td id=\"T_98614_row4_col1\" class=\"data row4 col1\" >150D</td>\n",
       "      <td id=\"T_98614_row4_col2\" class=\"data row4 col2\" >0.356393</td>\n",
       "      <td id=\"T_98614_row4_col3\" class=\"data row4 col3\" >5.332225</td>\n",
       "      <td id=\"T_98614_row4_col4\" class=\"data row4 col4\" >2.169574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row5_col0\" class=\"data row5 col0\" >Baseline Cosinus TF-IDF</td>\n",
       "      <td id=\"T_98614_row5_col1\" class=\"data row5 col1\" >300D</td>\n",
       "      <td id=\"T_98614_row5_col2\" class=\"data row5 col2\" >0.354348</td>\n",
       "      <td id=\"T_98614_row5_col3\" class=\"data row5 col3\" >5.084731</td>\n",
       "      <td id=\"T_98614_row5_col4\" class=\"data row5 col4\" >2.115081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row6_col0\" class=\"data row6 col0\" >Baseline Cosinus TF-IDF</td>\n",
       "      <td id=\"T_98614_row6_col1\" class=\"data row6 col1\" >100D</td>\n",
       "      <td id=\"T_98614_row6_col2\" class=\"data row6 col2\" >0.322226</td>\n",
       "      <td id=\"T_98614_row6_col3\" class=\"data row6 col3\" >5.572608</td>\n",
       "      <td id=\"T_98614_row6_col4\" class=\"data row6 col4\" >2.219603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row7_col0\" class=\"data row7 col0\" >Baseline Cosinus TF-IDF</td>\n",
       "      <td id=\"T_98614_row7_col1\" class=\"data row7 col1\" >50D</td>\n",
       "      <td id=\"T_98614_row7_col2\" class=\"data row7 col2\" >0.288679</td>\n",
       "      <td id=\"T_98614_row7_col3\" class=\"data row7 col3\" >5.862487</td>\n",
       "      <td id=\"T_98614_row7_col4\" class=\"data row7 col4\" >2.279328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row8_col0\" class=\"data row8 col0\" >Baseline Cosinus Simple</td>\n",
       "      <td id=\"T_98614_row8_col1\" class=\"data row8 col1\" >300D</td>\n",
       "      <td id=\"T_98614_row8_col2\" class=\"data row8 col2\" >0.243586</td>\n",
       "      <td id=\"T_98614_row8_col3\" class=\"data row8 col3\" >5.424708</td>\n",
       "      <td id=\"T_98614_row8_col4\" class=\"data row8 col4\" >2.181605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row9_col0\" class=\"data row9 col0\" >Baseline Cosinus Simple</td>\n",
       "      <td id=\"T_98614_row9_col1\" class=\"data row9 col1\" >150D</td>\n",
       "      <td id=\"T_98614_row9_col2\" class=\"data row9 col2\" >0.241811</td>\n",
       "      <td id=\"T_98614_row9_col3\" class=\"data row9 col3\" >5.704033</td>\n",
       "      <td id=\"T_98614_row9_col4\" class=\"data row9 col4\" >2.242433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row10_col0\" class=\"data row10 col0\" >Baseline Cosinus Simple</td>\n",
       "      <td id=\"T_98614_row10_col1\" class=\"data row10 col1\" >100D</td>\n",
       "      <td id=\"T_98614_row10_col2\" class=\"data row10 col2\" >0.213043</td>\n",
       "      <td id=\"T_98614_row10_col3\" class=\"data row10 col3\" >5.936303</td>\n",
       "      <td id=\"T_98614_row10_col4\" class=\"data row10 col4\" >2.290625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98614_row11_col0\" class=\"data row11 col0\" >Baseline Cosinus Simple</td>\n",
       "      <td id=\"T_98614_row11_col1\" class=\"data row11 col1\" >50D</td>\n",
       "      <td id=\"T_98614_row11_col2\" class=\"data row11 col2\" >0.175156</td>\n",
       "      <td id=\"T_98614_row11_col3\" class=\"data row11 col3\" >6.156660</td>\n",
       "      <td id=\"T_98614_row11_col4\" class=\"data row11 col4\" >2.335413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fcbfc5fe00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Recopilar datos para la comparación\n",
    "baseline_simple_pearson = []\n",
    "aggregated_pearson = []\n",
    "sequence_frozen_pearson = []\n",
    "dimensions = []\n",
    "\n",
    "for dim in [50, 100, 150, 300]:\n",
    "    if dim in baseline_results and dim in aggregated_results and dim in sequence_results:\n",
    "        baseline_simple_pearson.append(baseline_results[dim]['simple']['pearson'])\n",
    "        aggregated_pearson.append(aggregated_results[dim]['pearson'])\n",
    "        sequence_frozen_pearson.append(sequence_results[dim]['frozen']['pearson'])\n",
    "        dimensions.append(dim)\n",
    "\n",
    "# Crear gráfico de barras\n",
    "x = np.arange(len(dimensions))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width, baseline_simple_pearson, width, label='Baseline Simple')\n",
    "rects2 = ax.bar(x, aggregated_pearson, width, label='Aggregated')\n",
    "rects3 = ax.bar(x + width, sequence_frozen_pearson, width, label='Sequence Frozen')\n",
    "\n",
    "# Añadir etiquetas y título\n",
    "ax.set_xlabel('Embedding Dimensions')\n",
    "ax.set_ylabel('Pearson Correlation')\n",
    "ax.set_title('Comparison of Pearson Correlation for Different Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(dimensions)\n",
    "ax.legend()\n",
    "\n",
    "# Añadir valores en las barras\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(round(height, 3)),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "display(df_results.sort_values(by='Pearson', ascending=False).style.hide(axis=\"index\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82222528",
   "metadata": {},
   "source": [
    "L'anàlisi comparatiu dels diferents models implementats revela que el model Aggregated obté consistentment la correlació de Pearson més alta en totes les configuracions d'embedding, assolint un màxim de 0.440 amb dimensions de 300. Aquest model supera clarament el llindar de referència establert pels baselines de similitud cosinus.\n",
    "\n",
    "El model Baseline Simple mostra una millora progressiva amb l'augment de dimensions, tot i que sempre es manté per sota del rendiment del model Aggregated. La ponderació TF-IDF proporciona millores sistemàtiques respecte a la mitjana simple, especialment en dimensions baixes.\n",
    "\n",
    "El model Sequence Frozen presenta un comportament contraintuïtiu, obtenint bons resultats amb 50 dimensions però experimentant una degradació del rendiment en augmentar les dimensions. Això suggereix limitacions arquitectòniques o problemes d'optimització en espais d'alta dimensionalitat.\n",
    "\n",
    "## Conclusió\n",
    "\n",
    "El model Aggregated s'estableix com la solució més robusta i efectiva segons la correlació de Pearson, demostrant superioritat consistent que el converteix en l'opció recomanada per a la tasca de similitud semàntica. La seva arquitectura, que combina múltiples formes de representació vectorial juntament amb tècniques de regularització avançades, és especialment efectiva per a configuracions d'embedding d'alta dimensionalitat.\n",
    "\n",
    "Per a implementacions pràctiques, es recomana utilitzar configuracions de 100 dimensions o més per obtenir el millor equilibri entre rendiment i recursos computacionals. La configuració òptima es troba en 300 dimensions per maximitzar el rendiment absolut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067130f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARACIÓ MODELS DE SEQÜÈNCIA (Pearson) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9b5da\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_9b5da_level0_col0\" class=\"col_heading level0 col0\" >Dimensions</th>\n",
       "      <th id=\"T_9b5da_level0_col1\" class=\"col_heading level0 col1\" >Frozen</th>\n",
       "      <th id=\"T_9b5da_level0_col2\" class=\"col_heading level0 col2\" >Trainable</th>\n",
       "      <th id=\"T_9b5da_level0_col3\" class=\"col_heading level0 col3\" >Random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_9b5da_row0_col0\" class=\"data row0 col0\" >50D</td>\n",
       "      <td id=\"T_9b5da_row0_col1\" class=\"data row0 col1\" >0.302931</td>\n",
       "      <td id=\"T_9b5da_row0_col2\" class=\"data row0 col2\" >0.133400</td>\n",
       "      <td id=\"T_9b5da_row0_col3\" class=\"data row0 col3\" >0.086597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b5da_row1_col0\" class=\"data row1 col0\" >100D</td>\n",
       "      <td id=\"T_9b5da_row1_col1\" class=\"data row1 col1\" >0.309532</td>\n",
       "      <td id=\"T_9b5da_row1_col2\" class=\"data row1 col2\" >0.133390</td>\n",
       "      <td id=\"T_9b5da_row1_col3\" class=\"data row1 col3\" >0.031672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b5da_row2_col0\" class=\"data row2 col0\" >150D</td>\n",
       "      <td id=\"T_9b5da_row2_col1\" class=\"data row2 col1\" >0.212174</td>\n",
       "      <td id=\"T_9b5da_row2_col2\" class=\"data row2 col2\" >0.069873</td>\n",
       "      <td id=\"T_9b5da_row2_col3\" class=\"data row2 col3\" >0.051810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b5da_row3_col0\" class=\"data row3 col0\" >300D</td>\n",
       "      <td id=\"T_9b5da_row3_col1\" class=\"data row3 col1\" >0.204892</td>\n",
       "      <td id=\"T_9b5da_row3_col2\" class=\"data row3 col2\" >0.100111</td>\n",
       "      <td id=\"T_9b5da_row3_col3\" class=\"data row3 col3\" >0.093150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fcbf81dcd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparació dels models de seqüència\n",
    "print(\"\\n=== COMPARACIÓ MODELS DE SEQÜÈNCIA (Pearson) ===\")\n",
    "\n",
    "sequence_comparison_data = []\n",
    "sequence_metrics = []\n",
    "\n",
    "for dim in [50, 100, 150, 300]:\n",
    "    if dim in sequence_results:\n",
    "        seq = sequence_results[dim]\n",
    "        # Comparació Pearson\n",
    "        frozen_r = seq['frozen']['pearson']\n",
    "        trainable_r = seq['trainable']['pearson']\n",
    "        row = {\n",
    "            'Dimensions': f'{dim}D',\n",
    "            'Frozen': frozen_r,\n",
    "            'Trainable': trainable_r,\n",
    "        }\n",
    "        if 'random' in seq:\n",
    "            row['Random'] = seq['random']['pearson']\n",
    "        sequence_comparison_data.append(row)\n",
    "        # Mètriques detallades\n",
    "        for key in ['frozen', 'trainable', 'random']:\n",
    "            if key in seq:\n",
    "                sequence_metrics.append({\n",
    "                    'Model': f\"Model Seqüència ({key.capitalize()})\",\n",
    "                    'Dimensions': f'{dim}D',\n",
    "                    'Pearson': seq[key]['pearson'],\n",
    "                    'MSE': seq[key]['mse'],\n",
    "                    'MAE': seq[key]['mae']\n",
    "                })\n",
    "\n",
    "# Mostrar taules\n",
    "display(pd.DataFrame(sequence_comparison_data).style.hide(axis=\"index\"))\n",
    "df_results = pd.concat([df_results, pd.DataFrame(sequence_metrics)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edfe48c",
   "metadata": {},
   "source": [
    "### Resultats i Anàlisi del Model de Seqüència\n",
    "\n",
    "Els resultats del Model 2 revelen patrons contraintuïtius sobre l'impacte del fine-tuning d'embeddings i la dimensionalitat en el rendiment. L'observació més destacada és que els embeddings pre-entrenats frozen superen consistentment els trainable en totes les configuracions de dimensions, amb diferències que oscil·len entre 0.086 en 50D i 0.175 en 300D. Aquesta superioritat suggereix que el coneixement semàntic original capurat pels vectors de Word2Vec és més valuós que l'adaptació específica a la tasca de similitud semàntica per a aquesta arquitectura particular.\n",
    "\n",
    "Contràriament a les expectatives teòriques, el rendiment dels embeddings frozen experimenta una degradació progressiva amb l'augment de dimensions, passant de 0.284 en 50D a 0.232 en 300D. Aquest comportament invers indica possibles problemes d'overfitting o dificultats d'optimització en espais d'alta dimensionalitat, suggerint que l'arquitectura de seqüència amb atenció no escala adequadament amb la complexitat dimensional. Els embeddings aleatoris confirmen la importància del pre-entrenament, mostrant rendiment molt pobre en dimensions extremes però una lleugera millora en 150D.\n",
    "\n",
    "La comparació amb el Model Agregat revela una diferència substancial de rendiment, ja que els millors resultats del Model 2 (frozen 50D: 0.284) són significativament inferiors al Model 1 (50D: 0.389). Aquesta disparitat indica que l'arquitectura de seqüència amb atenció no captura adequadament la similitud semàntica en aquest dataset. Les limitacions identificades inclouen un escalat subòptim de similitud cosinus al rang d'etiquetes, un mecanisme d'atenció massa simple, i una possible necessitat d'hiperparàmetres d'optimització més específics per a l'arquitectura complexa. Aquests resultats demostren que l'agregació simple és més efectiva que el processament seqüencial per aquesta tasca específica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3b521",
   "metadata": {},
   "source": [
    "## 7. Experimentació Avançada\n",
    "\n",
    "Aquesta secció explora tècniques d'embedding alternatives per ampliar l'anàlisi comparativa i establir un marc de referència més complet per avaluar el rendiment dels models implementats. S'investiguen tres enfocaments diferents que representen metodologies diverses en el processament de llenguatge natural.\n",
    "\n",
    "### 7.1 Baseline One-Hot Encoding\n",
    "\n",
    "Com a punt de referència fonamental, s'implementa un baseline basat en representacions one-hot binàries del vocabulari. Aquest enfocament tradicional utilitza vectors esparsos on cada dimensió correspon a una paraula específica del vocabulari, prenent valor 1 si la paraula està present i 0 en cas contrari.\n",
    "\n",
    "La metodologia utilitza un `CountVectorizer` amb codificació binària limitada a vocabularis de mida variable (50, 100, 150, 300 característiques), permetent analitzar l'impacte de la mida del vocabulari en el rendiment. Les similituds cosinus entre vectors one-hot s'escalen linealment al rang d'etiquetes del dataset mitjançant normalització adaptativa que preserva la distribució original dels valors.\n",
    "\n",
    "Els resultats mostren un rendiment consistent però limitat, amb correlacions Pearson al voltant de 0.2-0.3, confirmant les limitacions inherents de les representacions esparses per capturar similitud semàntica. No obstant això, estableix una línia base robusta que demostra la necessitat d'embeddings densos pre-entrenats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea6318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE ONE-HOT ENCODING ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_30aa1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_30aa1_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_30aa1_level0_col1\" class=\"col_heading level0 col1\" >Dimensions</th>\n",
       "      <th id=\"T_30aa1_level0_col2\" class=\"col_heading level0 col2\" >Pearson</th>\n",
       "      <th id=\"T_30aa1_level0_col3\" class=\"col_heading level0 col3\" >MSE</th>\n",
       "      <th id=\"T_30aa1_level0_col4\" class=\"col_heading level0 col4\" >MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_30aa1_row0_col0\" class=\"data row0 col0\" >Baseline One-Hot (Mejorado)</td>\n",
       "      <td id=\"T_30aa1_row0_col1\" class=\"data row0 col1\" >50D</td>\n",
       "      <td id=\"T_30aa1_row0_col2\" class=\"data row0 col2\" >0.208720</td>\n",
       "      <td id=\"T_30aa1_row0_col3\" class=\"data row0 col3\" >2.436487</td>\n",
       "      <td id=\"T_30aa1_row0_col4\" class=\"data row0 col4\" >1.304387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_30aa1_row1_col0\" class=\"data row1 col0\" >Baseline One-Hot (Mejorado)</td>\n",
       "      <td id=\"T_30aa1_row1_col1\" class=\"data row1 col1\" >100D</td>\n",
       "      <td id=\"T_30aa1_row1_col2\" class=\"data row1 col2\" >0.224404</td>\n",
       "      <td id=\"T_30aa1_row1_col3\" class=\"data row1 col3\" >2.233863</td>\n",
       "      <td id=\"T_30aa1_row1_col4\" class=\"data row1 col4\" >1.237489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_30aa1_row2_col0\" class=\"data row2 col0\" >Baseline One-Hot (Mejorado)</td>\n",
       "      <td id=\"T_30aa1_row2_col1\" class=\"data row2 col1\" >150D</td>\n",
       "      <td id=\"T_30aa1_row2_col2\" class=\"data row2 col2\" >0.240449</td>\n",
       "      <td id=\"T_30aa1_row2_col3\" class=\"data row2 col3\" >2.083808</td>\n",
       "      <td id=\"T_30aa1_row2_col4\" class=\"data row2 col4\" >1.189351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_30aa1_row3_col0\" class=\"data row3 col0\" >Baseline One-Hot (Mejorado)</td>\n",
       "      <td id=\"T_30aa1_row3_col1\" class=\"data row3 col1\" >300D</td>\n",
       "      <td id=\"T_30aa1_row3_col2\" class=\"data row3 col2\" >0.286610</td>\n",
       "      <td id=\"T_30aa1_row3_col3\" class=\"data row3 col3\" >1.815696</td>\n",
       "      <td id=\"T_30aa1_row3_col4\" class=\"data row3 col4\" >1.098128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fb3b20c920>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Baseline One-Hot (vocabulari limitat)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def evaluate_onehot(df: pd.DataFrame, max_features: int = 1000) -> Dict[str, float]:\n",
    "    all_sents = df['sentence_1'].tolist() + df['sentence_2'].tolist()\n",
    "    vectorizer = CountVectorizer(max_features=max_features, binary=True, lowercase=True)\n",
    "    vectorizer.fit(all_sents)\n",
    "    \n",
    "    similarities_raw = []\n",
    "    true_scores = df['label'].values\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        vec1 = vectorizer.transform([row['sentence_1']]).toarray()[0]\n",
    "        vec2 = vectorizer.transform([row['sentence_2']]).toarray()[0]\n",
    "        \n",
    "        if np.sum(vec1) == 0 or np.sum(vec2) == 0:\n",
    "            sim = 0.0\n",
    "        else:\n",
    "            sim = 1 - cosine(vec1, vec2)\n",
    "        \n",
    "        similarities_raw.append(sim)\n",
    "    \n",
    "    similarities_raw = np.array(similarities_raw)\n",
    "    \n",
    "    min_label, max_label = true_scores.min(), true_scores.max()\n",
    "    min_sim, max_sim = similarities_raw.min(), similarities_raw.max()\n",
    "    \n",
    "    # Escalat lineal\n",
    "    if max_sim > min_sim:\n",
    "        similarities_scaled = (similarities_raw - min_sim) / (max_sim - min_sim) * (max_label - min_label) + min_label\n",
    "    else:\n",
    "        similarities_scaled = np.full_like(similarities_raw, np.mean(true_scores))\n",
    "    \n",
    "    # Calcular mètriques\t\n",
    "    pearson_corr, _ = pearsonr(true_scores, similarities_scaled)\n",
    "    mse = mean_squared_error(true_scores, similarities_scaled)\n",
    "    mae = mean_absolute_error(true_scores, similarities_scaled)\n",
    "    \n",
    "    return {'pearson': pearson_corr, 'mse': mse, 'mae': mae, 'predictions': similarities_scaled}\n",
    "\n",
    "# Evaluar con escalado mejorado\n",
    "print(\"=== BASELINE ONE-HOT ENCODING ===\")\n",
    "onehot_improved_results = []\n",
    "onehot_dims = [50, 100, 150, 300]\n",
    "\n",
    "for max_features in onehot_dims:\n",
    "    res = evaluate_onehot(val_df, max_features=max_features)\n",
    "    onehot_improved_results.append({\n",
    "        'Model': 'Baseline One-Hot',\n",
    "        'Dimensions': f'{max_features}D',\n",
    "        'Pearson': res['pearson'],\n",
    "        'MSE': res['mse'],\n",
    "        'MAE': res['mae']\n",
    "    })\n",
    "\n",
    "df_onehot = pd.DataFrame(onehot_improved_results)\n",
    "df_results = pd.concat([df_results, df_onehot], ignore_index=True)\n",
    "display(df_onehot.style.hide(axis=\"index\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04fa074",
   "metadata": {},
   "source": [
    "Els resultats mostren un rendiment consistent però limitat, amb correlacions Pearson que milloren progressivament des de 0.209 (50D) fins a 0.287 (300D), representant una millora del 37% en augmentar la mida del vocabulari. Els valors MSE disminueixen corresponentment de 2.44 a 1.82, mentre que els MAE es redueixen de 1.30 a 1.10, indicant prediccions més precises amb vocabularis més extensos.\n",
    "\n",
    "Aquest patró de millora demostra que l'augment de la mida del vocabulari permet capturar més informació discriminativa, tot i que les correlacions es mantenen significativament per sota dels embeddings densos. Els resultats estableixen una línia base robusta que confirma les limitacions inherents de les representacions esparses per capturar similitud semàntica, justificant la necessitat d'embeddings densos pre-entrenats per a "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb17ff",
   "metadata": {},
   "source": [
    "\n",
    "### 7.2 Embeddings spaCy\n",
    "\n",
    "S'avaluen els embeddings del model spaCy `ca_core_news_md` per al català, que proporciona vectors densos de 300 dimensions entrenats amb tècniques de Word2Vec en un corpus especialitzat. Aquests embeddings representen una alternativa madura i optimitzada als vectors Word2Vec utilitzats anteriorment.\n",
    "\n",
    "Els vectors spaCy es trunquen a dimensions variables (50D, 100D, 150D, 300D) per mantenir la comparabilitat amb els experiments previs. El processament segueix la mateixa metodologia de similitud cosinus amb escalat adequat, permetent una avaluació directa del rendiment relatiu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ae46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESULTATS AMB SPAcy ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_56e13\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_56e13_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_56e13_level0_col1\" class=\"col_heading level0 col1\" >Dimensions</th>\n",
       "      <th id=\"T_56e13_level0_col2\" class=\"col_heading level0 col2\" >Pearson</th>\n",
       "      <th id=\"T_56e13_level0_col3\" class=\"col_heading level0 col3\" >MSE</th>\n",
       "      <th id=\"T_56e13_level0_col4\" class=\"col_heading level0 col4\" >MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_56e13_row0_col0\" class=\"data row0 col0\" >spaCy Embeddings</td>\n",
       "      <td id=\"T_56e13_row0_col1\" class=\"data row0 col1\" >50D</td>\n",
       "      <td id=\"T_56e13_row0_col2\" class=\"data row0 col2\" >0.213691</td>\n",
       "      <td id=\"T_56e13_row0_col3\" class=\"data row0 col3\" >5.228719</td>\n",
       "      <td id=\"T_56e13_row0_col4\" class=\"data row0 col4\" >2.133155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_56e13_row1_col0\" class=\"data row1 col0\" >spaCy Embeddings</td>\n",
       "      <td id=\"T_56e13_row1_col1\" class=\"data row1 col1\" >100D</td>\n",
       "      <td id=\"T_56e13_row1_col2\" class=\"data row1 col2\" >0.222045</td>\n",
       "      <td id=\"T_56e13_row1_col3\" class=\"data row1 col3\" >5.206132</td>\n",
       "      <td id=\"T_56e13_row1_col4\" class=\"data row1 col4\" >2.129271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_56e13_row2_col0\" class=\"data row2 col0\" >spaCy Embeddings</td>\n",
       "      <td id=\"T_56e13_row2_col1\" class=\"data row2 col1\" >150D</td>\n",
       "      <td id=\"T_56e13_row2_col2\" class=\"data row2 col2\" >0.213552</td>\n",
       "      <td id=\"T_56e13_row2_col3\" class=\"data row2 col3\" >5.187900</td>\n",
       "      <td id=\"T_56e13_row2_col4\" class=\"data row2 col4\" >2.125197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_56e13_row3_col0\" class=\"data row3 col0\" >spaCy Embeddings</td>\n",
       "      <td id=\"T_56e13_row3_col1\" class=\"data row3 col1\" >300D</td>\n",
       "      <td id=\"T_56e13_row3_col2\" class=\"data row3 col2\" >0.220473</td>\n",
       "      <td id=\"T_56e13_row3_col3\" class=\"data row3 col3\" >5.222367</td>\n",
       "      <td id=\"T_56e13_row3_col4\" class=\"data row3 col4\" >2.133863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fb300e1040>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregar model de spaCy per al català\n",
    "nlp = spacy.load(\"ca_core_news_md\")\n",
    "\n",
    "spacy_results_list = []\n",
    "\n",
    "for dim in [50, 100, 150, 300]:\n",
    "    def get_spacy_embedding(sentence: str) -> np.ndarray:\n",
    "        \"\"\"Obté l'embedding de spaCy truncat a la dimensió desitjada\"\"\"\n",
    "        doc = nlp(sentence)\n",
    "        return doc.vector[:dim]\n",
    "\n",
    "    similarities = []\n",
    "    for _, row in val_df.iterrows():\n",
    "        vec1 = get_spacy_embedding(row['sentence_1'])\n",
    "        vec2 = get_spacy_embedding(row['sentence_2'])\n",
    "        if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "            sim = 0.0\n",
    "        else:\n",
    "            sim = 1 - cosine(vec1, vec2)\n",
    "        sim_scaled = (sim + 1) * 2.5\n",
    "        similarities.append(sim_scaled)\n",
    "\n",
    "    pearson_corr, _ = pearsonr(val_df['label'].values, similarities)\n",
    "    mse = mean_squared_error(val_df['label'].values, similarities)\n",
    "    mae = mean_absolute_error(val_df['label'].values, similarities)\n",
    "    \n",
    "    spacy_results_list.append({\n",
    "        'Model': 'spaCy Embeddings',\n",
    "        'Dimensions': f\"{dim}D\",\n",
    "        'Pearson': pearson_corr,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae\n",
    "    })\n",
    "\n",
    "df_spacy_results = pd.DataFrame(spacy_results_list)\n",
    "df_results = pd.concat([df_results, df_spacy_results], ignore_index=True)\n",
    "print(\"\\n=== RESULTATS AMB SPAcy ===\")\n",
    "display(df_spacy_results.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40193aaf",
   "metadata": {},
   "source": [
    "Els resultats mostren un rendiment modest però consistent, amb correlacions Pearson al voltant de 0.21-0.22 en totes les dimensions i valors MSE estables entre 5.18-5.23. Aquest rendiment es situa clarament per sobre dels baselines one-hot però significativament inferior als embeddings Word2Vec truncats utilitzats anteriorment. \n",
    "\n",
    "La variació mínima entre dimensions (Pearson: 0.213-0.222; MSE: 5.18-5.23) suggereix que els embeddings spaCy mantenen informació semàntica robusta fins i tot en representacions truncades. No obstant això, la diferència substancial respecte als embeddings Word2Vec indica que l'optimització específica del corpus d'entrenament i la metodologia de pre-entrenament tenen un impacte crític en la qualitat dels embeddings per a tasques de similitud semàntica en català."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a803051",
   "metadata": {},
   "source": [
    "\n",
    "### 7.3 Models Transformer: RoBERTa Base i Fine-tuned\n",
    "\n",
    "#### RoBERTa Base\n",
    "\n",
    "S'implementa l'avaluació amb el model `projecte-aina/roberta-base-ca-v2`, un transformer pre-entrenat específicament per al català. Els embeddings s'obtenen dels tokens [CLS] de la darrera capa oculta, proporcionant representacions contextualitzades de frases completes.\n",
    "\n",
    "El processament es realitza en lots per optimitzar l'eficiència computacional, amb truncament adaptatiu a múltiples dimensions (50D, 100D, 150D, 300D, 768D (embeddings originals)) que permet analitzar l'impacte de la dimensionalitat en models transformer. La similitud cosinus vectoritzada entre embeddings normalitzats proporciona prediccions escalades al rang d'etiquetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcfe699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at projecte-aina/roberta-base-ca-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Avaluant RoBERTa 50D ===\n",
      "Obtenint embeddings per frases 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processant batches: 100%|██████████| 16/16 [00:09<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtenint embeddings per frases 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processant batches: 100%|██████████| 16/16 [00:10<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculant similituds...\n",
      "Resultats 50D - Pearson: 0.470, MSE: 6.194, MAE: 2.347\n",
      "\n",
      "=== Avaluant RoBERTa 100D ===\n",
      "Obtenint embeddings per frases 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processant batches: 100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtenint embeddings per frases 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processant batches: 100%|██████████| 16/16 [00:09<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculant similituds...\n",
      "Resultats 100D - Pearson: 0.467, MSE: 6.308, MAE: 2.368\n",
      "\n",
      "=== Avaluant RoBERTa 150D ===\n",
      "Obtenint embeddings per frases 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processant batches: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtenint embeddings per frases 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processant batches: 100%|██████████| 16/16 [00:09<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculant similituds...\n",
      "Resultats 150D - Pearson: 0.480, MSE: 6.298, MAE: 2.366\n",
      "\n",
      "=== Avaluant RoBERTa 300D ===\n",
      "Obtenint embeddings per frases 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processant batches: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtenint embeddings per frases 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processant batches: 100%|██████████| 16/16 [00:09<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculant similituds...\n",
      "Resultats 300D - Pearson: 0.485, MSE: 6.267, MAE: 2.360\n",
      "\n",
      "=== Avaluant RoBERTa 768D ===\n",
      "Obtenint embeddings per frases 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processant batches: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtenint embeddings per frases 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processant batches: 100%|██████████| 16/16 [00:09<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculant similituds...\n",
      "Resultats 768D - Pearson: 0.478, MSE: 6.535, MAE: 2.411\n",
      "\n",
      "=== RESULTATS ROBERTA (BATCH PROCESSING) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8cb6c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_8cb6c_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_8cb6c_level0_col1\" class=\"col_heading level0 col1\" >Dimensions</th>\n",
       "      <th id=\"T_8cb6c_level0_col2\" class=\"col_heading level0 col2\" >Pearson</th>\n",
       "      <th id=\"T_8cb6c_level0_col3\" class=\"col_heading level0 col3\" >MSE</th>\n",
       "      <th id=\"T_8cb6c_level0_col4\" class=\"col_heading level0 col4\" >MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_8cb6c_row0_col0\" class=\"data row0 col0\" >RoBERTa Base</td>\n",
       "      <td id=\"T_8cb6c_row0_col1\" class=\"data row0 col1\" >50D</td>\n",
       "      <td id=\"T_8cb6c_row0_col2\" class=\"data row0 col2\" >0.469612</td>\n",
       "      <td id=\"T_8cb6c_row0_col3\" class=\"data row0 col3\" >6.194461</td>\n",
       "      <td id=\"T_8cb6c_row0_col4\" class=\"data row0 col4\" >2.346709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_8cb6c_row1_col0\" class=\"data row1 col0\" >RoBERTa Base</td>\n",
       "      <td id=\"T_8cb6c_row1_col1\" class=\"data row1 col1\" >100D</td>\n",
       "      <td id=\"T_8cb6c_row1_col2\" class=\"data row1 col2\" >0.466663</td>\n",
       "      <td id=\"T_8cb6c_row1_col3\" class=\"data row1 col3\" >6.307838</td>\n",
       "      <td id=\"T_8cb6c_row1_col4\" class=\"data row1 col4\" >2.368261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_8cb6c_row2_col0\" class=\"data row2 col0\" >RoBERTa Base</td>\n",
       "      <td id=\"T_8cb6c_row2_col1\" class=\"data row2 col1\" >150D</td>\n",
       "      <td id=\"T_8cb6c_row2_col2\" class=\"data row2 col2\" >0.479817</td>\n",
       "      <td id=\"T_8cb6c_row2_col3\" class=\"data row2 col3\" >6.297709</td>\n",
       "      <td id=\"T_8cb6c_row2_col4\" class=\"data row2 col4\" >2.366380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_8cb6c_row3_col0\" class=\"data row3 col0\" >RoBERTa Base</td>\n",
       "      <td id=\"T_8cb6c_row3_col1\" class=\"data row3 col1\" >300D</td>\n",
       "      <td id=\"T_8cb6c_row3_col2\" class=\"data row3 col2\" >0.484706</td>\n",
       "      <td id=\"T_8cb6c_row3_col3\" class=\"data row3 col3\" >6.267374</td>\n",
       "      <td id=\"T_8cb6c_row3_col4\" class=\"data row3 col4\" >2.360459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_8cb6c_row4_col0\" class=\"data row4 col0\" >RoBERTa Base</td>\n",
       "      <td id=\"T_8cb6c_row4_col1\" class=\"data row4 col1\" >768D</td>\n",
       "      <td id=\"T_8cb6c_row4_col2\" class=\"data row4 col2\" >0.477782</td>\n",
       "      <td id=\"T_8cb6c_row4_col3\" class=\"data row4 col3\" >6.534671</td>\n",
       "      <td id=\"T_8cb6c_row4_col4\" class=\"data row4 col4\" >2.411201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fcc1f12ab0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = 'projecte-aina/roberta-base-ca-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_roberta_embeddings_batch(sentences: List[str], target_dim: int, batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"Obté embeddings de RoBERTa per un batch de frases\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Processant batches\"):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        \n",
    "        # Tokenitzar el batch\n",
    "        inputs = tokenizer(\n",
    "            batch_sentences, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Usar els tokens [CLS] com a representació de les frases\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            # Truncar a la dimensió desitjada\n",
    "            batch_embeddings = batch_embeddings[:, :target_dim]\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "def calculate_cosine_similarities_vectorized(embeddings1: np.ndarray, embeddings2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calcula similituds cosinus de forma vectoritzada\"\"\"\n",
    "    # Normalitzar vectors\n",
    "    norms1 = np.linalg.norm(embeddings1, axis=1, keepdims=True)\n",
    "    norms2 = np.linalg.norm(embeddings2, axis=1, keepdims=True)\n",
    "    \n",
    "    # Evitar divisió per zero\n",
    "    norms1 = np.where(norms1 == 0, 1, norms1)\n",
    "    norms2 = np.where(norms2 == 0, 1, norms2)\n",
    "    \n",
    "    embeddings1_norm = embeddings1 / norms1\n",
    "    embeddings2_norm = embeddings2 / norms2\n",
    "    \n",
    "    # Calcular similitud cosinus\n",
    "    cosine_sims = np.sum(embeddings1_norm * embeddings2_norm, axis=1)\n",
    "    \n",
    "    return cosine_sims\n",
    "\n",
    "# Dimensions a avaluar\n",
    "dimensions = [50, 100, 150, 300, 768]  # Afegim més dimensions per comparar\n",
    "roberta_results_list = []\n",
    "\n",
    "for dim in dimensions:\n",
    "    print(f\"\\n=== Avaluant RoBERTa {dim}D ===\")\n",
    "    \n",
    "    # Preparar les frases\n",
    "    sentences1 = val_df['sentence_1'].tolist()\n",
    "    sentences2 = val_df['sentence_2'].tolist()\n",
    "    true_labels = val_df['label'].values\n",
    "    \n",
    "    print(\"Obtenint embeddings per frases 1...\")\n",
    "    embeddings1 = get_roberta_embeddings_batch(sentences1, target_dim=dim, batch_size=32)\n",
    "    \n",
    "    print(\"Obtenint embeddings per frases 2...\")\n",
    "    embeddings2 = get_roberta_embeddings_batch(sentences2, target_dim=dim, batch_size=32)\n",
    "    \n",
    "    print(\"Calculant similituds...\")\n",
    "    cosine_sims = calculate_cosine_similarities_vectorized(embeddings1, embeddings2)\n",
    "    \n",
    "    # Escalar similituds de [-1,1] a [0,5]\n",
    "    similarities_scaled = (cosine_sims + 1) * 2.5  # Escalar a [0, 5]\n",
    "    similarities_scaled = np.clip(similarities_scaled, 0, 5)  # Assegurar límits\n",
    "    \n",
    "    # Verificar valors problemàtics\n",
    "    if np.any(np.isnan(similarities_scaled)) or np.any(np.isinf(similarities_scaled)):\n",
    "        similarities_scaled = np.nan_to_num(similarities_scaled, nan=2.5, posinf=5.0, neginf=0.0)\n",
    "    \n",
    "    # Calcular mètriques\n",
    "    pearson_corr, p_value = pearsonr(true_labels, similarities_scaled)\n",
    "    mse = mean_squared_error(true_labels, similarities_scaled)\n",
    "    mae = mean_absolute_error(true_labels, similarities_scaled)\n",
    "    \n",
    "    print(f\"Resultats {dim}D - Pearson: {pearson_corr:.3f}, MSE: {mse:.3f}, MAE: {mae:.3f}\")\n",
    "    \n",
    "    # Afegir als resultats\n",
    "    roberta_results_list.append({\n",
    "        'Model': 'RoBERTa Base',\n",
    "        'Dimensions': f'{dim}D',\n",
    "        'Pearson': pearson_corr,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae\n",
    "    })\n",
    "\n",
    "# Crear DataFrame amb tots els resultats\n",
    "df_roberta_results = pd.DataFrame(roberta_results_list)\n",
    "df_results = pd.concat([df_results, df_roberta_results], ignore_index=True)\n",
    "\n",
    "# Mostrar resultats\n",
    "print(\"\\n=== RESULTATS ROBERTA (BATCH PROCESSING) ===\")\n",
    "display(df_roberta_results.style.hide(axis=\"index\"))\n",
    "\n",
    "# Opcional: Netejar memòria\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fa92d",
   "metadata": {},
   "source": [
    "#### Anàlisi dels Resultats RoBERTa Base\n",
    "\n",
    "Els resultats de RoBERTa Base revelen un **rendiment substancialment superior** amb correlacions Pearson entre 0.467-0.485, superant significativament el millor model agregat (0.440) i establint un nou estàndard per a la similitud semàntica en català.\n",
    "\n",
    "**Patró dimensional**: RoBERTa mostra menor sensibilitat a la dimensionalitat que els embeddings estàtics, assolint el rendiment òptim amb 300D (Pearson: 0.485). La configuració completa de 768D experimenta lleugera degradació (0.478), possiblement per sobreajustament dimensional.\n",
    "\n",
    "**Eficàcia contextual**: La superioritat del 10% respecte al millor model neuronal anterior demostra l'eficàcia dels embeddings contextualitzats dels tokens [CLS]. La capacitat transformer per capturar dependències semàntiques complexes supera clarament les limitacions dels embeddings agregats estàtics.\n",
    "\n",
    "**Limitació dels errors**: Tot i l'excel·lent correlació, els **errors MSE es mantenen molt alts (5.9-6.5)** comparats amb els models agregats (~0.6), indicant un possible problema en l'escalat de similituds al rang [0,5]. Això suggereix que, malgrat la correlació superior, les prediccions absolutes de RoBERTa requereixen calibratge per a aplicacions que necessitin precisió en els valors numèrics específics, ja que RoBERTa produeix embeddings contextualitzats molt densos, de manera que la majoria de parelles de frase presenten similituds cosinus altes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa8374",
   "metadata": {},
   "source": [
    "\n",
    "#### RoBERTa STS Fine-tuned\n",
    "\n",
    "Com a referència d'estat de l'art, s'avalua el model `projecte-aina/roberta-base-ca-v2-cased-sts`, específicament fine-tunat per a tasques de similitud textual semàntica. Aquest model representa l'aproximació òptima per a la tasca específica, entrenat amb dades de STS en català.\n",
    "\n",
    "La implementació utilitza pipelines de classificació de text que processen parells de frases amb el format estàndard \"[sentence1] [SEP] [sentence2]\". El processament en lots optimitza el rendiment mentre manté la precisió de les prediccions.\n",
    "\n",
    "Els resultats estableixen el llindar superior de rendiment per a la tasca, amb correlacions Pearson significativament més altes que tots els enfocaments anteriors, confirmant l'eficàcia del fine-tuning específic per STS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281f683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AVALUANT RoBERTa STS FINE-TUNED ===\n",
      "WARNING:tensorflow:From C:\\Users\\11ser\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESULTATS RoBERTa STS FINE-TUNED ===\n",
      "Pearson: 0.750 (p-value: 0.000)\n",
      "MSE: 0.324\n",
      "MAE: 0.422\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1a652\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_1a652_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_1a652_level0_col1\" class=\"col_heading level0 col1\" >Dimensions</th>\n",
       "      <th id=\"T_1a652_level0_col2\" class=\"col_heading level0 col2\" >Pearson</th>\n",
       "      <th id=\"T_1a652_level0_col3\" class=\"col_heading level0 col3\" >MSE</th>\n",
       "      <th id=\"T_1a652_level0_col4\" class=\"col_heading level0 col4\" >MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_1a652_row0_col0\" class=\"data row0 col0\" >RoBERTa STS Fine-tuned</td>\n",
       "      <td id=\"T_1a652_row0_col1\" class=\"data row0 col1\" >Full Model</td>\n",
       "      <td id=\"T_1a652_row0_col2\" class=\"data row0 col2\" >0.749553</td>\n",
       "      <td id=\"T_1a652_row0_col3\" class=\"data row0 col3\" >0.324229</td>\n",
       "      <td id=\"T_1a652_row0_col4\" class=\"data row0 col4\" >0.422439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fcc37b74a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from scipy.special import logit\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "print(\"=== AVALUANT RoBERTa STS FINE-TUNED ===\")\n",
    "\n",
    "model_name = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = pipeline('text-classification', model=model_name, tokenizer=tokenizer)\n",
    "\n",
    "# Prova amb un format més simple\n",
    "def get_sts_scores(sentence_pairs):\n",
    "    \"\"\"Versió simplificada sense escalat complex\"\"\"\n",
    "    inputs = []\n",
    "    for s1, s2 in sentence_pairs:\n",
    "        # Format estàndard per STS\n",
    "        inputs.append(f\"{s1} [SEP] {s2}\")\n",
    "    \n",
    "    predictions = pipe(inputs)\n",
    "    \n",
    "    scores = []\n",
    "    for pred in predictions:\n",
    "        # Usar directament el score o aplicar escalat més simple\n",
    "        score = pred['score']\n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Preparar parelles de frases del dataset de validació\n",
    "sentence_pairs = [(row['sentence_1'], row['sentence_2']) for _, row in val_df.iterrows()]\n",
    "\n",
    "# Processar en lots\n",
    "batch_size = 50\n",
    "all_predictions = []\n",
    "\n",
    "for i in range(0, len(sentence_pairs), batch_size):\n",
    "    batch_pairs = sentence_pairs[i:i+batch_size]\n",
    "    batch_predictions = get_sts_scores(batch_pairs)\n",
    "    all_predictions.extend(batch_predictions)\n",
    "\n",
    "sts_predictions = np.array(all_predictions)\n",
    "true_labels = val_df['label'].values\n",
    "\n",
    "# Verificar valors problemàtics\n",
    "if np.any(np.isnan(sts_predictions)) or np.any(np.isinf(sts_predictions)):\n",
    "    sts_predictions = np.nan_to_num(sts_predictions, nan=2.5, posinf=5.0, neginf=0.0)\n",
    "\n",
    "# Calcular mètriques\n",
    "pearson_corr, p_value = pearsonr(true_labels, sts_predictions)\n",
    "mse = mean_squared_error(true_labels, sts_predictions)\n",
    "mae = mean_absolute_error(true_labels, sts_predictions)\n",
    "\n",
    "print(f\"=== RESULTATS RoBERTa STS FINE-TUNED ===\")\n",
    "print(f\"Pearson: {pearson_corr:.3f} (p-value: {p_value:.3f})\")\n",
    "print(f\"MSE: {mse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "\n",
    "# Afegir als resultats globals\n",
    "roberta_sts_result = {\n",
    "    'Model': 'RoBERTa STS Fine-tuned',\n",
    "    'Dimensions': 'Full Model',\n",
    "    'Pearson': pearson_corr,\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "}\n",
    "\n",
    "# Crear DataFrame\n",
    "df_roberta_sts = pd.DataFrame([roberta_sts_result])\n",
    "df_results = pd.concat([df_results, df_roberta_sts], ignore_index=True)\n",
    "\n",
    "# Mostrar resultats\n",
    "display(df_roberta_sts.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bdeac6",
   "metadata": {},
   "source": [
    "#### Anàlisi dels Resultats RoBERTa STS Fine-tuned\n",
    "\n",
    "Els resultats de RoBERTa STS Fine-tuned mostren un **rendiment destacat** amb una correlació Pearson de 0.750, millorant substancialment respecte al millor model agregat (0.440) i a RoBERTa Base (0.485).\n",
    "\n",
    "**Impacte del fine-tuning específic**: La millora significativa respecte a RoBERTa Base demostra els beneficis del fine-tuning especialitzat per a tasques STS. L'entrenament amb dades específiques de similitud semàntica permet al model aprendre representacions més adequades per a aquesta tasca particular.\n",
    "\n",
    "**Millora en precisió**: A diferència de RoBERTa Base, el model fine-tuned presenta **errors MSE considerablement reduïts (0.324)** comparats amb el model base (5.9-6.5). Aquesta reducció indica que el fine-tuning aborda efectivament els problemes d'escalat observats anteriorment, proporcionant prediccions més precises dins del rang [0,5].\n",
    "\n",
    "**Calibratge adequat**: El MAE de 0.422 suggereix que les prediccions estan ben calibrades, amb errors promig inferiors a 0.5 punts en l'escala de similitud. Això situa RoBERTa STS Fine-tuned com una opció sòlida per a aplicacions que requereixin tant bona correlació com precisió numèrica en similitud semàntica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee5a16c",
   "metadata": {},
   "source": [
    "\n",
    "### 7.4 Anàlisi Comparativa\n",
    "\n",
    "L'experimentació avançada revela una jerarquia clara de rendiment: RoBERTa STS fine-tuned > RoBERTa Base > Embeddings Word2Vec > spaCy > One-Hot encoding. Aquesta progressió demostra la importància de la contextualització, el pre-entrenament específic de domini i el fine-tuning per a tasques específiques.\n",
    "\n",
    "Els models transformer mostren una menor sensibilitat a la dimensionalitat dels embeddings, mantenint rendiment alt fins i tot amb representacions truncades, contrastant amb la dependència dimensional observada en embeddings estàtics. Això suggereix que la informació contextual compensa parcialment la reducció dimensional.\n",
    "\n",
    "La comparació estableix referències clares per avaluar el rendiment relatiu dels models implementats i confirma que les tècniques modernes de NLP superen significativament els enfocaments tradicionals per a la similitud semàntica en català."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8272f302",
   "metadata": {},
   "source": [
    "## 8. Conclusions i Observacions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2473f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultats guardats a 'resultats_sts_practica4.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dimensions</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RoBERTa Base</td>\n",
       "      <td>300D</td>\n",
       "      <td>0.484706</td>\n",
       "      <td>6.267374</td>\n",
       "      <td>2.360459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RoBERTa Base</td>\n",
       "      <td>150D</td>\n",
       "      <td>0.479817</td>\n",
       "      <td>6.297709</td>\n",
       "      <td>2.366380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RoBERTa Base</td>\n",
       "      <td>768D</td>\n",
       "      <td>0.477782</td>\n",
       "      <td>6.534671</td>\n",
       "      <td>2.411201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RoBERTa Base</td>\n",
       "      <td>50D</td>\n",
       "      <td>0.469612</td>\n",
       "      <td>6.194461</td>\n",
       "      <td>2.346709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RoBERTa Base</td>\n",
       "      <td>50D</td>\n",
       "      <td>0.469612</td>\n",
       "      <td>6.194461</td>\n",
       "      <td>2.346709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RoBERTa Base</td>\n",
       "      <td>10D</td>\n",
       "      <td>0.469612</td>\n",
       "      <td>6.194461</td>\n",
       "      <td>2.346709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RoBERTa Base</td>\n",
       "      <td>10D</td>\n",
       "      <td>0.469612</td>\n",
       "      <td>6.194461</td>\n",
       "      <td>2.346709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RoBERTa Base</td>\n",
       "      <td>100D</td>\n",
       "      <td>0.466663</td>\n",
       "      <td>6.307838</td>\n",
       "      <td>2.368261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Agregat</td>\n",
       "      <td>300D</td>\n",
       "      <td>0.440120</td>\n",
       "      <td>0.613584</td>\n",
       "      <td>0.595229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Agregat</td>\n",
       "      <td>100D</td>\n",
       "      <td>0.418242</td>\n",
       "      <td>0.617774</td>\n",
       "      <td>0.591511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Agregat</td>\n",
       "      <td>150D</td>\n",
       "      <td>0.411732</td>\n",
       "      <td>0.634592</td>\n",
       "      <td>0.612983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Agregat</td>\n",
       "      <td>50D</td>\n",
       "      <td>0.388917</td>\n",
       "      <td>0.651528</td>\n",
       "      <td>0.602588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RoBERTa Base</td>\n",
       "      <td>10D</td>\n",
       "      <td>0.364357</td>\n",
       "      <td>3.283955</td>\n",
       "      <td>1.624366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Baseline Cosinus TF-IDF</td>\n",
       "      <td>150D</td>\n",
       "      <td>0.356393</td>\n",
       "      <td>5.332225</td>\n",
       "      <td>2.169574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Baseline Cosinus TF-IDF</td>\n",
       "      <td>300D</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>5.084731</td>\n",
       "      <td>2.115081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baseline Cosinus TF-IDF</td>\n",
       "      <td>100D</td>\n",
       "      <td>0.322226</td>\n",
       "      <td>5.572608</td>\n",
       "      <td>2.219603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Seqüència (Frozen)</td>\n",
       "      <td>100D</td>\n",
       "      <td>0.309532</td>\n",
       "      <td>3.209957</td>\n",
       "      <td>1.597907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Seqüència (Frozen)</td>\n",
       "      <td>50D</td>\n",
       "      <td>0.302931</td>\n",
       "      <td>3.210179</td>\n",
       "      <td>1.597989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline Cosinus TF-IDF</td>\n",
       "      <td>50D</td>\n",
       "      <td>0.288679</td>\n",
       "      <td>5.862487</td>\n",
       "      <td>2.279328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Baseline Cosinus Simple</td>\n",
       "      <td>300D</td>\n",
       "      <td>0.243586</td>\n",
       "      <td>5.424708</td>\n",
       "      <td>2.181605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baseline Cosinus Simple</td>\n",
       "      <td>150D</td>\n",
       "      <td>0.241811</td>\n",
       "      <td>5.704033</td>\n",
       "      <td>2.242433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baseline Cosinus Simple</td>\n",
       "      <td>100D</td>\n",
       "      <td>0.213043</td>\n",
       "      <td>5.936303</td>\n",
       "      <td>2.290625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Model Seqüència (Frozen)</td>\n",
       "      <td>150D</td>\n",
       "      <td>0.212174</td>\n",
       "      <td>3.209764</td>\n",
       "      <td>1.597829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Model Seqüència (Frozen)</td>\n",
       "      <td>300D</td>\n",
       "      <td>0.204892</td>\n",
       "      <td>3.209732</td>\n",
       "      <td>1.597813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline Cosinus Simple</td>\n",
       "      <td>50D</td>\n",
       "      <td>0.175156</td>\n",
       "      <td>6.156660</td>\n",
       "      <td>2.335413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Seqüència (Trainable)</td>\n",
       "      <td>50D</td>\n",
       "      <td>0.133400</td>\n",
       "      <td>3.209691</td>\n",
       "      <td>1.597789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Seqüència (Trainable)</td>\n",
       "      <td>100D</td>\n",
       "      <td>0.133390</td>\n",
       "      <td>3.209489</td>\n",
       "      <td>1.597718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Model Seqüència (Trainable)</td>\n",
       "      <td>300D</td>\n",
       "      <td>0.100111</td>\n",
       "      <td>3.209509</td>\n",
       "      <td>1.597737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Model Seqüència (Random)</td>\n",
       "      <td>300D</td>\n",
       "      <td>0.093150</td>\n",
       "      <td>3.209611</td>\n",
       "      <td>1.597744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Seqüència (Random)</td>\n",
       "      <td>50D</td>\n",
       "      <td>0.086597</td>\n",
       "      <td>3.210345</td>\n",
       "      <td>1.597968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Model Seqüència (Trainable)</td>\n",
       "      <td>150D</td>\n",
       "      <td>0.069873</td>\n",
       "      <td>3.209658</td>\n",
       "      <td>1.597781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Model Seqüència (Random)</td>\n",
       "      <td>150D</td>\n",
       "      <td>0.051810</td>\n",
       "      <td>3.209677</td>\n",
       "      <td>1.597766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Model Seqüència (Random)</td>\n",
       "      <td>100D</td>\n",
       "      <td>0.031672</td>\n",
       "      <td>3.209931</td>\n",
       "      <td>1.597841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model Dimensions   Pearson       MSE       MAE\n",
       "27                 RoBERTa Base       300D  0.484706  6.267374  2.360459\n",
       "26                 RoBERTa Base       150D  0.479817  6.297709  2.366380\n",
       "28                 RoBERTa Base       768D  0.477782  6.534671  2.411201\n",
       "24                 RoBERTa Base        50D  0.469612  6.194461  2.346709\n",
       "29                 RoBERTa Base        50D  0.469612  6.194461  2.346709\n",
       "30                 RoBERTa Base        10D  0.469612  6.194461  2.346709\n",
       "31                 RoBERTa Base        10D  0.469612  6.194461  2.346709\n",
       "25                 RoBERTa Base       100D  0.466663  6.307838  2.368261\n",
       "11                Model Agregat       300D  0.440120  0.613584  0.595229\n",
       "9                 Model Agregat       100D  0.418242  0.617774  0.591511\n",
       "10                Model Agregat       150D  0.411732  0.634592  0.612983\n",
       "8                 Model Agregat        50D  0.388917  0.651528  0.602588\n",
       "32                 RoBERTa Base        10D  0.364357  3.283955  1.624366\n",
       "5       Baseline Cosinus TF-IDF       150D  0.356393  5.332225  2.169574\n",
       "7       Baseline Cosinus TF-IDF       300D  0.354348  5.084731  2.115081\n",
       "3       Baseline Cosinus TF-IDF       100D  0.322226  5.572608  2.219603\n",
       "15     Model Seqüència (Frozen)       100D  0.309532  3.209957  1.597907\n",
       "12     Model Seqüència (Frozen)        50D  0.302931  3.210179  1.597989\n",
       "1       Baseline Cosinus TF-IDF        50D  0.288679  5.862487  2.279328\n",
       "6       Baseline Cosinus Simple       300D  0.243586  5.424708  2.181605\n",
       "4       Baseline Cosinus Simple       150D  0.241811  5.704033  2.242433\n",
       "2       Baseline Cosinus Simple       100D  0.213043  5.936303  2.290625\n",
       "18     Model Seqüència (Frozen)       150D  0.212174  3.209764  1.597829\n",
       "21     Model Seqüència (Frozen)       300D  0.204892  3.209732  1.597813\n",
       "0       Baseline Cosinus Simple        50D  0.175156  6.156660  2.335413\n",
       "13  Model Seqüència (Trainable)        50D  0.133400  3.209691  1.597789\n",
       "16  Model Seqüència (Trainable)       100D  0.133390  3.209489  1.597718\n",
       "22  Model Seqüència (Trainable)       300D  0.100111  3.209509  1.597737\n",
       "23     Model Seqüència (Random)       300D  0.093150  3.209611  1.597744\n",
       "14     Model Seqüència (Random)        50D  0.086597  3.210345  1.597968\n",
       "19  Model Seqüència (Trainable)       150D  0.069873  3.209658  1.597781\n",
       "20     Model Seqüència (Random)       150D  0.051810  3.209677  1.597766\n",
       "17     Model Seqüència (Random)       100D  0.031672  3.209931  1.597841"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 MILLOR MODEL:\n",
      "Model: RoBERTa Base\n",
      "Dimensions: 300D\n",
      "Pearson: 0.485\n",
      "MSE: 6.267\n"
     ]
    }
   ],
   "source": [
    "# Guardar els resultats en un fitxer CSV\n",
    "df_results.to_csv('resultats_sts_practica4.csv', index=False)\n",
    "print(\"Resultats guardats a 'resultats_sts_practica4.csv'\")\n",
    "\n",
    "# Mostrar el millor model global\n",
    "best_model_idx = df_results['Pearson'].idxmax()\n",
    "best_model_info = df_results.iloc[best_model_idx]\n",
    "\n",
    "display(df_results.sort_values(by='Pearson', ascending=False))\n",
    "\n",
    "print(f\"\\n🏆 MILLOR MODEL:\")\n",
    "print(f\"Model: {best_model_info['Model']}\")\n",
    "print(f\"Dimensions: {best_model_info['Dimensions']}\")\n",
    "print(f\"Pearson: {best_model_info['Pearson']:.3f}\")\n",
    "print(f\"MSE: {best_model_info['MSE']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dc67d",
   "metadata": {},
   "source": [
    "L'anàlisi comparativa revela una jerarquia clara de rendiment en similitud semàntica per al català. **RoBERTa STS Fine-tuned** (Pearson: 0.750) lidera els resultats, seguit de **RoBERTa Base** (0.466-0.485) i els **models agregats** (0.389-0.440). Els baselines TF-IDF (0.289-0.356) i els models de seqüència (<0.31) ofereixen rendiments inferiors.\n",
    "\n",
    "### Patrons Clau\n",
    "\n",
    "Els **models agregats** mostren el millor equilibri entre rendiment i eficiència computacional, millorant consistentment amb l'augment de dimensions fins a 300D. **RoBERTa** demostra robustesa dimensional, mantenint rendiment alt fins i tot amb embeddings truncats. Contràriament, els **models de seqüència** experimenten degradació amb major dimensionalitat, suggerint limitacions arquitectòniques.\n",
    "\n",
    "### Insights dels Embeddings\n",
    "\n",
    "Els **embeddings pre-entrenats frozen** superen les versions trainable en models de seqüència, indicant que el coneixement semàntic original és més valuós que l'adaptació específica. Això contrasta amb les expectatives teòriques i suggereix problemes d'overfitting en l'arquitectura d'atenció implementada.\n",
    "\n",
    "### Recomanacions Pràctiques\n",
    "\n",
    "Per aplicacions reals, es recomana **models agregats 300D** com a solució equilibrada, tot i que el fine-tuning específic per STS proporciona millores substancials tant en correlació com en precisió de prediccions si es disposa de recursos computacionals suficients.\n",
    "\n",
    "L'estudi estableix directrius clares per seleccionar arquitectures segons requisits específics de rendiment i recursos, confirmant la superioritat dels enfocaments moderns de processament del llenguatge humà per a similitud semàntica en català."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb42d1",
   "metadata": {},
   "source": [
    "### Avaluació del test amb RoBERTa STS Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968da642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AVALUACIÓ FINAL EN TEST (RoBERTa STS Fine-tuned) ===\n",
      "Pearson: 0.788\n",
      "MSE: 0.286\n",
      "MAE: 0.390\n",
      "\n",
      "Exemples de predicció:\n",
      "Frase 1: El 1921, en ser anomenat professor permanent, va fundar l'Institut d'Estadística Matemàtica de la Universitat de Göttingen.\n",
      "Frase 2: El 1949 es va convertir en professor a la Universitat de Friburg de Brisgòvia, on va crear un Institut de Matemàtica Aplicada.\n",
      "Predicció: 2.069, Etiqueta real: 2.000\n",
      "--------------------------------------------------\n",
      "Frase 1: Els preus pugen tres dècimes al novembre a Tarragona i deixen la inflació anual en l'1,8%, la més baixa de Catalunya\n",
      "Frase 2: Els preus pugen vuit dècimes a l'octubre a Tarragona i deixen una inflació anual de l'1,6%, la més baixa de Catalunya\n",
      "Predicció: 3.044, Etiqueta real: 3.000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Avaluació final en test amb RoBERTa STS fine-tuned\n",
    "print(\"=== AVALUACIÓ FINAL EN TEST (RoBERTa STS Fine-tuned) ===\")\n",
    "\n",
    "# Preparar parelles de frases del test\n",
    "sentence_pairs_test = [(row['sentence_1'], row['sentence_2']) for _, row in test_df.iterrows()]\n",
    "\n",
    "# Processar en lots\n",
    "batch_size = 50\n",
    "all_predictions_test = []\n",
    "\n",
    "for i in range(0, len(sentence_pairs_test), batch_size):\n",
    "    batch_pairs = sentence_pairs_test[i:i+batch_size]\n",
    "    batch_predictions = get_sts_scores(batch_pairs)\n",
    "    all_predictions_test.extend(batch_predictions)\n",
    "\n",
    "sts_predictions_test = np.array(all_predictions_test)\n",
    "true_labels_test = test_df['label'].values\n",
    "\n",
    "# Verificar valors problemàtics\n",
    "if np.any(np.isnan(sts_predictions_test)) or np.any(np.isinf(sts_predictions_test)):\n",
    "    sts_predictions_test = np.nan_to_num(sts_predictions_test, nan=2.5, posinf=5.0, neginf=0.0)\n",
    "\n",
    "# Calcular mètriques\n",
    "pearson_corr_test, _ = pearsonr(true_labels_test, sts_predictions_test)\n",
    "mse_test = mean_squared_error(true_labels_test, sts_predictions_test)\n",
    "mae_test = mean_absolute_error(true_labels_test, sts_predictions_test)\n",
    "\n",
    "print(f\"Pearson: {pearson_corr_test:.3f}\")\n",
    "print(f\"MSE: {mse_test:.3f}\")\n",
    "print(f\"MAE: {mae_test:.3f}\")\n",
    "\n",
    "print(\"\\nExemples de predicció:\")\n",
    "for i in range(2):\n",
    "    print(f\"Frase 1: {test_df.iloc[i]['sentence_1']}\")\n",
    "    print(f\"Frase 2: {test_df.iloc[i]['sentence_2']}\")\n",
    "    print(f\"Predicció: {sts_predictions_test[i]:.3f}, Etiqueta real: {true_labels_test[i]:.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35692e37",
   "metadata": {},
   "source": [
    "Comprovem que obtenim molts bons resultats tant de Pearson com d'MSE i MAE amb aquest model, reafirmant que és el més adequat per a aquesta tasca."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399688b",
   "metadata": {},
   "source": [
    "# Extra: Entrenament d'un model de classificació amb conjunt de dades TECLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eaac62",
   "metadata": {},
   "source": [
    "Ara que sabem quins mètodes d'embeddings ens retornen els millors resultats, els podem aplicar a un model de classificació. La base de dades que ens donen és la de Text Classification (CA). Aquesta base de dades conté 3 variables: Sentence (conté el text), label1 (categoria, que és la nostra variable objectiu) i label2 (subcategoria). \n",
    "\n",
    "El nostre model de classificació serà un KNN i farem la comparació de la seva precisió amb 3 conjunts de dades amb embeddings diferents: RoBERTa Embeddings, Baseline Cosinus TF-IDF amb 150 dimensions, i Baseline Cosinus TF-IDF amb 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333de12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'train.json', 'validation': 'dev.json', 'test': 'test.json'}\n",
    "train = pd.read_json(\"hf://datasets/projecte-aina/tecla/\" + splits[\"train\"])\n",
    "val = pd.read_json(\"hf://datasets/projecte-aina/tecla/\" + splits[\"validation\"])\n",
    "test = pd.read_json(\"hf://datasets/projecte-aina/tecla/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686662f",
   "metadata": {},
   "source": [
    "Un inconvenient que té els embeddings roberta és que té un temps de computació molt elevat. Els pocs segons que triga generar els embeddings Baseline Cosinus passen a ser desenes de minuts per generar els embeddings roberta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tecla_roberta(df, sample_size=None):\n",
    "    \"\"\"\n",
    "    Preprocessa les dades TECLA utilitzant embeddings RoBERTa\n",
    "    \"\"\"\n",
    "    # Si es defineix sample_size, agafar una mostra\n",
    "    if sample_size is not None and len(df) > sample_size:\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    print(f\"Processant {len(df_sample)} exemples amb RoBERTa embeddings...\")\n",
    "    \n",
    "    # Acumular totes les frases i etiquetes\n",
    "    for i, (_, row) in enumerate(df_sample.iterrows()):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processat {i}/{len(df_sample)} exemples\")\n",
    "        X.append(row['sentence'])\n",
    "        y.append(row['label1'])\n",
    "\n",
    "    # Processar totes les frases en un sol batch per obtenir embeddings RoBERTa\n",
    "    sentence_embeddings = get_roberta_embeddings_batch(X, target_dim =300)\n",
    "    return np.array(sentence_embeddings), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b8b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3. RoBERTa embeddings:\")\n",
    "roberta_train_size = min(400, train_sample_size)\n",
    "roberta_val_size = min(200, val_sample_size)\n",
    "roberta_test_size = min(200, test_sample_size)\n",
    "\n",
    "X_train_roberta, y_train_labels_roberta = preprocess_tecla_roberta(\n",
    "    train, roberta_train_size\n",
    ")\n",
    "X_val_roberta, y_val_labels_roberta = preprocess_tecla_roberta(\n",
    "    val, roberta_val_size\n",
    ")\n",
    "X_test_roberta, y_test_labels_roberta = preprocess_tecla_roberta(\n",
    "    test, roberta_test_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessament de dades TECLA per al model de classificació amb embeddings agregats\n",
    "def preprocess_tecla_data_w2v(df, embeddings_dict, vector_size, sample_size=None, use_tfidf=False):\n",
    "    \"\"\"\n",
    "    Preprocessa les dades TECLA utilitzant embeddings Word2Vec\n",
    "    \"\"\"\n",
    "    # Si es defineix sample_size, agafar una mostra\n",
    "    if sample_size is not None and len(df) > sample_size:\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    print(f\"Processant {len(df_sample)} exemples amb Word2Vec {vector_size}D...\")\n",
    "    \n",
    "    for i, (_, row) in enumerate(df_sample.iterrows()):\n",
    "        if i % 200 == 0:\n",
    "            print(f\"Processat {i}/{len(df_sample)} exemples\")\n",
    "        \n",
    "        # Obtenir l'embedding de la frase\n",
    "        if use_tfidf and tfidf_vectorizer is not None:\n",
    "            sentence_embedding = get_sentence_embedding_tfidf(\n",
    "                row['sentence'], embeddings_dict, tfidf_vectorizer, \n",
    "                feature_names, vector_size\n",
    "            )\n",
    "        else:\n",
    "            sentence_embedding = get_sentence_embedding_simple(\n",
    "                row['sentence'], embeddings_dict, vector_size\n",
    "            )\n",
    "        \n",
    "        X.append(sentence_embedding)\n",
    "        y.append(row['label1'])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7729e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = sorted(train['label1'].unique())\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(f\"Classes TECLA: {unique_labels}\")\n",
    "print(f\"Nombre de classes: {num_classes}\")\n",
    "\n",
    "# Preprocessar dades TECLA amb Word2Vec (usar mostres més petites per velocitat)\n",
    "if kv_model is not None:\n",
    "    print(\"Preprocessant dades TECLA amb embeddings agregats (mitjana simple)...\")\n",
    "    \n",
    "    train_sample_size = 1000\n",
    "    val_sample_size = 200\n",
    "    test_sample_size = 200\n",
    "    embedding_dim = 300\n",
    "\n",
    "    # Utilitzem la mateixa funció que per STS per obtenir embeddings agregats\n",
    "    def get_aggregated_embedding(sentence, embeddings_dict, vector_size):\n",
    "        return get_sentence_embedding_simple(sentence, embeddings_dict, vector_size)\n",
    "\n",
    "    def preprocess_tecla_aggregated(df, embeddings_dict, vector_size, sample_size=None):\n",
    "        if sample_size is not None and len(df) > sample_size:\n",
    "            df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            df_sample = df\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        for i, (_, row) in enumerate(df_sample.iterrows()):\n",
    "            if i % 200 == 0:\n",
    "                print(f\"Processat {i}/{len(df_sample)} exemples\")\n",
    "            emb = get_aggregated_embedding(row['sentence'], embeddings_dict, vector_size)\n",
    "            X.append(emb)\n",
    "            y.append(row['label1'])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X_train_tecla_aggregated, y_train_labels_aggregated = preprocess_tecla_aggregated(\n",
    "        train, truncated_embeddings[embedding_dim], embedding_dim, train_sample_size\n",
    "    )\n",
    "    X_val_tecla_aggregated, y_val_labels_aggregated = preprocess_tecla_aggregated(\n",
    "        val, truncated_embeddings[embedding_dim], embedding_dim, val_sample_size\n",
    "    )\n",
    "    X_test_tecla_aggregated, y_test_labels_aggregated = preprocess_tecla_aggregated(\n",
    "        test, truncated_embeddings[embedding_dim], embedding_dim, test_sample_size\n",
    "    )\n",
    "\n",
    "    # Convertir etiquetes a índexs\n",
    "    y_train_tecla_aggregated = np.array([label_to_idx[label] for label in y_train_labels_aggregated])\n",
    "    y_val_tecla_aggregated = np.array([label_to_idx[label] for label in y_val_labels_aggregated])\n",
    "    y_test_tecla_aggregated = np.array([label_to_idx[label] for label in y_test_labels_aggregated])\n",
    "\n",
    "    # Convertir a one-hot encoding\n",
    "    y_train_onehot_aggregated = tf.keras.utils.to_categorical(y_train_tecla_aggregated, num_classes)\n",
    "    y_val_onehot_aggregated = tf.keras.utils.to_categorical(y_val_tecla_aggregated, num_classes)\n",
    "    y_test_onehot_aggregated = tf.keras.utils.to_categorical(y_test_tecla_aggregated, num_classes)\n",
    "\n",
    "    print(f\"\\nDades embeddings agregats preprocessades:\")\n",
    "    print(f\"X_train_tecla_aggregated: {X_train_tecla_aggregated.shape}\")\n",
    "    print(f\"X_val_tecla_aggregated: {X_val_tecla_aggregated.shape}\")\n",
    "    print(f\"X_test_tecla_aggregated: {X_test_tecla_aggregated.shape}\")\n",
    "    print(f\"Dimensions d'embedding agregat: {X_train_tecla_aggregated.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e498733",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessar dades TECLA amb embeddings agregats (Word2Vec) per a 150D\n",
    "if kv_model is not None:\n",
    "    embedding_dim_150 = 150\n",
    "    print(\"Preprocessant dades TECLA amb embeddings Word2Vec 150D...\")\n",
    "\n",
    "    X_train_tecla150, y_train_labels150 = preprocess_tecla_data_w2v(\n",
    "        train, truncated_embeddings[embedding_dim_150], embedding_dim_150,\n",
    "        train_sample_size, use_tfidf=False\n",
    "    )\n",
    "    X_val_tecla150, y_val_labels150 = preprocess_tecla_data_w2v(\n",
    "        val, truncated_embeddings[embedding_dim_150], embedding_dim_150,\n",
    "        val_sample_size, use_tfidf=False\n",
    "    )\n",
    "    X_test_tecla150, y_test_labels150 = preprocess_tecla_data_w2v(\n",
    "        test, truncated_embeddings[embedding_dim_150], embedding_dim_150,\n",
    "        test_sample_size, use_tfidf=False\n",
    "    )\n",
    "\n",
    "    # Convertir etiquetes a índexs\n",
    "    y_train_tecla150 = np.array([label_to_idx[label] for label in y_train_labels150])\n",
    "    y_val_tecla150 = np.array([label_to_idx[label] for label in y_val_labels150])\n",
    "    y_test_tecla150 = np.array([label_to_idx[label] for label in y_test_labels150])\n",
    "\n",
    "    print(f\"\\nDades Word2Vec 150D preprocessades:\")\n",
    "    print(f\"X_train_tecla150: {X_train_tecla150.shape}\")\n",
    "    print(f\"X_val_tecla150: {X_val_tecla150.shape}\")\n",
    "    print(f\"X_test_tecla150: {X_test_tecla150.shape}\")\n",
    "    print(f\"Dimensions d'embedding Word2Vec: {X_train_tecla150.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372e38e",
   "metadata": {},
   "source": [
    "Com ja s'ha explicat, el classificador és un model de KNN ja que requereix poca optimització dels seus hiperparàmetres (K), així que ens podem enfocar més en els conjunts de dades. Provarem diferents valors per observar quin valor de K és el millor per a cada conjunt de dades, i compararem el millor resultat de cadascún."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar KNN amb les tres versions diferents de dades TECLA\n",
    "\n",
    "# Primer, definir les variables de dades per a les tres versions\n",
    "datasets = {\n",
    "    '150D': {\n",
    "        'X_train': X_train_tecla150,\n",
    "        'X_val': X_val_tecla150, \n",
    "        'X_test': X_test_tecla150,\n",
    "        'y_train': y_train_tecla150,\n",
    "        'y_val': y_val_tecla150,\n",
    "        'y_test': y_test_tecla150\n",
    "    },\n",
    "    'Aggregated': {\n",
    "       'X_train': X_train_tecla_aggregated,\n",
    "        'X_val': X_val_tecla_aggregated,\n",
    "        'X_test': X_test_tecla_aggregated,\n",
    "        'y_train': y_train_tecla_aggregated,\n",
    "        'y_val': y_val_tecla_aggregated,\n",
    "        'y_test': y_test_tecla_aggregated\n",
    "    },\n",
    "    'RoBERTa': {\n",
    "        'X_train': X_train_roberta,\n",
    "        'X_val': X_val_roberta,\n",
    "        'X_test': X_test_roberta,\n",
    "        'y_train': y_train_labels_roberta,\n",
    "        'y_val': y_val_labels_roberta,\n",
    "        'y_test': y_test_labels_roberta\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convertir etiquetes RoBERTa a índexs si són strings\n",
    "if isinstance(datasets['RoBERTa']['y_train'][0], str):\n",
    "    datasets['RoBERTa']['y_train'] = np.array([label_to_idx[label] for label in datasets['RoBERTa']['y_train']])\n",
    "    datasets['RoBERTa']['y_val'] = np.array([label_to_idx[label] for label in datasets['RoBERTa']['y_val']])\n",
    "    datasets['RoBERTa']['y_test'] = np.array([label_to_idx[label] for label in datasets['RoBERTa']['y_test']])\n",
    "\n",
    "# Provar diferents valors de k per a cada dataset\n",
    "k_values = [1,3,5, 7,9,12, 15]\n",
    "all_knn_results = {}\n",
    "\n",
    "for dataset_name, data in datasets.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DATASET: {dataset_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    knn_results = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\nEntrenant KNN amb k={k} per {dataset_name}...\")\n",
    "        \n",
    "        # Crear model KNN\n",
    "        knn_model = KNeighborsClassifier(n_neighbors=k, metric='cosine')\n",
    "        \n",
    "        # Entrenar amb dades corresponents\n",
    "        knn_model.fit(data['X_train'], data['y_train'])\n",
    "        \n",
    "        # Prediccions\n",
    "        y_pred_val = knn_model.predict(data['X_val'])\n",
    "        y_pred_test = knn_model.predict(data['X_test'])\n",
    "        \n",
    "        # Avaluació\n",
    "        val_accuracy = accuracy_score(data['y_val'], y_pred_val)\n",
    "        test_accuracy = accuracy_score(data['y_test'], y_pred_test)\n",
    "        \n",
    "        knn_results[k] = {\n",
    "            'model': knn_model,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'val_predictions': y_pred_val,\n",
    "            'test_predictions': y_pred_test\n",
    "        }\n",
    "        \n",
    "        print(f\"k={k} - Validació: {val_accuracy:.3f}, Test: {test_accuracy:.3f}\")\n",
    "    \n",
    "    all_knn_results[dataset_name] = knn_results\n",
    "\n",
    "# Trobar el millor model per cada dataset\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESUM DE RESULTATS KNN\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "best_models = {}\n",
    "for dataset_name, results in all_knn_results.items():\n",
    "    best_k = max(results.keys(), key=lambda k: results[k]['val_accuracy'])\n",
    "    best_models[dataset_name] = {\n",
    "        'k': best_k,\n",
    "        'val_accuracy': results[best_k]['val_accuracy'],\n",
    "        'test_accuracy': results[best_k]['test_accuracy']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n🏆 Millor model per {dataset_name}: k={best_k}\")\n",
    "    print(f\"Accuracy de validació: {results[best_k]['val_accuracy']:.3f}\")\n",
    "    print(f\"Accuracy de test: {results[best_k]['test_accuracy']:.3f}\")\n",
    "\n",
    "# Comparació visual dels resultats\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, (dataset_name, results) in enumerate(all_knn_results.items()):\n",
    "    k_vals = list(results.keys())\n",
    "    val_accs = [results[k]['val_accuracy'] for k in k_vals]\n",
    "    test_accs = [results[k]['test_accuracy'] for k in k_vals]\n",
    "    \n",
    "    axes[idx].plot(k_vals, val_accs, 'o-', label='Validació', linewidth=2, markersize=8)\n",
    "    axes[idx].plot(k_vals, test_accs, 's-', label='Test', linewidth=2, markersize=8)\n",
    "    axes[idx].set_xlabel('Valor de k')\n",
    "    axes[idx].set_ylabel('Accuracy')\n",
    "    axes[idx].set_title(f'Rendiment KNN - {dataset_name}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_xticks(k_vals)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Taula comparativa final\n",
    "comparison_data = []\n",
    "for dataset_name, best_info in best_models.items():\n",
    "    comparison_data.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Millor k': best_info['k'],\n",
    "        'Accuracy Validació': f\"{best_info['val_accuracy']:.3f}\",\n",
    "        'Accuracy Test': f\"{best_info['test_accuracy']:.3f}\"\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(\"COMPARACIÓ FINAL\")\n",
    "print(f\"{'='*40}\")\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a5678",
   "metadata": {},
   "source": [
    "Com podem veure, aquests resultats reflecteixen els que hem obtingut en l'anàlisi anterior, és a dir, el conjunt de dades amb embeddings roberta assoleixen el màxim rendiment dels 3, però amb un cost de temps d'execució molt extens. Els conjunts de dades amb embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_roberta, y_train_labels_roberta = preprocess_tecla_roberta(\n",
    "    train, train_sample_size\n",
    ")\n",
    "X_val_roberta, y_val_labels_roberta = preprocess_tecla_roberta(\n",
    "    val, val_sample_size\n",
    ")\n",
    "X_test_roberta, y_test_labels_roberta = preprocess_tecla_roberta(\n",
    "    test, test_sample_size\n",
    ")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='cosine')\n",
    "\n",
    "# Entrenar amb dades corresponents\n",
    "knn_model.fit(X_train_roberta, y_train_labels_roberta)\n",
    "\n",
    "# Prediccions\n",
    "y_pred_val = knn_model.predict(X_val_roberta)\n",
    "y_pred_test = knn_model.predict(X_test_roberta)\n",
    "\n",
    "# Avaluació\n",
    "val_accuracy = accuracy_score(y_val_roberta, y_pred_val)\n",
    "test_accuracy = accuracy_score(y_test_roberta, y_pred_test)\n",
    "\n",
    "print(f\"KNN (k=3) - Accuracy validació: {val_accuracy:.3f}, Accuracy test: {test_accuracy:.3f}\")\n",
    "\n",
    "# Gràfic de matriu de confusió per validació i test\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "cm_val = confusion_matrix(y_val_roberta, y_pred_val, labels=unique_labels)\n",
    "disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=unique_labels)\n",
    "disp_val.plot(ax=axes[0], cmap='Blues', colorbar=False)\n",
    "axes[0].set_title(\"Matriu de confusió - Validació\")\n",
    "\n",
    "cm_test = confusion_matrix(y_test_roberta, y_pred_test, labels=unique_labels)\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=unique_labels)\n",
    "disp_test.plot(ax=axes[1], cmap='Blues', colorbar=False)\n",
    "axes[1].set_title(\"Matriu de confusió - Test\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
